<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>LLM（7）-大模型微调技术 | 小傻瓜别回头</title>
  <meta name="description" content="参考文献：ChatGPT微调技术框架-完美Clone ChatGPT背后的技术  根据instructGPT论文和ChatGPT的技术报告，背后的微调技术逐步挖掘出来，大致分为：SFT；ReWard模型训练；RLHF（PPO）三个阶段。思考：是否可以在第一阶段之前，补充一个PT预训练微调。 1. 可微调阶段 1.1. stage1: SFTSFT（supervised-fintuning），使用">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM（7）-大模型微调技术">
<meta property="og:url" content="https://tlylft.github.io/LLM/07_finetuning/index.html">
<meta property="og:site_name" content="灵魂都失控">
<meta property="og:description" content="参考文献：ChatGPT微调技术框架-完美Clone ChatGPT背后的技术  根据instructGPT论文和ChatGPT的技术报告，背后的微调技术逐步挖掘出来，大致分为：SFT；ReWard模型训练；RLHF（PPO）三个阶段。思考：是否可以在第一阶段之前，补充一个PT预训练微调。 1. 可微调阶段 1.1. stage1: SFTSFT（supervised-fintuning），使用">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-10-22-21-36.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-10-20-48-36.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-11-10-55-28.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-11-11-00-15.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-11-11-03-32.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-10-22-14-49.png">
<meta property="article:published_time" content="2023-05-16T00:52:19.000Z">
<meta property="article:modified_time" content="2023-08-11T09:25:04.929Z">
<meta property="article:author" content="Icey Liu">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tlylft.github.io/LLM/07_finetuning/2023-08-10-22-21-36.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://tlylft.github.io/LLM/07_finetuning/index.html">
  
    <link rel="alternate" href="/atom.xml" title="灵魂都失控" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/avatar.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 4.2.1"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tlylft" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Icey</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">小傻瓜,别回头</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hebei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>Welcome!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/">LLM</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/transformer/">transformer</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leecode/">Leecode</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">62</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/NER/">NER</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/algorithm/">algorithm</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/bert/">bert</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/corrector/">corrector</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/label/">label</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/segment/">segment</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/sensitive/">sensitive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/similarity/">similarity</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/word2vec/">word2vec</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">信息提取</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/">cv</a><span class="category-list-count">24</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/cv/pointcloud/">pointcloud</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/segmentation/">segmentation</a><span class="category-list-count">11</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/knowledge-graph/">knowledge graph</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/autoML/">autoML</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/time-series/">time series</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">53</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/Django/">Django</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/Flask/">Flask</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/keras/">keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/pandas/">pandas</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python/">python</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</a><span class="category-list-count">8</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/">tools</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/tools/ftp/">ftp</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/kubernetes/">kubernetes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/paper/">paper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/">产品经理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/">大数据技术</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/hive/">hive</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/kafka/">kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/mysql/">mysql</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">对话系统</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><span class="category-list-count">13</span></li></ul>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Anaconda/" style="font-size: 13px;">Anaconda</a> <a href="/tags/Django/" style="font-size: 13.14px;">Django</a> <a href="/tags/Docker/" style="font-size: 13px;">Docker</a> <a href="/tags/Flask/" style="font-size: 13.57px;">Flask</a> <a href="/tags/LLM/" style="font-size: 13.57px;">LLM</a> <a href="/tags/Leecode/" style="font-size: 13.14px;">Leecode</a> <a href="/tags/NER/" style="font-size: 13.36px;">NER</a> <a href="/tags/NLP/" style="font-size: 13.93px;">NLP</a> <a href="/tags/RASA/" style="font-size: 13.71px;">RASA</a> <a href="/tags/algorithm/" style="font-size: 13.29px;">algorithm</a> <a href="/tags/anaconda/" style="font-size: 13px;">anaconda</a> <a href="/tags/autoML/" style="font-size: 13px;">autoML</a> <a href="/tags/bert/" style="font-size: 13.64px;">bert</a> <a href="/tags/bert4keras/" style="font-size: 13.14px;">bert4keras</a> <a href="/tags/blog/" style="font-size: 13px;">blog</a> <a href="/tags/corrector/" style="font-size: 13.21px;">corrector</a> <a href="/tags/cv/" style="font-size: 13.79px;">cv</a> <a href="/tags/deep-learning/" style="font-size: 13.86px;">deep learning</a> <a href="/tags/doccano/" style="font-size: 13px;">doccano</a> <a href="/tags/docker/" style="font-size: 13.07px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/github/" style="font-size: 13px;">github</a> <a href="/tags/hive/" style="font-size: 13.29px;">hive</a> <a href="/tags/k8s/" style="font-size: 13px;">k8s</a> <a href="/tags/kafka/" style="font-size: 13px;">kafka</a> <a href="/tags/keras/" style="font-size: 13px;">keras</a> <a href="/tags/kerberos/" style="font-size: 13px;">kerberos</a> <a href="/tags/knowledge-graph/" style="font-size: 13.5px;">knowledge graph</a> <a href="/tags/kubeflow/" style="font-size: 13px;">kubeflow</a> <a href="/tags/kubernetes/" style="font-size: 13px;">kubernetes</a> <a href="/tags/label/" style="font-size: 13.07px;">label</a> <a href="/tags/linux/" style="font-size: 13.5px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 13.86px;">machine learning</a> <a href="/tags/math/" style="font-size: 13.14px;">math</a> <a href="/tags/matplotlib/" style="font-size: 13px;">matplotlib</a> <a href="/tags/mysql/" style="font-size: 13.07px;">mysql</a> <a href="/tags/navicat/" style="font-size: 13px;">navicat</a> <a href="/tags/neo4j/" style="font-size: 13.14px;">neo4j</a> <a href="/tags/nlp/" style="font-size: 13px;">nlp</a> <a href="/tags/numpy/" style="font-size: 13px;">numpy</a> <a href="/tags/paddle/" style="font-size: 13px;">paddle</a> <a href="/tags/pandas/" style="font-size: 13.21px;">pandas</a> <a href="/tags/pointcloud/" style="font-size: 13.07px;">pointcloud</a> <a href="/tags/ppt/" style="font-size: 13px;">ppt</a> <a href="/tags/pycharm/" style="font-size: 13.07px;">pycharm</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/pytorch/" style="font-size: 13.21px;">pytorch</a> <a href="/tags/segment/" style="font-size: 13.07px;">segment</a> <a href="/tags/segmentation/" style="font-size: 13.64px;">segmentation</a> <a href="/tags/sensitive/" style="font-size: 13.07px;">sensitive</a> <a href="/tags/similarity/" style="font-size: 13.43px;">similarity</a> <a href="/tags/tensorflow/" style="font-size: 13.07px;">tensorflow</a> <a href="/tags/time-series/" style="font-size: 13.07px;">time series</a> <a href="/tags/tools/" style="font-size: 13.64px;">tools</a> <a href="/tags/vp/" style="font-size: 13px;">vp</a> <a href="/tags/word2vec/" style="font-size: 13.36px;">word2vec</a> <a href="/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" style="font-size: 13px;">产品经理</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/" style="font-size: 13.14px;">信息提取</a> <a href="/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" style="font-size: 13.86px;">对话系统</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.14px;">强化学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 13.07px;">爬虫</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 13.5px;">环境搭建</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 13.36px;">计算机视觉</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 13.64px;">读书笔记</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">24</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-LLM/07_finetuning" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      LLM（7）-大模型微调技术
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/LLM/07_finetuning/" class="article-date">
	  <time datetime="2023-05-16T00:52:19.000Z" itemprop="datePublished">创建于: 2023-05-16</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/LLM/" rel="tag">LLM</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/LLM/07_finetuning/" class="leancloud_visitors"  data-flag-title="LLM（7）-大模型微调技术">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/LLM/07_finetuning/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <blockquote>
<p>参考文献：<br><a href="https://zhuanlan.zhihu.com/p/618028708?utm_id=0" target="_blank" rel="noopener">ChatGPT微调技术框架-完美Clone ChatGPT背后的技术</a></p>
</blockquote>
<p>根据instructGPT论文和ChatGPT的技术报告，背后的微调技术逐步挖掘出来，大致分为：SFT；ReWard模型训练；RLHF（PPO）三个阶段。<br>思考：是否可以在第一阶段之前，补充一个PT预训练微调。</p>
<h1 id="1-可微调阶段"><a href="#1-可微调阶段" class="headerlink" title="1. 可微调阶段"></a>1. 可微调阶段</h1><p><img src="/LLM/07_finetuning/2023-08-10-22-21-36.png" alt></p>
<h2 id="1-1-stage1-SFT"><a href="#1-1-stage1-SFT" class="headerlink" title="1.1. stage1: SFT"></a>1.1. stage1: SFT</h2><p>SFT（supervised-fintuning），使用instruction datasets数据进行监督微调</p>
<h2 id="1-2-stage2-Reward-Model"><a href="#1-2-stage2-Reward-Model" class="headerlink" title="1.2. stage2: Reward Model"></a>1.2. stage2: Reward Model</h2><p>训练奖励模型，它通过对于同一个 prompt 的不同输出进行人工排序，得到对应分数，监督训练奖励模型。</p>
<h2 id="1-3-stage3-RLHF-Reinforcement-Learning-from-Human-Feedback"><a href="#1-3-stage3-RLHF-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="1.3. stage3: RLHF(Reinforcement Learning from Human Feedback)"></a>1.3. stage3: RLHF(Reinforcement Learning from Human Feedback)</h2><p>这个部分是Openai， ChatGPT和GPT4作为核心的部分,该训练流程基于PPO（Proximal Policy Optimization，近端策略优化）算法来进行优化。<br>RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法。<br><img src="/LLM/07_finetuning/2023-08-10-20-48-36.png" alt></p>
<h1 id="2-微调框架和流程"><a href="#2-微调框架和流程" class="headerlink" title="2. 微调框架和流程"></a>2. 微调框架和流程</h1><h2 id="2-1-模型选择"><a href="#2-1-模型选择" class="headerlink" title="2.1. 模型选择"></a>2.1. 模型选择</h2><p>根据业务场景和算例条件，选择合适的底座参数模型</p>
<blockquote>
<p><strong>微调和高效微调</strong><br>微调， Fine-Tuning，一般指全参数的微调 (全量微调)，指是一类较早诞生的微调方法，全参数微调需要消耗大量的算力，实际使用起来并不方便，因此不久之后又诞生了只围绕部分参数进行微调的高效微调方法;不能适用于大模型敏捷开发。<br>高效微调，State-of-the-art Parameter-Efficient Fine-Tuning (SOTA PEFT)，特指部分参数的微调方法，这种方法算力功耗比更高，也是目前最为常见的微调方法;除此之外，Fine-Tuning也可以代指全部微调方法，同时OpenAl中模型微调API的名称也是Fine-Tuning，实际也是PEFT</p>
<h2 id="2-2-pipeline训练"><a href="#2-2-pipeline训练" class="headerlink" title="2.2. pipeline训练"></a>2.2. pipeline训练</h2></blockquote>
<p><strong><a href="https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md" target="_blank" rel="noopener">ColossalAI</a>初步打造全流程雏形</strong><br>根据底层技术优化很好的把三个阶段训练进行了串联，实现微调的高效性。体验基于Meta LLAM 7B 微调后模型效果，中文的效果不行（可能是meta llama底座对中文不是非常友好，单纯微调中文英文的可以基于GLM模型来微调）<br>注：</p>
<ol>
<li>一些介绍：<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650872068&amp;idx=1&amp;sn=2cb6257bbbc10dc4da19e88d4fc58cbc" target="_blank" rel="noopener">0门槛克隆ChatGPT方案再升级，开源模型完整复现</a></li>
<li>似乎只是基于LLaMA</li>
</ol>
<p><strong><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener">DeepSpeed Chat</a></strong></p>
<ol>
<li>一些介绍：<a href="https://zhuanlan.zhihu.com/p/621780753" target="_blank" rel="noopener">DeepSpeed-Chat：最强ChatGPT训练框架，一键完成RLHF训练</a></li>
<li>似乎只是gpt-2这种，但github上有chatglm相关的</li>
</ol>
<p>目前整个开源社区还没有其他能够在一个框架上面训练如上的三个阶段的流程化训练，大家都是分阶段独立训练</p>
<h3 id="2-2-1-stage1-SFT"><a href="#2-2-1-stage1-SFT" class="headerlink" title="2.2.1. stage1: SFT"></a>2.2.1. stage1: SFT</h3><p>SFT（supervised-fintuning），使用instruction datasets数据进行监督微调<br>数据集：<br>instruct:xxx<br>output: xxxx<br>微调方式：</p>
<ul>
<li>全量微调</li>
<li>Freeze</li>
<li>lora</li>
<li>P-Tuning</li>
<li>Prefix Tuning</li>
<li>PromptTuning</li>
<li>AdaLora</li>
</ul>
<p>开源框架：<br><strong><a href="https://github.com/huggingface/peft" target="_blank" rel="noopener">peft</a></strong>: 支持多个模型，如：GPT-2，LLaMA, ChatGLM<br><strong><a href="https://github.com/liucongg/ChatGLM-Finetuning" target="_blank" rel="noopener">ChatGLM-Finetuning</a></strong>: 支持chatGLM</p>
<h3 id="2-2-2-stage2-Reward-Model-amp-stage-3-RLHF"><a href="#2-2-2-stage2-Reward-Model-amp-stage-3-RLHF" class="headerlink" title="2.2.2. stage2: Reward Model &amp; stage 3: RLHF"></a>2.2.2. stage2: Reward Model &amp; stage 3: RLHF</h3><p>数据集：<br>微调方式：</p>
<p>开源框架：<br><a href="https://github.com/huggingface/trl" target="_blank" rel="noopener">TRL</a>: 支持Reward模型训练和PPO优化过程<br><strong><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener">DeepSpeed Chat</a></strong>： 微软开源提供的最好的RLHF流程</p>
<h1 id="3-SFT微调方法"><a href="#3-SFT微调方法" class="headerlink" title="3. SFT微调方法"></a>3. SFT微调方法</h1><p>参考文章： <a href="https://baijiahao.baidu.com/s?id=1771636050471546897&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">人工智能大语言模型微调技术：SFT、LoRA、Freeze 监督微调方法</a><br>随着技术的发展，涌现出越来越多的大语言模型，且模型参数越来越多，比如 GPT3 已经达到 1750 亿的参数量，传统的监督微调方法已经不再能适用现阶段的大语言模型。为了解决微调参数量太多的问题，同时也要保证微调效果，急需研发出参数高效的微调方法（Parameter Efficient Fine Tuning, PEFT）</p>
<h2 id="3-1-LoRA"><a href="#3-1-LoRA" class="headerlink" title="3.1. LoRA"></a>3.1. LoRA</h2><p><strong>一句话： 冻结预训练参数，微调新增层结构，大大减少训练参数量。</strong><br>LoRA（Low-Rank Adaptation of Large Language Models），直译为大语言模型的低阶自适应。<br><strong>背景：</strong><br>随着大语言模型的发展，模型的参数量越来越大，比如 GPT-3 参数量已经高达 1750 亿，因此，微调所有模型参数变得不可行。LoRA 微调方法由微软提出，通过只微调新增参数的方式，大大减少了下游任务的可训练参数数量。<br><strong>原理简述：</strong><br>基于大模型的内在低秩特性，增加旁路矩阵来<strong>模拟全参数微调</strong>;<br>LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型参数参与微调类似的效果。<br>在大语言模型微调的过程中，LoRA 冻结了预先训练好的模型权重，并将可训练的秩的分解矩阵注入到 Transformer 体系结构的每一层。例如，对于预训练的权重矩阵W0，可以让其更新受到用低秩分解表示后者的约束：<br><img src="/LLM/07_finetuning/2023-08-11-10-55-28.png" alt><br><strong>优势：</strong></p>
<ul>
<li>预训练模型参数可以被共享，用于为不同的任务构建许多小的 LoRA 模块。冻结共享模型，并通过替换矩阵 A 和 B 可以有效地切换任务，从而显著降低存储需求和多个任务切换的成本。<br>当使用自适应优化器时，由于不需要计算梯度以及保存太多模型参数，LoRA 使得微调效果更好，并将微调的硬件门槛降低了 3 倍。</li>
<li>低秩分解采用线性设计的方式使得在部署时能够将可训练的参数矩阵与冻结的参数矩阵合并，与完全微调的方法相比，不引入推理延迟。</li>
<li>LoRA 与其它多种微调方法不冲突，可以与其它微调方法相结合，比如将要介绍的前缀调优方法等。</li>
<li>在图像生成任务中diffusion model进行微调，表现惊艳。<h2 id="3-2-Prefix-tuning"><a href="#3-2-Prefix-tuning" class="headerlink" title="3.2. Prefix-tuning"></a>3.2. Prefix-tuning</h2>Prefix-tuning（ Optimizing Continuous Prompts for Generation）基于提示词前缀优化的微调方法。由斯坦福大学提出。<br><strong>原理简述：</strong>在原始模型基础上，增加一个可被训练的Embedding层，用于给提示词增加前缀，从而让模型更好的理解提示词意图并在训练过程中不断优化这些参数。<br>在模型中加入 prefix，即连续的特定任务向量，微调时只优化这一小段参数。<br><img src="/LLM/07_finetuning/2023-08-11-11-00-15.png" alt><br><strong>优势：</strong><br>既能够在模型结构上增加一些新的灵活性，又能够在模型使用上提供一种自动的、能够改进模型表现的天天机创<h2 id="3-3-P-tuning-Prompt-tuning-v1"><a href="#3-3-P-tuning-Prompt-tuning-v1" class="headerlink" title="3.3. P-tuning/Prompt tuning v1"></a>3.3. P-tuning/Prompt tuning v1</h2>P-tuning v1 微调方法由谷歌提出的轻量级优化方法<br><strong>原理简述：</strong><br>无需调整模型参数，在已有参数中，选择一部分参数作为可学习参数，用于创建每个Prompt的前缀，从而帮助模型更好地理解和处理特定的任务。<br>不同于Prefix方法，Prompt Tuning训练得到的前缀是具备可解释性的我们可以通过查看这些前缀，来查看模型是如何帮我们优化prompt的。类似于<strong>自动化提示工程的工具。</strong><br>将Prompt 加入到微调过程中，只对 Prompt 部分的参数进行训练，而语言模型的参数固定不变。</li>
</ul>
<p><img src="/LLM/07_finetuning/2023-08-11-11-03-32.png" alt><br><strong>优势：</strong><br>该方法在参数规模非常大的模型微调时效果很好，当参数规模达100亿时和全量微调效果一致;<br><strong>不足：</strong><br>P-tuning v1 微调方法缺少普遍性。实验表明，对于那些较小的模型，P-tuning v1 方法和全参数微调方法的表现有很大差异，效果很差。同时，P-tuning v1 缺少跨任务的通用性，在序列标注任务中的有效性没有得到验证。序列标注需要预测一连串的标签，而且大都是无实际意义的标签，对于 P-tuning v1 微调方法极具挑战。此外，当模型层数很深时，微调时模型的稳定性难以保证。模型层数越深，第一层输入的 prompt 对后面的影响难以预估。</p>
<h2 id="3-4-P-tuning-v2"><a href="#3-4-P-tuning-v2" class="headerlink" title="3.4. P-tuning v2"></a>3.4. P-tuning v2</h2><p>P-tuning v2 微调方法是 P-tuning v1 微调方法的改进版，同时借鉴了 prefix-tuning 微调的方法。<br><img src="/LLM/07_finetuning/2023-08-10-22-14-49.png" alt><br>与 P-tuning v1 微调方法相比，P-tuning v2 微调方法采用了 prefix-tuning 的做法，在输入前面的每一层都加入可微调的参数。在 prefix 部分，每一层的 transformer 的 embedding 输入都需要被微调，而 P-tuning v1 只在第一层进行微调。同时，对于 prefix 部分，每一层 transformer 的输入不是从上一层输出，而是随机初始化的 embedding 作为输入。</p>
<p>此外，P-Tuning v2 还包括以下改进：</p>
<p>移除 Reparamerization 加速训练方式；<br>采用多任务学习优化：基于多任务数据集的 Prompt 进行预训练，然后再适配的下游任务。<br>舍弃词汇 Mapping 的 Verbalizer 的使用，重新利用 [CLS] 和字符标签，跟传统微调方法一样利用 cls 或者 token 的输出做自然语言理解，以增强通用性，可以适配到序列标注任务。<br><strong>优点：</strong></p>
<ul>
<li><p>P-tuning v2 微调方法解决了 P-tuning v1 方法的缺陷，是一种参数高效的大语言模型微调方法。</p>
</li>
<li><p>P-tuning v2 微调方法仅精调 0.1% 参数量（固定 LM 参数），在各个参数规模语言模型上，均取得和 - Fine-tuning 相比肩的性能，解决了 P-tuning v1 在参数量不够多的模型中微调效果很差的问题。如下图所示（横坐标表示模型参数量，纵坐标表示微调效果）：</p>
</li>
<li><p>将 Prompt tuning 技术首次拓展至序列标注等复杂的 NLU 任务上，而 P-tuning v1 在此任务上无法运作。</p>
</li>
<li>非常适合GLM这种双向预训练大模型微调<h2 id="3-5-freeze"><a href="#3-5-freeze" class="headerlink" title="3.5. freeze"></a>3.5. freeze</h2>Freeze 方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行 TP 或 PP 操作，就可以对大模型进行训练。在语言模型模型微调中，Freeze 微调方法仅微调 Transformer 后几层的全连接层参数，而冻结其它所有参数<br>优点：<br>大量减少了大语言模型的微调参数，是一种参数高效的微调方法；<br>由于只需微调高层特征，加快了模型的收敛，节约了微调的时间；<br>最大程度地保留了大语言模型预训练所学习到的语言的 “共性”，可解释性较强。</li>
</ul>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://tlylft.github.io/LLM/07_finetuning/" title="LLM（7）-大模型微调技术" target="_blank" rel="external">https://tlylft.github.io/LLM/07_finetuning/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tlylft" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tlylft" target="_blank"><span class="text-dark">Icey</span><small class="ml-1x">小傻瓜,别回头</small></a></h3>
        <div>以梦为马，随处可栖。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/LLM/06_bard/" title="LLM（6）-BARD"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/machine_learning/time_series/prophet/" title="prophet"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'eL0FL59XsOPmrzo5OTb0zhgE-gzGzoHsz',
    appKey: '53Yf97Qf3bKKWC81Bh4EM9oY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>