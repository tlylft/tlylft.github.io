<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>LLM（8）分布式并行训练 | 小傻瓜别回头</title>
  <meta name="description" content="参考资料：LLM的分布式并行训练方式总结万字长文详解大模型平台技术栈deepspeed入门教程deepspeed 官方文档  目录[TOC] 1. 并行技术1.1. 数据并行核心思路：每个GPU处理一部分数据，常用于单机多卡。将模型参数和优化器状态复制到多个 GPU 上，每个 Worker 并行处理数据的一部分，执行前向和反向传播以获取梯度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM（8）分布式并行训练">
<meta property="og:url" content="https://tlylft.github.io/LLM/08_LLM_parallelism/index.html">
<meta property="og:site_name" content="灵魂都失控">
<meta property="og:description" content="参考资料：LLM的分布式并行训练方式总结万字长文详解大模型平台技术栈deepspeed入门教程deepspeed 官方文档  目录[TOC] 1. 并行技术1.1. 数据并行核心思路：每个GPU处理一部分数据，常用于单机多卡。将模型参数和优化器状态复制到多个 GPU 上，每个 Worker 并行处理数据的一部分，执行前向和反向传播以获取梯度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-43-15.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-45-31.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-45-46.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-31-52.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-16-55.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-22-27.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-22-44-32.png">
<meta property="article:published_time" content="2023-08-15T00:48:21.000Z">
<meta property="article:modified_time" content="2023-08-29T01:09:52.598Z">
<meta property="article:author" content="Icey Liu">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tlylft.github.io/LLM/08_LLM_parallelism/2023-08-28-15-43-15.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://tlylft.github.io/LLM/08_LLM_parallelism/index.html">
  
    <link rel="alternate" href="/atom.xml" title="灵魂都失控" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/avatar.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 4.2.1"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tlylft" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Icey</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">小傻瓜,别回头</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hebei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>Welcome!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/">LLM</a><span class="category-list-count">12</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/transformer/">transformer</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leecode/">Leecode</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">59</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/NER/">NER</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/algorithm/">algorithm</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/bert/">bert</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/corrector/">corrector</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/label/">label</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/segment/">segment</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/sensitive/">sensitive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/similarity/">similarity</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/word2vec/">word2vec</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">信息提取</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/">cv</a><span class="category-list-count">24</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/cv/pointcloud/">pointcloud</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/segmentation/">segmentation</a><span class="category-list-count">11</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/knowledge-graph/">knowledge graph</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/autoML/">autoML</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/time-series/">time series</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">53</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/Django/">Django</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/Flask/">Flask</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/keras/">keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/pandas/">pandas</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python/">python</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</a><span class="category-list-count">8</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/">tools</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/tools/ftp/">ftp</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/kubernetes/">kubernetes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/paper/">paper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/">产品经理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/">大数据技术</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/hive/">hive</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/kafka/">kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/mysql/">mysql</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">对话系统</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><span class="category-list-count">13</span></li></ul>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Anaconda/" style="font-size: 13px;">Anaconda</a> <a href="/tags/Django/" style="font-size: 13.14px;">Django</a> <a href="/tags/Docker/" style="font-size: 13px;">Docker</a> <a href="/tags/Flask/" style="font-size: 13.57px;">Flask</a> <a href="/tags/LLM/" style="font-size: 13.71px;">LLM</a> <a href="/tags/Leecode/" style="font-size: 13.14px;">Leecode</a> <a href="/tags/NER/" style="font-size: 13.36px;">NER</a> <a href="/tags/NLP/" style="font-size: 13.93px;">NLP</a> <a href="/tags/RASA/" style="font-size: 13.71px;">RASA</a> <a href="/tags/algorithm/" style="font-size: 13.29px;">algorithm</a> <a href="/tags/anaconda/" style="font-size: 13px;">anaconda</a> <a href="/tags/autoML/" style="font-size: 13px;">autoML</a> <a href="/tags/bert/" style="font-size: 13.5px;">bert</a> <a href="/tags/bert4keras/" style="font-size: 13.14px;">bert4keras</a> <a href="/tags/blog/" style="font-size: 13px;">blog</a> <a href="/tags/corrector/" style="font-size: 13.21px;">corrector</a> <a href="/tags/cv/" style="font-size: 13.79px;">cv</a> <a href="/tags/deep-learning/" style="font-size: 13.86px;">deep learning</a> <a href="/tags/doccano/" style="font-size: 13px;">doccano</a> <a href="/tags/docker/" style="font-size: 13.07px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/github/" style="font-size: 13px;">github</a> <a href="/tags/hive/" style="font-size: 13.29px;">hive</a> <a href="/tags/k8s/" style="font-size: 13px;">k8s</a> <a href="/tags/kafka/" style="font-size: 13px;">kafka</a> <a href="/tags/keras/" style="font-size: 13px;">keras</a> <a href="/tags/kerberos/" style="font-size: 13px;">kerberos</a> <a href="/tags/knowledge-graph/" style="font-size: 13.5px;">knowledge graph</a> <a href="/tags/kubeflow/" style="font-size: 13px;">kubeflow</a> <a href="/tags/kubernetes/" style="font-size: 13px;">kubernetes</a> <a href="/tags/label/" style="font-size: 13.07px;">label</a> <a href="/tags/linux/" style="font-size: 13.5px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 13.86px;">machine learning</a> <a href="/tags/math/" style="font-size: 13.14px;">math</a> <a href="/tags/matplotlib/" style="font-size: 13px;">matplotlib</a> <a href="/tags/mysql/" style="font-size: 13.07px;">mysql</a> <a href="/tags/navicat/" style="font-size: 13px;">navicat</a> <a href="/tags/neo4j/" style="font-size: 13.14px;">neo4j</a> <a href="/tags/nlp/" style="font-size: 13px;">nlp</a> <a href="/tags/numpy/" style="font-size: 13px;">numpy</a> <a href="/tags/paddle/" style="font-size: 13px;">paddle</a> <a href="/tags/pandas/" style="font-size: 13.21px;">pandas</a> <a href="/tags/pointcloud/" style="font-size: 13.07px;">pointcloud</a> <a href="/tags/ppt/" style="font-size: 13px;">ppt</a> <a href="/tags/pycharm/" style="font-size: 13.07px;">pycharm</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/pytorch/" style="font-size: 13.21px;">pytorch</a> <a href="/tags/segment/" style="font-size: 13.07px;">segment</a> <a href="/tags/segmentation/" style="font-size: 13.64px;">segmentation</a> <a href="/tags/sensitive/" style="font-size: 13.07px;">sensitive</a> <a href="/tags/similarity/" style="font-size: 13.43px;">similarity</a> <a href="/tags/tensorflow/" style="font-size: 13.07px;">tensorflow</a> <a href="/tags/time-series/" style="font-size: 13.07px;">time series</a> <a href="/tags/tools/" style="font-size: 13.64px;">tools</a> <a href="/tags/vp/" style="font-size: 13px;">vp</a> <a href="/tags/word2vec/" style="font-size: 13.36px;">word2vec</a> <a href="/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" style="font-size: 13px;">产品经理</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/" style="font-size: 13.14px;">信息提取</a> <a href="/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" style="font-size: 13.86px;">对话系统</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.14px;">强化学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 13.07px;">爬虫</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 13.5px;">环境搭建</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 13.36px;">计算机视觉</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 13.64px;">读书笔记</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">16</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">24</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-LLM/08_LLM_parallelism" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      LLM（8）分布式并行训练
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/LLM/08_LLM_parallelism/" class="article-date">
	  <time datetime="2023-08-15T00:48:21.000Z" itemprop="datePublished">创建于: 2023-08-15</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/LLM/" rel="tag">LLM</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/LLM/08_LLM_parallelism/" class="leancloud_visitors"  data-flag-title="LLM（8）分布式并行训练">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/LLM/08_LLM_parallelism/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <blockquote>
<p>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/635825108" target="_blank" rel="noopener">LLM的分布式并行训练方式总结</a><br><a href="https://mp.weixin.qq.com/s/zoXxkdVj70XOuAMdkGtuEQ" target="_blank" rel="noopener">万字长文详解大模型平台技术栈</a><br><a href="https://zhuanlan.zhihu.com/p/630734624" target="_blank" rel="noopener">deepspeed入门教程</a><br><a href="https://www.deepspeed.ai/getting-started/" target="_blank" rel="noopener">deepspeed 官方文档</a></p>
</blockquote>
<p><strong>目录</strong><br>[TOC]</p>
<h1 id="1-并行技术"><a href="#1-并行技术" class="headerlink" title="1. 并行技术"></a>1. 并行技术</h1><h2 id="1-1-数据并行"><a href="#1-1-数据并行" class="headerlink" title="1.1. 数据并行"></a>1.1. 数据并行</h2><p><strong>核心思路：每个GPU处理一部分数据，常用于单机多卡。</strong><br>将模型参数和优化器状态复制到多个 GPU 上，每个 Worker 并行处理数据的一部分，执行前向和反向传播以获取梯度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量的梯度，从而更新所有 GPU 上的模型。<br><strong>DP：</strong> 参数服务器<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-43-15.png" alt><br><strong>DDP:</strong> 解决DP在多机情况下，通讯负载不均的问题，将Server上的通讯压力均衡转到各个Worker上。<br>1) 数据切分： 假如有n块gpu, 每块gpu上的数据也被分成n份（避免出现oom）。</p>
<p>2）Reduce-Scatter：定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行累加。每一次累加更新都形成一个拓扑环，因此被称为Ring。<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-45-31.png" alt><br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-45-46.png" alt><br>缺点：<br>1） 存储开销大。每块GPU上都存了一份完整的模型，造成冗余。<br>2）通讯开销大。Server需要和每一个Worker进行梯度传输，Worker间并不通讯。因此Server承担了系统所有的通讯压力。 当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。<br>3）当模型可以放进单个 GPU 时才有效，模型太大无法做数据并行。<br>改进：<br>为了解决这个问题，微软提出了 ZeRO，</p>
<h2 id="1-2-流水线并行"><a href="#1-2-流水线并行" class="headerlink" title="1.2. 流水线并行"></a>1.2. 流水线并行</h2><p><strong>核心思路：每个GPU处理一部分模型层</strong></p>
<h3 id="朴素流水线并行："><a href="#朴素流水线并行：" class="headerlink" title="朴素流水线并行："></a>朴素流水线并行：</h3><p>将模型的不同 layer 分配到多个 GPU 中，数据在不同层（也即是不同 GPU ）之间移动。下图中 a 展示了流水线并行的计算方式，前向传播时，数据从 Device0 依次传递到 Device3；然后经历反向传播，计算梯度从 Device3 依次传递回 Device0。<br><strong>缺点：</strong><br>1） gpu利用率低，因为每个 GPU 必须等待前一个 GPU 计算完成，从而导致不必要的气泡开销。<br>2）中间激活激活值结果占据大量内存。<br><strong>改进：</strong><br>为了减少气泡开销，GPipe[60] 和 PipeDream[61] 提出如果同时进行多个迭代，每个节点在同一时刻负责不同迭代的计算，就可以避免数据依赖，不用在原地干等了。<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-31-52.png" alt></p>
<h2 id="1-3-张量并行"><a href="#1-3-张量并行" class="headerlink" title="1.3. 张量并行"></a>1.3. 张量并行</h2><p>张量并行把全连接层的参数和计算分割到多个 GPU 上，与流水线不同，张量并行专注于分解模型的张量（参数矩阵），Megatron-LM [62] 论文中关于这个问题有详细阐述。</p>
<h2 id="1-4-3D-Parallelism概念"><a href="#1-4-3D-Parallelism概念" class="headerlink" title="1.4. 3D Parallelism概念"></a>1.4. 3D Parallelism概念</h2><p>3D 并行实际上是三种常用并行训练技术的组合，即数据并行、流水线并行和张量并行。</p>
<p><img src="/LLM/08_LLM_parallelism/2023-08-28-15-16-55.png" alt><br><strong>通讯量比较：TP &gt; DP &gt; PP</strong></p>
<p>因此优先将张量并行放在同一机器上（机器内的带宽大，通讯时间也会小）。 流水线并行因为通讯量消耗低，可以跨机器。 若张量并行没有跨机器，则数据并行也不需要跨机器；否则数据并行组也需要跨机器。</p>
<h2 id="1-5-ZeRO"><a href="#1-5-ZeRO" class="headerlink" title="1.5. ZeRO"></a>1.5. ZeRO</h2><p>零冗余优化器，Zero Redundancy Optimizer</p>
<p>这是一种新的并行优化器，它可以大大减少模型和数据并行所需的资源，同时可以大量增加可训练的参数数量。</p>
<p>其原理就是普通的数据并行，只是每个 GPU 没有都复制完整的模型参数、梯度和优化器状态，而是只存储其中的一部分。在之后的训练过程，当需要给定层的完整层参数时，所有 GPU 通过通信同步以相互提供它们缺失的部分。</p>
<p>ZeRO 具有三个主要的优化阶段，分别对应于优化器状态、梯度和参数分区。</p>
<p><img src="/LLM/08_LLM_parallelism/2023-08-28-15-22-27.png" alt><br><strong>特点：</strong></p>
<ol>
<li>克服了数据并行和模型并行的局限性，同时实现两者的优点，它通过跨数据并行进程将模型状态划分为上图所示的参数、梯度和优化器状态分区，而不是复制它们，从而消除了数据并行进程之间的内存冗余。</li>
<li>在训练期间使用动态通信规划（dynamic communication schedule），在分布式设备之间共享必要的状态，以保持数据并行的计算粒度和通信量。</li>
<li>与 PyTorch 兼容，DeepSpeed API 是在 PyTorch 上进行的轻量级封装</li>
<li>DeepSpeed 管理着所有样板化的 SOTA 训练技术，例如分布式训练、混合精度、梯度累积和检查点，开发者可以专注于模型开发。</li>
</ol>
<p><strong>优点：</strong></p>
<ol>
<li>规模大： 可运行当前最先进的模型参数量，多达1000亿个参数。</li>
<li>速度快： 吞吐量比SOTA高出5呗。</li>
<li>成本低： 提高吞吐量意味着大大降低训练成本，例如，要训练具有 200 亿个参数的模型，DeepSpeed 需要的资源是原来的 3/4。</li>
<li>易用性强： 只需更改几行代码即可使 PyTorch 模型使用 DeepSpeed 和 ZeRO。不需要重新设计代码或重构模型，它也没有对模型尺寸、批处理大小或任何其它训练参数加以限制。<h1 id="2-通用概念"><a href="#2-通用概念" class="headerlink" title="2. 通用概念"></a>2. 通用概念</h1><h2 id="2-1-local-rank与global-rank"><a href="#2-1-local-rank与global-rank" class="headerlink" title="2.1. local_rank与global_rank"></a>2.1. local_rank与global_rank</h2>假设：有三个server，每个server有8个GPU，一共24个GPU，<br>word_size: 是整个分布式环境中进程的总数。<br>local_rank： 就是在当前server上本地的rank,范围在0-7， 当设置为-1时，表示不再分布式设置下运行。<br>global_rank：就是每个进程在全局分布式训练中的的rank,用于标识该进程在分布式环境中的位置， 范围在0到word_size-1</li>
</ol>
<p>获取方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置local_rank:</span><br><span class="line">torch.device(&#39;cuda&#39;, local_rank)</span><br><span class="line">deepspeed.init_distributed()  # 初始化分布式后端</span><br><span class="line"># 获取gobal</span><br><span class="line">global_rank &#x3D; torch.distributed.get_rank()</span><br></pre></td></tr></table></figure></p>
<p><strong>torch.distributed.barrier()</strong><br>是一个同步函数，用于在分布式环境中同步各个进程的状态。 调用时进程会阻塞等待，所有进程都调用后解除。<br>用途： </p>
<ol>
<li>同步梯度更新， 每个进程完成一轮反向传播后，互相同步梯度后再进行下一轮更新。</li>
<li>并行训练使，数据读取只在local_rank=0的进程中进行，其他进程阻塞等待。</li>
</ol>
<h2 id="2-2-卡间通讯方式"><a href="#2-2-卡间通讯方式" class="headerlink" title="2.2. 卡间通讯方式"></a>2.2. 卡间通讯方式</h2><p>gpu之间的通信时间会限制multi-gpu的训练速度，而gpu之间的通信模式如果不是NVLink，多块卡的训练速度可能比一块卡要慢。 通过以下命令查看卡间通讯方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi topo -m</span><br></pre></td></tr></table></figure></p>
<h1 id="3-分布式框架-pytorch"><a href="#3-分布式框架-pytorch" class="headerlink" title="3. 分布式框架 pytorch"></a>3. 分布式框架 pytorch</h1><h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net &#x3D; torch.nn.DataParallel(model, device_ids&#x3D;[0, 1, 2])</span><br><span class="line">output &#x3D; net(input_var)</span><br></pre></td></tr></table></figure>
<h1 id="4-分布式框架-DeepSpeed"><a href="#4-分布式框架-DeepSpeed" class="headerlink" title="4. 分布式框架 DeepSpeed"></a>4. 分布式框架 DeepSpeed</h1><p>DeepSpeed是微软推出的大规模模型分布式训练的工具，主要实现了ZeRO并行训练算法。通过提高规模、速度、可用性并降低成本，可以在当前一代的 GPU 集群上训练具有超过 1000 亿个参数的深度学习模型，极大促进大型模型的训练。同时，与最新技术相比，其系统性能可以提高 5 倍以上。</p>
<p>原始文档：<a href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed" target="_blank" rel="noopener">https://huggingface.co/docs/transformers/main/main_classes/deepspeed</a></p>
<p><img src="/LLM/08_LLM_parallelism/2023-08-28-22-44-32.png" alt></p>
<h2 id="4-1-主要功能"><a href="#4-1-主要功能" class="headerlink" title="4.1. 主要功能"></a>4.1. 主要功能</h2><ol>
<li>Optimizer state partitioning (ZeRO stage 1)</li>
<li>Gradient partitioning (ZeRO stage 2)</li>
<li>Parameter partitioning (ZeRO stage 3)</li>
<li>Custom mixed precision training handling</li>
<li>A range of fast CUDA-extension-based optimizers</li>
<li>ZeRO-Offload to CPU and NVMe</li>
</ol>
<h2 id="4-2-参数说明"><a href="#4-2-参数说明" class="headerlink" title="4.2. 参数说明"></a>4.2. 参数说明</h2><p>与数据相关的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data path: 数据路径，huggingface数据库， 比如:Dahoas&#x2F;rm-static</span><br><span class="line">data_split&quot; 数据的拆分方式，比如2,4,4 是为step1，2，3分配的数据比例</span><br><span class="line">max_seq_len: 最大序列长度, 超过会对数据进行截断，过大会内存不足</span><br><span class="line">data_output_path:相关数据的存储地址，需要是local storage，不能是shared storage</span><br></pre></td></tr></table></figure><br>与模型相关的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_name_or_path: 模型地址，huggingface模型，比如:facebook&#x2F;opt-1.3b</span><br><span class="line">lora_dim: 如果大于0，则使用LORA优化</span><br><span class="line">lora_module_name: 设置LORA的范围，比如可以只针对 decoder.layers</span><br><span class="line">only_optimize lora:是否只优化LORA的参数</span><br></pre></td></tr></table></figure><br>与训练相关的参数(需要不断调试和调整)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">per_device_train_batch_size :训练时的 Batch size (per device:每个GPU的size)</span><br><span class="line">per_device eval_batch_size:评价时的的 Batch size (per device)，增加可提高训练的稳定性</span><br><span class="line">learning_rate:学习率</span><br><span class="line">weight_decay:权重衰减，防止模型过拟合的技术</span><br><span class="line">num_train_epochs:训练 epoch 数</span><br><span class="line">gradient_accumulation_steps : 累积多少个 mini-batch 的梯度后再进行一次参数更新</span><br><span class="line">Ir_scheduler_type: learning rate的调整策略，比如 linear，cosine</span><br></pre></td></tr></table></figure><br>deepspeed相关参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zero_stage:这个对应者DeepSpeed工具中的zero方式，分别是0，1，2，3 (一个gpu装不下需设置为3)</span><br><span class="line">offload: ZeRO-0ffload 通过利用主机CPU上的计算和内存资源来执行优化器，从而减少此类模型的GPU计算和内存需求</span><br><span class="line">local_rank :分布式训练时的一个变量，用于标识当前 GPU 设备的本地排名 (本机排名，与global-rank不同)gradient_checkpointing:降低深度学习模型训练过程中内存消耗的技术</span><br><span class="line"></span><br><span class="line">seed: 随机排序是的seed</span><br><span class="line">output_dir: 模型的存储目录</span><br></pre></td></tr></table></figure></p>
<h2 id="4-3-训练启动配置"><a href="#4-3-训练启动配置" class="headerlink" title="4.3. 训练启动配置"></a>4.3. 训练启动配置</h2><p>相关默认路径： /job/hostfile<br>相关参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- num_gpus 设置为1时，不再查找hostfile文件</span><br><span class="line">-- hostfile 集群ip配置文件，默认为&#x2F;job&#x2F;hostfile</span><br><span class="line">-- include</span><br></pre></td></tr></table></figure></p>
<h3 id="4-3-1-单机单卡训练"><a href="#4-3-1-单机单卡训练" class="headerlink" title="4.3.1. 单机单卡训练"></a>4.3.1. 单机单卡训练</h3><p>如果我们只在单个节点（具有一个或多个 GPU）上运行，DeepSpeed 不需要如上所述的主机文件。如果未检测到或传入主机文件，则DeepSpeed将查询本地计算机上的GPU数量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 删除&#x2F;job&#x2F;hostfile文件，执行下面语句</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 deepspeed --num_gpus&#x3D;1 train.py</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p><strong>为何单机单卡也是用deepspeed？</strong><br>1.使用ZeRO-offload，将部分数据offload到CPU，降低对显存的需求2<br>2.提供了对显存的管理，减少显存中的碎片</p>
</blockquote>
<h3 id="4-3-2-单机指定单GPU训练"><a href="#4-3-2-单机指定单GPU训练" class="headerlink" title="4.3.2. 单机指定单GPU训练"></a>4.3.2. 单机指定单GPU训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --include localhost:1 train.py ... --deepspeed ds_config.json</span><br></pre></td></tr></table></figure>
<h3 id="4-3-3-单机多卡训练"><a href="#4-3-3-单机多卡训练" class="headerlink" title="4.3.3. 单机多卡训练"></a>4.3.3. 单机多卡训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1 deepspeed train.py ... --deepspeed ds_config.json</span><br><span class="line">或</span><br><span class="line">deepspeed --num_gpus&#x3D;2 train.py ... --deepspeed ds_config.json</span><br><span class="line">或</span><br><span class="line">torch.distributed.run --nproc_per_node&#x3D;2 your_program.py --deepspeed ds_config.json</span><br></pre></td></tr></table></figure>
<h3 id="4-3-4-多节点多卡"><a href="#4-3-4-多节点多卡" class="headerlink" title="4.3.4. 多节点多卡"></a>4.3.4. 多节点多卡</h3><ol>
<li>设置hostfile文件，只需在一个节点上启动<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hostname1 slots&#x3D;8</span><br><span class="line">hostname2 slots&#x3D;8</span><br><span class="line"># 然后运行</span><br><span class="line">deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile_path --master_addr hostname1 --master_port&#x3D;9901 train.py ... --deepspeed ds_config.json</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="4-4-ZeRO配置文件"><a href="#4-4-ZeRO配置文件" class="headerlink" title="4.4. ZeRO配置文件"></a>4.4. ZeRO配置文件</h2><p>选择不同的Zero stage和offload?<br>1) 从左到右，越来越慢<br>Stage 0 (DDP) &gt; Stage 1 &gt; Stage 2 &gt; Stage 2 + offload &gt; Stage 3 &gt; Stage 3 + offloads<br>2) 从左到右，所需GPU显存越来越少<br>Stage 0 (DDP) &lt; Stage 1 &lt; Stage 2 &lt; Stage 2 + offload &lt; Stage 3 &lt; Stage 3 + offloads</p>
<h3 id="NVMe-Support"><a href="#NVMe-Support" class="headerlink" title="NVMe Support ??"></a>NVMe Support ??</h3><ul>
<li>ZeRO-Infinity 需要使用 ZeRO-3</li>
<li>ZeRO-3 会比 ZeRO-2 慢很多。使用以下策略，可以使得ZeRO-3 的速度更接近ZeRO-2<ol>
<li>将stage3_param_persistence_threshold参数设置的很大，比如6 <em> hidden_size </em> hidden_size</li>
<li>将offload_params参数关闭（可以极大改善性能）<h3 id="Zero-0"><a href="#Zero-0" class="headerlink" title="Zero-0"></a>Zero-0</h3>stage 0会禁用所有的分片，然后把DeepSpeed当作时DDP来使用。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Zero-1"><a href="#Zero-1" class="headerlink" title="Zero-1"></a>Zero-1</h3>只对优化器参数进行分片，可以加速一丢丢<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Zero-2"><a href="#Zero-2" class="headerlink" title="Zero-2"></a>Zero-2</h3>配置示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 2,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;allgather_partitions&quot;: true,</span><br><span class="line">        &quot;allgather_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;overlap_comm&quot;: true,   # 控制是否使用通信与计算的重叠。当设置为True时，DeepSpeed将在梯度计算时尝试并行执行梯度通信。可以有效地减少通信时间，从而加速整个训练过程。</span><br><span class="line">        &quot;reduce_scatter&quot;: true,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 2000,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
参数说明：</li>
</ol>
</li>
<li>overlap_comm: 控制是否使用通信与计算的重叠。当设置为True时，DeepSpeed将在梯度计算时尝试并行执行梯度通信。可以有效地减少通信时间，从而加速整个训练过程</li>
<li>allgather_bucket_size：用于控制Allgather操作的分桶大小。Allgather操作是指在分布式训练中，每个进程收集其他所有进程的张量，并将这些张量按顺序拼接起来。通过将张量划分为较小的桶（buckets），可以在通信过程中更高效地传输数据。allgather_bucket_size值越大，每个桶的大小越大，通信操作可能会变得更快，但也需要更多的内存来存储中间结果。合适的桶大小要根据实际情况调整。</li>
<li>reduce_bucket_size：类似于allgather_bucket_size，用于控制Allreduce操作的分桶大小。Allreduce操作是将所有进程的某个张量进行规约（例如求和），并将结果广播回所有进程。通过将张量划分为较小的桶，可以更高效地传输数据。reduce_bucket_size值越大，每个桶的大小越大，通信操作可能会变得更快，但同时也需要更多的内存来存储中间结果。合适的桶大小需要根据实际情况进行调整。</li>
<li>overlap_comm使用的是allgather_bucket_size和reduce_bucket_size值的4.5倍。如果它们被设置为5e8，需要9GB显存（5e8 x 2Bytes x 2 x 4.5）。如果内存大小是8GB或更小，需要将这些参数减少到约2e8，从而避免OOM，这需要3.6GB显存。如果在大容量GPU上也出现OOM，也需要做同样的调整。</li>
<li>在deepspeed==0.4.4中新增了 round_robin_gradients 选项，可以并行化CPU的offload。当梯度累积的步数增加，或者GPU数量增加时，会有更好的性能优势。</li>
</ul>
<h3 id="Zero-3"><a href="#Zero-3" class="headerlink" title="Zero-3"></a>Zero-3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 3,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;offload_param&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true,</span><br><span class="line">        &quot;sub_group_size&quot;: 1e9,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">        &quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">        &quot;stage3_gather_16bit_weights_on_model_save&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 2000,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>stage3_max_live_parameters 是保留在 GPU 上的完整参数数量的上限。</li>
<li>stage3_max_reuse_distance 是指将来何时再次使用参数的指标，从而决定是丢弃参数还是保留参数。 如果一个参数在不久的将来要再次使用（小于 stage3_max_reuse_distance），可以保留以减少通信开销。 使用activation checkpointing时，这一点非常有用。</li>
<li>如果遇到 OOM，可以减少 stage3_max_live_parameters 和 stage3_max_reuse_distance。 除非正在使用activation checkpointing，否则它们对性能的影响应该很小。 1e9 会消耗 ~2GB。 内存由 stage3_max_live_parameters 和 stage3_max_reuse_distance 共享，所以不是相加的，一共 2GB。</li>
<li>stage3_gather_16bit_weights_on_model_save 在保存模型时启用模型 fp16 权重合并。 对大型模型和多GPU，在内存和速度方面都是一项昂贵的操作。 如果打算恢复训练，目前需要使用它。 未来的更新将消除此限制。</li>
<li>sub_group_size 控制在optimizer steps中更新参数的粒度。 参数被分组到 sub_group_size 的桶中，每个桶一次更新一个。 当与 ZeRO-Infinity 中的 NVMe offload一起使用时，sub_group_size 控制模型状态在optimizer steps期间从 NVMe 移入和移出 CPU 内存的粒度。 防止超大模型耗尽 CPU 内存。不使用NVMe offload时，使其保持默认值。出现OOM时，减小sub_group_size。当优化器迭代很慢时，可以增大sub_group_size 。</li>
<li>ZeRO-3 中未使用 allgather_partitions、allgather_bucket_size 和 reduce_scatter 配置参数<h3 id="训练文件加载配置"><a href="#训练文件加载配置" class="headerlink" title="训练文件加载配置"></a>训练文件加载配置</h3>train.py <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 加载配置文件</span><br><span class="line">with open(args.deepspeed, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fh:</span><br><span class="line">    ds_config &#x3D; json.load(fh)</span><br><span class="line"># init deepspeed</span><br><span class="line">model, optimizer, _, lr_scheduler &#x3D; deepspeed.initialize(model&#x3D;model, args&#x3D;args, config&#x3D;ds_config, dist_init_required&#x3D;True)</span><br></pre></td></tr></table></figure>
<h3 id="调参步骤"><a href="#调参步骤" class="headerlink" title="调参步骤"></a>调参步骤</h3>原则： 先保证显存够用，再保证速度够快<br>大致的顺序是先尝试stage0-&gt;stage3；如果还不够，开启offload到cpu；降低一些参数设置，使用混合精度训练。<br><strong>参考设置</strong><br>如果训模型from scratch，hidden size最好可以被16整除<br>batch size最好可以被2整除<br><strong>参考步骤：</strong></li>
</ul>
<ol>
<li>将batch_size设置为1，通过梯度累积实现任意的有效batch_size</li>
<li>如果OOM则，设置—gradient_checkpointing 1 (HF Trainer)，或者 model.gradient_checkpointing_enable()</li>
<li>如果OOM则，尝试ZeRO stage 2</li>
<li>如果OOM则，尝试ZeRO stage 2 + offload_optimizer</li>
<li>如果OOM则，尝试ZeRO stage 3</li>
<li>如果OOM则，尝试offload_param到CPU</li>
<li>如果OOM则，尝试offload_optimizer到CPU</li>
<li>如果OOM则，尝试降低一些默认参数。比如使用generate时，减小beam search的搜索范围</li>
<li>如果OOM则，使用混合精度训练，在Ampere的GPU上使用bf16，在旧版本GPU上使用fp16</li>
<li>如果仍然OOM，则使用ZeRO-Infinity ，使用offload_param和offload_optimizer到NVME</li>
<li>一旦使用batch_size=1时，没有导致OOM，测量此时的有效吞吐量，然后尽可能增大batch_size</li>
<li>开始优化参数，可以关闭offload参数，或者降低ZeRO stage，然后调整batch_size，然后继续测量吞吐量，直到性能比较满意（调参可以增加66%的性能）</li>
</ol>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://tlylft.github.io/LLM/08_LLM_parallelism/" title="LLM（8）分布式并行训练" target="_blank" rel="external">https://tlylft.github.io/LLM/08_LLM_parallelism/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tlylft" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tlylft" target="_blank"><span class="text-dark">Icey</span><small class="ml-1x">小傻瓜,别回头</small></a></h3>
        <div>以梦为马，随处可栖。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/LLM/09_prompt_learning/" title="LLM（9）prompt learning提示学习"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/LLM/transformer/GPT/" title="【transformer】 03-GPT系列 微调到prompt"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'eL0FL59XsOPmrzo5OTb0zhgE-gzGzoHsz',
    appKey: '53Yf97Qf3bKKWC81Bh4EM9oY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>