<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>LLM（1）-大语言模型简介 | 小傻瓜别回头</title>
  <meta name="description" content="大模型选型的一点思考ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT  1. LLM大语言模型（LLM）是指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。大语言模型可以处理多种自然语言任务，如文本分类、问答、对话等，是通向人工智能的一条重要途径。 现在是2023年5月，截止目前，网络上已经开源了众多的LLM，如何用较低的成">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM（1）-大语言模型简介">
<meta property="og:url" content="https://tlylft.github.io/LLM/01_intro/index.html">
<meta property="og:site_name" content="灵魂都失控">
<meta property="og:description" content="大模型选型的一点思考ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT  1. LLM大语言模型（LLM）是指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。大语言模型可以处理多种自然语言任务，如文本分类、问答、对话等，是通向人工智能的一条重要途径。 现在是2023年5月，截止目前，网络上已经开源了众多的LLM，如何用较低的成">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tlylft.github.io/LLM/01_intro/2023-06-18-14-08-39.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/01_intro/2023-05-21-21-01-19.png">
<meta property="og:image" content="https://tlylft.github.io/LLM/01_intro/2023-05-20-22-33-35.png">
<meta property="article:published_time" content="2023-05-16T00:52:19.000Z">
<meta property="article:modified_time" content="2023-08-10T13:49:32.556Z">
<meta property="article:author" content="Icey Liu">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tlylft.github.io/LLM/01_intro/2023-06-18-14-08-39.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://tlylft.github.io/LLM/01_intro/index.html">
  
    <link rel="alternate" href="/atom.xml" title="灵魂都失控" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/avatar.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 4.2.1"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tlylft" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Icey</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">小傻瓜,别回头</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hebei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>Welcome!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/">LLM</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/transformer/">transformer</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leecode/">Leecode</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">62</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/NER/">NER</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/algorithm/">algorithm</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/bert/">bert</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/corrector/">corrector</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/label/">label</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/segment/">segment</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/sensitive/">sensitive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/similarity/">similarity</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/word2vec/">word2vec</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">信息提取</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/">cv</a><span class="category-list-count">24</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/cv/pointcloud/">pointcloud</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/segmentation/">segmentation</a><span class="category-list-count">11</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/knowledge-graph/">knowledge graph</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/autoML/">autoML</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/time-series/">time series</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">53</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/Django/">Django</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/Flask/">Flask</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/keras/">keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/pandas/">pandas</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python/">python</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</a><span class="category-list-count">8</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/">tools</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/tools/ftp/">ftp</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/kubernetes/">kubernetes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/paper/">paper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/">产品经理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/">大数据技术</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/hive/">hive</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/kafka/">kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/mysql/">mysql</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">对话系统</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><span class="category-list-count">13</span></li></ul>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Anaconda/" style="font-size: 13px;">Anaconda</a> <a href="/tags/Django/" style="font-size: 13.14px;">Django</a> <a href="/tags/Docker/" style="font-size: 13px;">Docker</a> <a href="/tags/Flask/" style="font-size: 13.57px;">Flask</a> <a href="/tags/LLM/" style="font-size: 13.57px;">LLM</a> <a href="/tags/Leecode/" style="font-size: 13.14px;">Leecode</a> <a href="/tags/NER/" style="font-size: 13.36px;">NER</a> <a href="/tags/NLP/" style="font-size: 13.93px;">NLP</a> <a href="/tags/RASA/" style="font-size: 13.71px;">RASA</a> <a href="/tags/algorithm/" style="font-size: 13.29px;">algorithm</a> <a href="/tags/anaconda/" style="font-size: 13px;">anaconda</a> <a href="/tags/autoML/" style="font-size: 13px;">autoML</a> <a href="/tags/bert/" style="font-size: 13.64px;">bert</a> <a href="/tags/bert4keras/" style="font-size: 13.14px;">bert4keras</a> <a href="/tags/blog/" style="font-size: 13px;">blog</a> <a href="/tags/corrector/" style="font-size: 13.21px;">corrector</a> <a href="/tags/cv/" style="font-size: 13.79px;">cv</a> <a href="/tags/deep-learning/" style="font-size: 13.86px;">deep learning</a> <a href="/tags/doccano/" style="font-size: 13px;">doccano</a> <a href="/tags/docker/" style="font-size: 13.07px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/github/" style="font-size: 13px;">github</a> <a href="/tags/hive/" style="font-size: 13.29px;">hive</a> <a href="/tags/k8s/" style="font-size: 13px;">k8s</a> <a href="/tags/kafka/" style="font-size: 13px;">kafka</a> <a href="/tags/keras/" style="font-size: 13px;">keras</a> <a href="/tags/kerberos/" style="font-size: 13px;">kerberos</a> <a href="/tags/knowledge-graph/" style="font-size: 13.5px;">knowledge graph</a> <a href="/tags/kubeflow/" style="font-size: 13px;">kubeflow</a> <a href="/tags/kubernetes/" style="font-size: 13px;">kubernetes</a> <a href="/tags/label/" style="font-size: 13.07px;">label</a> <a href="/tags/linux/" style="font-size: 13.5px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 13.86px;">machine learning</a> <a href="/tags/math/" style="font-size: 13.14px;">math</a> <a href="/tags/matplotlib/" style="font-size: 13px;">matplotlib</a> <a href="/tags/mysql/" style="font-size: 13.07px;">mysql</a> <a href="/tags/navicat/" style="font-size: 13px;">navicat</a> <a href="/tags/neo4j/" style="font-size: 13.14px;">neo4j</a> <a href="/tags/nlp/" style="font-size: 13px;">nlp</a> <a href="/tags/numpy/" style="font-size: 13px;">numpy</a> <a href="/tags/paddle/" style="font-size: 13px;">paddle</a> <a href="/tags/pandas/" style="font-size: 13.21px;">pandas</a> <a href="/tags/pointcloud/" style="font-size: 13.07px;">pointcloud</a> <a href="/tags/ppt/" style="font-size: 13px;">ppt</a> <a href="/tags/pycharm/" style="font-size: 13.07px;">pycharm</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/pytorch/" style="font-size: 13.21px;">pytorch</a> <a href="/tags/segment/" style="font-size: 13.07px;">segment</a> <a href="/tags/segmentation/" style="font-size: 13.64px;">segmentation</a> <a href="/tags/sensitive/" style="font-size: 13.07px;">sensitive</a> <a href="/tags/similarity/" style="font-size: 13.43px;">similarity</a> <a href="/tags/tensorflow/" style="font-size: 13.07px;">tensorflow</a> <a href="/tags/time-series/" style="font-size: 13.07px;">time series</a> <a href="/tags/tools/" style="font-size: 13.64px;">tools</a> <a href="/tags/vp/" style="font-size: 13px;">vp</a> <a href="/tags/word2vec/" style="font-size: 13.36px;">word2vec</a> <a href="/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" style="font-size: 13px;">产品经理</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/" style="font-size: 13.14px;">信息提取</a> <a href="/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" style="font-size: 13.86px;">对话系统</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.14px;">强化学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 13.07px;">爬虫</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 13.5px;">环境搭建</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 13.36px;">计算机视觉</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 13.64px;">读书笔记</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">24</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-LLM/01_intro" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      LLM（1）-大语言模型简介
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/LLM/01_intro/" class="article-date">
	  <time datetime="2023-05-16T00:52:19.000Z" itemprop="datePublished">创建于: 2023-05-16</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/LLM/">LLM</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/LLM/" rel="tag">LLM</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/LLM/01_intro/" class="leancloud_visitors"  data-flag-title="LLM（1）-大语言模型简介">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/LLM/01_intro/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <blockquote>
<p><a href="https://mp.weixin.qq.com/s/NOcUlNPOXZheXJ9yX4wrLQ" target="_blank" rel="noopener">大模型选型的一点思考</a><br><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener">ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT</a></p>
</blockquote>
<h1 id="1-LLM"><a href="#1-LLM" class="headerlink" title="1. LLM"></a>1. LLM</h1><p>大语言模型（LLM）是指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。大语言模型可以处理多种自然语言任务，如文本分类、问答、对话等，是通向人工智能的一条重要途径。</p>
<p>现在是2023年5月，截止目前，网络上已经开源了众多的LLM，如何用较低的成本，判断LLM的基础性能，选到适合自己任务的LLM，成为一个关键。</p>
<h1 id="2-模型发展历史"><a href="#2-模型发展历史" class="headerlink" title="2. 模型发展历史"></a>2. 模型发展历史</h1><div class="table-container">
<table>
<thead>
<tr>
<th>时间</th>
<th>公司</th>
<th>模型</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017</td>
<td></td>
<td>微积分、概率统计、最优化、策略梯度、TRPO算法(2015年提出)</td>
<td></td>
</tr>
<tr>
<td>2017年6月</td>
<td>OpenAI联合DeepMind</td>
<td>RLHF</td>
<td>Deep Reinforcement Learning from Human Preferences，即基于人类偏好的深度强化学习</td>
</tr>
<tr>
<td>2017年6月</td>
<td></td>
<td><a href="https://blog.csdn.net/v_JULY_v/article/details/127411638" target="_blank" rel="noopener">Transformer</a></td>
<td>self-attention</td>
</tr>
<tr>
<td>2017年7月</td>
<td>OpenAI</td>
<td><a href="https://blog.csdn.net/v_JULY_v/article/details/128965854" target="_blank" rel="noopener">PPO算法</a></td>
<td>TRPO算法的改进</td>
</tr>
<tr>
<td>2018</td>
<td>Google</td>
<td>BERT</td>
<td></td>
</tr>
<tr>
<td>2018.07</td>
<td>OpenAI</td>
<td>GPT(Generative Pre-trained Transformer)</td>
<td>基于Transformer-Decoder的Masked Self-Attention</td>
</tr>
<tr>
<td>2019.02</td>
<td>OpenAI</td>
<td>GPT2</td>
<td>融合prompt learning的GPT2，prompt learning的意义在于不用微调也能做任务</td>
</tr>
<tr>
<td>2019年10月</td>
<td>Google</td>
<td>T5(transfer text to text transformer)</td>
<td>基于transformer的encoder-decoder架构，区别于BERT的编码器架构与GPT的解码器架构</td>
</tr>
<tr>
<td>2020年05月</td>
<td>OpenAI</td>
<td>GPT3.0</td>
<td>参数规模到了1750亿，终于真正做到预训练之后<strong>不用再微调模式</strong>，通过In-context learning(简称ICL)开启prompt新范式</td>
</tr>
<tr>
<td>2021年7月</td>
<td>OpenAI</td>
<td>Codex</td>
<td>通过对GPT3进行大量的代码训练迭代而出Codex，从而具备代码/推理能力 159G的python代码微调</td>
</tr>
<tr>
<td>2021年9月</td>
<td>Google</td>
<td>FLAN</td>
<td>基于指令微调技术Instruction Fine-Tuning (IFT)的大模型</td>
</tr>
<tr>
<td>2021年第四季度</td>
<td>OpenAI</td>
<td><strong>GPT3.5</strong></td>
<td>更大的数据量+context learning，用2021年之前的数据进行的训练</td>
</tr>
<tr>
<td>2022年1月</td>
<td>Google</td>
<td>CoT</td>
<td>研究者提出的思维链技术(Chain of Thought，简称CoT) ，<strong>模仿推理能力</strong></td>
</tr>
<tr>
<td>2022年3月</td>
<td>OpenAI</td>
<td>instructGPT</td>
<td>GPT3 + instruction tuning + RLHF + PPO</td>
</tr>
<tr>
<td>2022年11月</td>
<td>OpenAI-</td>
<td><strong>ChatGPT</strong></td>
<td>语言模型层面的核心架构是GPT3.5(基于Transformer-Decoder的Masked Self-Attention且融合了Codex的代码/推理能力、instruction tuning等技术) + RLHF + PPO3</td>
</tr>
<tr>
<td>2022年1月</td>
<td>Google</td>
<td>LaMDA</td>
<td>发布LaMDA论文『 LaMDA: Language Models for Dialog Applications』</td>
</tr>
<tr>
<td>2022年4月</td>
<td>Google</td>
<td>PaLM</td>
<td>提出PaLM: Scaling Language Modeling with Pathways，5400亿参数</td>
</tr>
<tr>
<td>2022年10月</td>
<td>Google</td>
<td>Flan-T5</td>
<td>提出Flan-T5</td>
</tr>
<tr>
<td>23年3月6日</td>
<td>Google</td>
<td>PaLM-E</td>
<td>提出多模态LLM模型</td>
</tr>
<tr>
<td>2023.02.24</td>
<td>Facebook</td>
<td>Meta llama</td>
<td>基础模型</td>
</tr>
<tr>
<td>2023.03.15</td>
<td>OpenAI</td>
<td><strong>GPT4.0</strong></td>
<td>增加了多模态(支持图片的输入形式)</td>
</tr>
<tr>
<td>2023.03.16</td>
<td>百度</td>
<td><strong>文心一言</strong></td>
<td></td>
</tr>
<tr>
<td>2023.03.17</td>
<td>微软</td>
<td>Microsoft 365 Copilot</td>
<td>集成GPT4的能力，实现自动化办公</td>
</tr>
<tr>
<td>2023.03.22</td>
<td>Google</td>
<td>Bard</td>
<td></td>
</tr>
<tr>
<td>2023.03.23</td>
<td>Github</td>
<td>Copilot X</td>
<td></td>
</tr>
<tr>
<td>2023.03.24</td>
<td>OpenAI</td>
<td>-</td>
<td>插件功能，赋予chatgpt使用工具、联网、运算的能力</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-1-GPT3：In-context-learning正式开启prompt新范式-小样本学习"><a href="#2-1-GPT3：In-context-learning正式开启prompt新范式-小样本学习" class="headerlink" title="2.1. GPT3：In-context learning正式开启prompt新范式(小样本学习)"></a>2.1. GPT3：In-context learning正式开启prompt新范式(小样本学习)</h3><p> GPT3在0样本、单样本、小样本下的突出能力<br>GPT3简单来说，就是参数规模大(有钱)、训练数据规模大(多金)、效果出奇好，具体而言<br>为形象描述，举一个GPT3在0样本、单样本、少量样本下的机器翻译使用范例，如下图<br><img src="/LLM/01_intro/2023-06-18-14-08-39.png" alt></p>
<h3 id="2-2-Prompt技术的升级与创新：指令微调技术-IFT-与思维链技术-CoT"><a href="#2-2-Prompt技术的升级与创新：指令微调技术-IFT-与思维链技术-CoT" class="headerlink" title="2.2. Prompt技术的升级与创新：指令微调技术(IFT)与思维链技术(CoT)"></a>2.2. Prompt技术的升级与创新：指令微调技术(IFT)与思维链技术(CoT)</h3><p>OpenAI的GPT3虽然不再微调模型(pre-training + prompt)，但Google依然坚持预训练 + 微调的模式<br>Google提出FLAN大模型：基于指令微调技术Instruction Fine-Tuning (IFT)<br>基于思维链(Chain-of-thought)技术下的prompt</p>
<p>因此导致后续OpenAI的chatgpt不得不关注再次关注微调技术，也转向指令微调</p>
<h1 id="3-开源模型-截止至2023-06-08"><a href="#3-开源模型-截止至2023-06-08" class="headerlink" title="3. 开源模型 截止至2023/06/08"></a>3. <a href="https://blog.csdn.net/zengNLP/article/details/131119734" target="_blank" rel="noopener">开源模型 截止至2023/06/08</a></h1><blockquote>
<p><a href="https://huaweidevelopers.csdn.net/64c1290ebfca273ff3548e81.html" target="_blank" rel="noopener">https://huaweidevelopers.csdn.net/64c1290ebfca273ff3548e81.html</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>time</th>
<th>model</th>
<th>github</th>
<th>owner</th>
<th>language</th>
<th>desc</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><a href="https://arxiv.org/abs/2302.13971v1" target="_blank" rel="noopener">LLaMA</a></td>
<td><a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener">代码</a></td>
<td>meta</td>
<td>英</td>
<td>在模型参数量降低的同时，增加训练的数据量，这样可以保证模型的效果。LLaMA完全是在公共开源预训练数据上训练。并且取得相当不错的效果，LaMA-13B在绝大部分的benchmarks上超越了GPT-3(175 B)，并且LLaMA-65B的效果能够和最好的大模型，Chinchilla-70B以及PaLM-540B相比。</td>
</tr>
<tr>
<td></td>
<td>[Chinchilla]</td>
<td></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td><a href="https://chatglm.cn/blog" target="_blank" rel="noopener">chatGLM-6B</a></td>
<td><a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">代码</a></td>
<td>清华</td>
<td>中英</td>
<td>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。</td>
</tr>
<tr>
<td></td>
<td><a href="https://arxiv.org/abs/2212.10560" target="_blank" rel="noopener">Alpaca</a></td>
<td><a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" rel="noopener">代码</a></td>
<td>斯坦福</td>
<td>英</td>
<td>一个遵循指令的LLaMA模型</td>
</tr>
<tr>
<td></td>
<td><a href="https://arxiv.org/pdf/2305.03025v1.pdf" target="_blank" rel="noopener">PandaLLM</a></td>
<td><a href="https://github.com/dandelionsllm/pandallm" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>中英</td>
<td>海外中文开源大语言模型：Panda 系列语言模型目前基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练, 使用了接近 15M 条数据, 并针对推理能力在中文 benchmark 上进行了评测, 希望能够为中文自然语言处理领域提供具有泛用性的通用基础工具.</td>
</tr>
<tr>
<td></td>
<td>[GPT4ALL]</td>
<td><a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>可在cpu上运行的开源LLM</td>
</tr>
<tr>
<td></td>
<td><a href="https://arxiv.org/pdf/2304.01097.pdf" target="_blank" rel="noopener">DoctorGLM (MedicalGPT-zh v2)</a></td>
<td><a href="https://github.com/xionghonglin/DoctorGLM" target="_blank" rel="noopener">代码</a> <a href="https://huggingface.co/zhaozh/medical_chat-en-zh" target="_blank" rel="noopener">huggingface</a></td>
<td>xxx</td>
<td>中英医疗</td>
<td>基于 ChatGLM-6B的中文问诊模型</td>
</tr>
<tr>
<td></td>
<td><a href="https://arxiv.org/pdf/2304.01097.pdf" target="_blank" rel="noopener">MedicalGPT-zh v1</a></td>
<td><a href="https://github.com/MediaBrain-SJTU/MedicalGPT-zh" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>中英医疗</td>
<td>基于ChatGLM-6B LoRA 16-bit指令微调的中文医疗通用模型。基于共计28科室的中文医疗共识与临床指南文本，我们生成医疗知识覆盖面更全，回答内容更加精准的高质量指令数据集。以此提高模型在医疗领域的知识与对话能力。</td>
</tr>
<tr>
<td></td>
<td>[Cornucopia-LLaMA-Fin-Chinese]</td>
<td><a href="https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese/tree/main" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>中文金融</td>
<td>基于中文金融知识的LLaMA微调模型：经过中文金融知识指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过中文金融公开数据+爬取的金融数据构建指令数据集，并在此基础上对LLaMA进行了指令微调，提高了 LLaMA 在金融领域的问答效果。</td>
</tr>
<tr>
<td></td>
<td>[minGPT]</td>
<td><a href="https://github.com/karpathy/minGPT" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td>[InstructGLM]</td>
<td><a href="https://github.com/yanqiangmiffy/InstructGLM" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>中英指令</td>
<td>基于ChatGLM-6B+LoRA在指令数据集上进行微调。</td>
</tr>
<tr>
<td></td>
<td>[FastChat]</td>
<td><a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td>[Luotuo-Chinese-LLM]</td>
<td><a href="https://github.com/LC1332/Luotuo-Chinese-LLM" target="_blank" rel="noopener">代码</a></td>
<td>商汤&amp;华中师范</td>
<td>中文</td>
<td>开源中文大语言模型</td>
</tr>
<tr>
<td></td>
<td>[CamelBell-Chinese-LoRA]</td>
<td><a href="https://github.com/LC1332/CamelBell-Chinese-LoRA" target="_blank" rel="noopener">代码</a></td>
<td>商汤&amp;华中师范</td>
<td>中文</td>
<td>开源中文大语言模型</td>
</tr>
<tr>
<td></td>
<td>[alpaca-lora]</td>
<td><a href="https://github.com/tloen/alpaca-lora" target="_blank" rel="noopener">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td>2023.06</td>
<td><a href="xxx">chatGLM2</a></td>
<td><a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank" rel="noopener">代码</a> <a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener">huggingface</a></td>
<td>清华</td>
<td>中英</td>
<td>1.基座模型升级，性能更强大 2. 支持8K-32k的上下文 3. 推理性能提升了42%</td>
</tr>
<tr>
<td></td>
<td><a href>baichuan</a></td>
<td><a href="xxx">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td><a href="xxx">LLama-2</a></td>
<td><a href="xxx">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td><a href="xxx"></a></td>
<td><a href="xxx">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
<tr>
<td></td>
<td><a href="xxx"></a></td>
<td><a href="xxx">代码</a></td>
<td>xxx</td>
<td>xxx</td>
<td>xxx</td>
</tr>
</tbody>
</table>
</div>
<h1 id="4-多模态"><a href="#4-多模态" class="headerlink" title="4. 多模态"></a>4. 多模态</h1><h3 id="4-1-LLaVA"><a href="#4-1-LLaVA" class="headerlink" title="4.1. LLaVA"></a>4.1. LLaVA</h3><h3 id="4-2-MiniGPT-4"><a href="#4-2-MiniGPT-4" class="headerlink" title="4.2. MiniGPT-4"></a>4.2. MiniGPT-4</h3><h3 id="4-3-KOSMOS-1-微软"><a href="#4-3-KOSMOS-1-微软" class="headerlink" title="4.3. KOSMOS-1 微软"></a>4.3. KOSMOS-1 微软</h3><p>《Language Is Not All You Need: Aligning Perception with Language Models》<br>论文地址：<a href="https://arxiv.org/pdf/2302.14045.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2302.14045.pdf</a></p>
<p>项目地址：<a href="https://github.com/microsoft/unilm" target="_blank" rel="noopener">https://github.com/microsoft/unilm</a><br><a href="https://www.thepaper.cn/newsDetail_forward_22122932?commTag=true" target="_blank" rel="noopener">https://www.thepaper.cn/newsDetail_forward_22122932?commTag=true</a></p>
<h3 id="4-4-X-LLM-中文多模态大模型"><a href="#4-4-X-LLM-中文多模态大模型" class="headerlink" title="4.4. X-LLM 中文多模态大模型"></a>4.4. X-LLM 中文多模态大模型</h3><p>简介：中文多模态大模型力作，中科院发布多模态 ChatGPT，图片、语言、视频都可以 Chat<br>来源：中科院 X-LLM<br>Paper: <a href="https://arxiv.org/pdf/2305.04160.pdf" target="_blank" rel="noopener">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a><br>主页：<a href="https://x-llm.github.io/" target="_blank" rel="noopener">https://x-llm.github.io/</a><br>思路：</p>
<ol>
<li>假设“GPT-4 的多模态能力来源于其更先进，更大的语音模型，即 GPT-4 是用语言的形式表达出了其他模态的内容”</li>
</ol>
<p>这个假设也就是讲，<strong>需要将多模态的数据“对齐”到语言数据之中，然后再投入大模型以获得多模态能力</strong>， 在这个假设的基础上，作者提出了 X2L 接口，其中 X 意味着多模态数据，而 L 则表示语言，<strong>X2L 接口即将多个单模态编码器与一个大规模语言模型（LLM）进行对齐</strong>。其中，图像接口 I2L 采用 BLIP-2 中的 Q-Former，视频接口 V2L 复用图像接口的参数，但是考虑了编码后的视频特征，语言接口 S2L 采用 CIF 与 Transformer 结构将语音转换为语言。整个 X-LLM 的训练包含三个阶段，分别是（1）转换多模态信息；（2）将 X2L 对齐到 LLM；（3）将多模态数据整合到 LLM 中。<br><img src="/LLM/01_intro/2023-05-21-21-01-19.png" alt></p>
<h1 id="5-主流模型参数性能对比"><a href="#5-主流模型参数性能对比" class="headerlink" title="5. 主流模型参数性能对比"></a>5. 主流模型参数性能对比</h1><p>1B = 10亿</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>model</th>
<th>Parameters</th>
<th>Layers</th>
<th>Attention heads</th>
<th>Embedding dimension</th>
<th>gpu</th>
<th>train data</th>
</tr>
</thead>
<tbody>
<tr>
<td>transformer</td>
<td>-</td>
<td>-</td>
<td>8</td>
<td>8*64=512</td>
<td>-</td>
</tr>
<tr>
<td>GPT-1</td>
<td>1.17亿</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-2</td>
<td>1.5B</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>GPT-3</td>
<td>175B</td>
<td>96</td>
<td>96</td>
<td>96*128=12888</td>
<td>-</td>
<td>45T</td>
</tr>
<tr>
<td>LLaMA 7B</td>
<td>6.7B</td>
<td>32</td>
<td>32</td>
<td>4,096</td>
<td>-</td>
</tr>
<tr>
<td>LLaMA 13B</td>
<td>13B</td>
<td>40</td>
<td>40</td>
<td>5,120</td>
<td>1 V100</td>
</tr>
<tr>
<td>LLaMA 33B</td>
<td>33B</td>
<td>60</td>
<td>52</td>
<td>6,656</td>
</tr>
<tr>
<td>LLaMA 65B</td>
<td>65B</td>
<td>80</td>
<td>64</td>
<td>8,192</td>
</tr>
<tr>
<td>chatgpt</td>
<td>可能175B</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>5个80GB的A100加载</td>
<td></td>
</tr>
<tr>
<td>baidu文心</td>
<td>2600亿</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td></td>
</tr>
<tr>
<td>GPT-4</td>
<td>100000B</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>KOSMOS-1（微软多模态）</td>
<td>1.6B</td>
<td>-</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
</tbody>
</table>
</div>
<h1 id="6-影响LLM性能的主要因素"><a href="#6-影响LLM性能的主要因素" class="headerlink" title="6. 影响LLM性能的主要因素"></a>6. 影响LLM性能的主要因素</h1><p>openAI 论文：Scaling Laws for Neural Language Models<br><img src="/LLM/01_intro/2023-05-20-22-33-35.png" alt><br>OpenAI的论文Scaling Laws中列举了影响模型性能最大的三个因素：<strong>计算量、数据集大小、模型参数量</strong>。也就是说，当其他因素不成为瓶颈时，计算量、数据集大小、模型参数量这3个因素中的单个因素指数增加时，loss会线性的下降。同时，DeepMind的研究也得出来和OpenAI类似的结论。那么我们可以基本确定，如果一个模型在这3个方面，均做的不错，那么将会是一个很好的备选。<br>模型参数量是我们最容易注意到的，一般而言，LLM也只在训练数据上训练1个epoch（如果还有算力，其实可以扩更多的新数据），那么，数据集的大小就是很关键的参数。训练OPT-175B的Susan Zhang在Stanford分享的时候，也提到了，如果能够重新再来一次，她会选择much much more data。可见数据量的重要性。</p>
<p>了解到Scaling Laws之后，<strong>为了降低模型的推理成本，可以在模型参数量降低的同时，增加训练的数据量，这样可以保证模型的效果。Chinchilla和LLaMA就是这样的思路。</strong></p>
<p>除了以上的因素之外，还有一个比较大的影响因素就是数据质量。</p>
<h1 id="7-一些概念"><a href="#7-一些概念" class="headerlink" title="7. 一些概念"></a>7. 一些概念</h1><h2 id="7-1-预训练和微调的区别"><a href="#7-1-预训练和微调的区别" class="headerlink" title="7.1. 预训练和微调的区别"></a>7.1. 预训练和微调的区别</h2><p>直观体现在数据集体量， 很大规模的训练集训练出的模型可称为预训练模型。<br>预训练： 首先在一个大规模的数据集上训练一个深度学习模型，例如使用自监督学习或者无监督学习算法进行预训练；<br>微调： 使用目标任务的训练集对预训练模型进行微调。通常，只有预训练模型中的一部分层被微调，例如只微调模型的最后几层或者某些中间层。在微调过程中，通过反向传播算法对模型进行优化，使得模型在目标任务上表现更好；也可进行参数全量微调。<br>评估： 使用目标任务的测试集对微调后的模型进行评估，得到模型在目标任务上的性能指标。</p>
<h2 id="7-2-instruct和promot的区别"><a href="#7-2-instruct和promot的区别" class="headerlink" title="7.2. instruct和promot的区别"></a>7.2. instruct和promot的区别</h2><h2 id="指令微调是什么"><a href="#指令微调是什么" class="headerlink" title="指令微调是什么"></a>指令微调是什么</h2>
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://tlylft.github.io/LLM/01_intro/" title="LLM（1）-大语言模型简介" target="_blank" rel="external">https://tlylft.github.io/LLM/01_intro/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tlylft" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tlylft" target="_blank"><span class="text-dark">Icey</span><small class="ml-1x">小傻瓜,别回头</small></a></h3>
        <div>以梦为马，随处可栖。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/machine_learning/time_series/nerualprophet/" title="Neural Prophet"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/LLM/06_bard/" title="LLM（6）-BARD"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'eL0FL59XsOPmrzo5OTb0zhgE-gzGzoHsz',
    appKey: '53Yf97Qf3bKKWC81Bh4EM9oY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>