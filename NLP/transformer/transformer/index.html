<!DOCTYPE html>
<html lang=zh>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>【transformer】 01-Transformer | 小傻瓜别回头</title>
  <meta name="description" content="Transformer自从Google在2017年此文《Attention is All You Need》提出来Transformer后，便开启了大规模预训练的新时代，也在历史的长河中一举催生出了BERT这样的大一统模型。 1. basic knowledge1.1. Seq2Seq 网络输入一个序列 输出一个序列，且长度可变。网络结构的提出主要用于机器翻译。输入输出很直观。 序列经过编码形成编">
<meta property="og:type" content="article">
<meta property="og:title" content="【transformer】 01-Transformer">
<meta property="og:url" content="https://tlylft.github.io/NLP/transformer/transformer/index.html">
<meta property="og:site_name" content="灵魂都失控">
<meta property="og:description" content="Transformer自从Google在2017年此文《Attention is All You Need》提出来Transformer后，便开启了大规模预训练的新时代，也在历史的长河中一举催生出了BERT这样的大一统模型。 1. basic knowledge1.1. Seq2Seq 网络输入一个序列 输出一个序列，且长度可变。网络结构的提出主要用于机器翻译。输入输出很直观。 序列经过编码形成编">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-09-59-05.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/7de837145a4b5f801bc7dbd3f79ed858.gif">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2023-08-12-18-20-37.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-09-59-25.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-10-09-16.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-10-11-54.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-10-13-52.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2023-08-12-21-09-40.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-08-39-09.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2023-08-12-20-56-52.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-09-19-31.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-09-41-26.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-09-51-44.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-11-05-19.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-11-07-42.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-08-44-24.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-08-06-11-30-13.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-08-57-13.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-09-00-36.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-09-02-40.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-12-50-49.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-12-52-27.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-13-04-20.png">
<meta property="og:image" content="https://tlylft.github.io/NLP/transformer/transformer/2021-10-19-13-35-11.png">
<meta property="article:published_time" content="2020-04-01T16:00:00.000Z">
<meta property="article:modified_time" content="2023-08-13T13:13:15.279Z">
<meta property="article:author" content="Icey Liu">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tlylft.github.io/NLP/transformer/transformer/2020-07-02-09-59-05.png">
  <!-- Canonical links -->
  <link rel="canonical" href="https://tlylft.github.io/NLP/transformer/transformer/index.html">
  
    <link rel="alternate" href="/atom.xml" title="灵魂都失控" type="application/atom+xml">
  
  
    <link rel="icon" href="/images/avatar.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 4.2.1"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="https://github.com/tlylft" target="_blank">
          <img class="img-circle img-rotate" src="/images/avatar.png" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">Icey</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">小傻瓜,别回头</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Hebei, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="搜索" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="想要查找什么..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">首页</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">归档</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-categories">
          <a href="/categories">
            
            <i class="icon icon-folder"></i>
            
            <span class="menu-title">分类</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-tags">
          <a href="/tags">
            
            <i class="icon icon-tags"></i>
            
            <span class="menu-title">标签</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">公告</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>Welcome!</p>
            </div>
        </div>
    </div>
</div>

    
      
  <div class="widget">
    <h3 class="widget-title">分类</h3>
    <div class="widget-body">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/">LLM</a><span class="category-list-count">9</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/LLM/transformer/">transformer</a><span class="category-list-count">2</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/Leecode/">Leecode</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a><span class="category-list-count">62</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/NER/">NER</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/algorithm/">algorithm</a><span class="category-list-count">5</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/bert/">bert</a><span class="category-list-count">11</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/corrector/">corrector</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/label/">label</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/segment/">segment</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/sensitive/">sensitive</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/similarity/">similarity</a><span class="category-list-count">7</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/word2vec/">word2vec</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/">信息提取</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/">cv</a><span class="category-list-count">24</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/cv/pointcloud/">pointcloud</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/cv/segmentation/">segmentation</a><span class="category-list-count">11</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/deep-learning/">deep learning</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/knowledge-graph/">knowledge graph</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/">machine learning</a><span class="category-list-count">21</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/autoML/">autoML</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/machine-learning/time-series/">time series</a><span class="category-list-count">4</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/math/">math</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a><span class="category-list-count">53</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/python/Django/">Django</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/Flask/">Flask</a><span class="category-list-count">9</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/keras/">keras</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/matplotlib/">matplotlib</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/pandas/">pandas</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/python/">python</a><span class="category-list-count">12</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/">环境搭建</a><span class="category-list-count">8</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/pytorch/">pytorch</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tensorflow/">tensorflow</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/">tools</a><span class="category-list-count">16</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/tools/ftp/">ftp</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/k8s/">k8s</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/kubernetes/">kubernetes</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/tools/paper/">paper</a><span class="category-list-count">1</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/">产品经理</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%89%8D%E7%AB%AF/">前端</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/">大数据技术</a><span class="category-list-count">8</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/hive/">hive</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/kafka/">kafka</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF/mysql/">mysql</a><span class="category-list-count">3</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/">对话系统</a><span class="category-list-count">17</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><span class="category-list-count">3</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%88%AC%E8%99%AB/">爬虫</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/">计算机视觉</a><span class="category-list-count">6</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a><span class="category-list-count">13</span></li></ul>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">标签云</h3>
    <div class="widget-body tagcloud">
      <a href="/tags/Anaconda/" style="font-size: 13px;">Anaconda</a> <a href="/tags/Django/" style="font-size: 13.14px;">Django</a> <a href="/tags/Docker/" style="font-size: 13px;">Docker</a> <a href="/tags/Flask/" style="font-size: 13.57px;">Flask</a> <a href="/tags/LLM/" style="font-size: 13.57px;">LLM</a> <a href="/tags/Leecode/" style="font-size: 13.14px;">Leecode</a> <a href="/tags/NER/" style="font-size: 13.36px;">NER</a> <a href="/tags/NLP/" style="font-size: 13.93px;">NLP</a> <a href="/tags/RASA/" style="font-size: 13.71px;">RASA</a> <a href="/tags/algorithm/" style="font-size: 13.29px;">algorithm</a> <a href="/tags/anaconda/" style="font-size: 13px;">anaconda</a> <a href="/tags/autoML/" style="font-size: 13px;">autoML</a> <a href="/tags/bert/" style="font-size: 13.64px;">bert</a> <a href="/tags/bert4keras/" style="font-size: 13.14px;">bert4keras</a> <a href="/tags/blog/" style="font-size: 13px;">blog</a> <a href="/tags/corrector/" style="font-size: 13.21px;">corrector</a> <a href="/tags/cv/" style="font-size: 13.79px;">cv</a> <a href="/tags/deep-learning/" style="font-size: 13.86px;">deep learning</a> <a href="/tags/doccano/" style="font-size: 13px;">doccano</a> <a href="/tags/docker/" style="font-size: 13.07px;">docker</a> <a href="/tags/git/" style="font-size: 13px;">git</a> <a href="/tags/github/" style="font-size: 13px;">github</a> <a href="/tags/hive/" style="font-size: 13.29px;">hive</a> <a href="/tags/k8s/" style="font-size: 13px;">k8s</a> <a href="/tags/kafka/" style="font-size: 13px;">kafka</a> <a href="/tags/keras/" style="font-size: 13px;">keras</a> <a href="/tags/kerberos/" style="font-size: 13px;">kerberos</a> <a href="/tags/knowledge-graph/" style="font-size: 13.5px;">knowledge graph</a> <a href="/tags/kubeflow/" style="font-size: 13px;">kubeflow</a> <a href="/tags/kubernetes/" style="font-size: 13px;">kubernetes</a> <a href="/tags/label/" style="font-size: 13.07px;">label</a> <a href="/tags/linux/" style="font-size: 13.5px;">linux</a> <a href="/tags/machine-learning/" style="font-size: 13.86px;">machine learning</a> <a href="/tags/math/" style="font-size: 13.14px;">math</a> <a href="/tags/matplotlib/" style="font-size: 13px;">matplotlib</a> <a href="/tags/mysql/" style="font-size: 13.07px;">mysql</a> <a href="/tags/navicat/" style="font-size: 13px;">navicat</a> <a href="/tags/neo4j/" style="font-size: 13.14px;">neo4j</a> <a href="/tags/nlp/" style="font-size: 13px;">nlp</a> <a href="/tags/numpy/" style="font-size: 13px;">numpy</a> <a href="/tags/paddle/" style="font-size: 13px;">paddle</a> <a href="/tags/pandas/" style="font-size: 13.21px;">pandas</a> <a href="/tags/pointcloud/" style="font-size: 13.07px;">pointcloud</a> <a href="/tags/ppt/" style="font-size: 13px;">ppt</a> <a href="/tags/pycharm/" style="font-size: 13.07px;">pycharm</a> <a href="/tags/python/" style="font-size: 14px;">python</a> <a href="/tags/pytorch/" style="font-size: 13.21px;">pytorch</a> <a href="/tags/segment/" style="font-size: 13.07px;">segment</a> <a href="/tags/segmentation/" style="font-size: 13.64px;">segmentation</a> <a href="/tags/sensitive/" style="font-size: 13.07px;">sensitive</a> <a href="/tags/similarity/" style="font-size: 13.43px;">similarity</a> <a href="/tags/tensorflow/" style="font-size: 13.07px;">tensorflow</a> <a href="/tags/time-series/" style="font-size: 13.07px;">time series</a> <a href="/tags/tools/" style="font-size: 13.64px;">tools</a> <a href="/tags/vp/" style="font-size: 13px;">vp</a> <a href="/tags/word2vec/" style="font-size: 13.36px;">word2vec</a> <a href="/tags/%E4%BA%A7%E5%93%81%E7%BB%8F%E7%90%86/" style="font-size: 13px;">产品经理</a> <a href="/tags/%E4%BF%A1%E6%81%AF%E6%8F%90%E5%8F%96/" style="font-size: 13.14px;">信息提取</a> <a href="/tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/" style="font-size: 13.86px;">对话系统</a> <a href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.14px;">强化学习</a> <a href="/tags/%E7%88%AC%E8%99%AB/" style="font-size: 13.07px;">爬虫</a> <a href="/tags/%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" style="font-size: 13.5px;">环境搭建</a> <a href="/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/" style="font-size: 13.36px;">计算机视觉</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" style="font-size: 13.64px;">读书笔记</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">归档</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/04/">四月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">三月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">十一月 2022</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">六月 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">五月 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/04/">四月 2022</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/03/">三月 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/02/">二月 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/01/">一月 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/12/">十二月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">十一月 2021</a><span class="archive-list-count">20</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/10/">十月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/08/">八月 2021</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/06/">六月 2021</a><span class="archive-list-count">15</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/05/">五月 2021</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/04/">四月 2021</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/03/">三月 2021</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/02/">二月 2021</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/01/">一月 2021</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">十二月 2020</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/11/">十一月 2020</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/10/">十月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/09/">九月 2020</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/08/">八月 2020</a><span class="archive-list-count">17</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/07/">七月 2020</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/06/">六月 2020</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/05/">五月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/04/">四月 2020</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a><span class="archive-list-count">24</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/01/">一月 2020</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/12/">十二月 2019</a><span class="archive-list-count">13</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/11/">十一月 2019</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">一月 2019</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/11/">十一月 2018</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/12/">十二月 2017</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>


    
  </div>
</aside>

  
  
<main class="main" role="main">
  <div class="content">
  <article id="post-NLP/transformer/transformer" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      【transformer】 01-Transformer
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/NLP/transformer/transformer/" class="article-date">
	  <time datetime="2020-04-02T00:36:56.000Z" itemprop="datePublished">创建于: 2020-04-02</time>
	</a>
</span>
        
  <span class="article-category">
    <i class="icon icon-folder"></i>
    <a class="article-category-link" href="/categories/LLM/">LLM</a>►<a class="article-category-link" href="/categories/LLM/transformer/">transformer</a>
  </span>

        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link" href="/tags/LLM/" rel="tag">LLM</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/NLP/transformer/transformer/" class="leancloud_visitors"  data-flag-title="【transformer】 01-Transformer">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/NLP/transformer/transformer/#comments" class="article-comment-link">评论</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p>Transformer<br>自从Google在2017年此文《<a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank" rel="noopener">Attention is All You Need</a>》提出来Transformer后，便开启了大规模预训练的新时代，也在历史的长河中一举催生出了BERT这样的大一统模型。</p>
<h2 id="1-basic-knowledge"><a href="#1-basic-knowledge" class="headerlink" title="1. basic knowledge"></a>1. basic knowledge</h2><h3 id="1-1-Seq2Seq-网络"><a href="#1-1-Seq2Seq-网络" class="headerlink" title="1.1. Seq2Seq 网络"></a>1.1. Seq2Seq 网络</h3><p>输入一个序列 输出一个序列，且长度可变。网络结构的提出主要用于机器翻译。输入输出很直观。</p>
<p><img src="/NLP/transformer/transformer/2020-07-02-09-59-05.png" alt><br><img src="/NLP/transformer/transformer/7de837145a4b5f801bc7dbd3f79ed858.gif" alt><br>序列经过编码形成编码向量，然后解码成输出序列，中间有一个向量C传递信息，且C的长度是固定的。<br><img src="/NLP/transformer/transformer/2023-08-12-18-20-37.png" alt></p>
<h3 id="1-2-RNN-vs-Transformer"><a href="#1-2-RNN-vs-Transformer" class="headerlink" title="1.2. RNN vs Transformer"></a>1.2. RNN vs Transformer</h3><p>RNN网络也算是一种encoder-decoder模型，一般设置为两三层解决问题，不能并行计算， 因为每个下一步都依赖于上一步。<br><img src="/NLP/transformer/transformer/2020-07-02-09-59-25.png" alt></p>
<p>而transformer通过self-attention机制解决RNN不能并行问题，输出结果是同时被计算出来的，目前基本取代RNN<br><img src="/NLP/transformer/transformer/2020-07-02-10-09-16.png" alt></p>
<h3 id="1-3-word2vec-vs-Transformer"><a href="#1-3-word2vec-vs-Transformer" class="headerlink" title="1.3. word2vec vs Transformer"></a>1.3. word2vec vs Transformer</h3><p><img src="/NLP/transformer/transformer/2020-07-02-10-11-54.png" alt></p>
<p><img src="/NLP/transformer/transformer/2020-07-02-10-13-52.png" alt></p>
<h3 id="1-4-seq2seq-vs-Transformer"><a href="#1-4-seq2seq-vs-Transformer" class="headerlink" title="1.4. seq2seq vs Transformer"></a>1.4. seq2seq vs Transformer</h3><p>seq2seq:<br>当输入句子比较长时，所有语义完全转换为一个中间语义向量C来表示，单词原始的信息已经消失，可想而知会丢失很多细节信息。<br>所以Encoder-Decoder是有缺陷的，其缺陷在于：当输入信息太长时，会丢失掉一些信息。<br>transformer:<br>为解决上述问题，设计了self-attention机制。</p>
<h2 id="2-整体结构"><a href="#2-整体结构" class="headerlink" title="2. 整体结构"></a>2. 整体结构</h2><p>模型主要分为编码器和解码器。<br>multi-Head中包含一个特殊的结构：self-attention<br>encoder和decoder结构略有不同：decoder的多头多了一层masked掩码<br><img src="/NLP/transformer/transformer/2023-08-12-21-09-40.png" alt></p>
<blockquote>
<p>注：<br>1.注意上图中的N*, encoder和decoder部分都是由n（一般6-7个）个结构相同但参数不同的编码器组成，是循环训练n个encoder，这里并不是并行训练。<br>2.albert会有部分的模型层参数是共享的。<br><img src="/NLP/transformer/transformer/2021-10-19-08-39-09.png" alt></p>
</blockquote>
<h3 id="2-1-引入Attention机制模型"><a href="#2-1-引入Attention机制模型" class="headerlink" title="2.1. 引入Attention机制模型"></a>2.1. 引入Attention机制模型</h3><p>相较于seq2seq,Attention 模型的特点是 Eecoder 不再将整个输入序列编码为固定长度的「中间向量Ｃ」，而是编码成一个向量的序列(包含多个向量)。引入了Attention的Encoder-Decoder 模型如下图。<br><img src="/NLP/transformer/transformer/2023-08-12-20-56-52.png" alt><br><strong>优势：</strong></p>
<ul>
<li>从输出的角度<br>每个输出的词Y会受到每个输入词的整体影响，不是只主要受某一个词的影响，毕竟整个输入语句是整体而连贯的，但同时每个输入词对每个输出的影响又是不一样的，即每个输出Y受每个输入词的影响权重不一样，而这个权重便是由Attention计算，也就是所谓的注意力分配系数，计算每一项输入对输出权重的影响大小</li>
<li>从编码的角度讲<br>在根据给到的信息进行编码时（或称特征提取），不同信息的重要程度是不一样的（可用权重表示），即有些信息是无关紧要的，比如一些语气助词之类的，所以这个时候在编码时，就可以有的放矢，根据不同的重要程度针对性汲取相关信息</li>
</ul>
<h3 id="2-2-self-attention-注意力机制"><a href="#2-2-self-attention-注意力机制" class="headerlink" title="2.2. self-attention 注意力机制"></a>2.2. self-attention 注意力机制</h3><p>简单来说解决的问题： 问题不同，需要关注的维度区域也不同.<br>含义公式：</p>
<p>$ Attention(Query, Source) = \sum_{i=1}^{L_x}Similarity(Query, Key_i) * Value_i $</p>
<p>矩阵公式：</p>
<p>$ Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k} })V $</p>
<p>举个例子：<br><img src="/NLP/transformer/transformer/2021-10-19-09-19-31.png" alt></p>
<p>Q: 查询矩阵 （输入的矩阵表示—婴儿）<br>K: 待查询矩阵 （图片区域的某种矩阵表示）<br>V: 自身含义的编码序列 图片区域的某种值向量表示）<br>QK做点乘得到某个值，一个向量在另一个向量上的投影长度，是个标量，可以反映两个向量的相似度，点乘结果越大，可以表示越相似<br><img src="/NLP/transformer/transformer/2021-10-19-09-41-26.png" alt><br>F(Q,K) 做点乘，或cos 或mlt<br>做softmax， a1+a2 +a3+a4 = 1<br>和V矩阵相乘得到attention value</p>
<p><strong>如何获取QKV?</strong><br><img src="/NLP/transformer/transformer/2021-10-19-09-51-44.png" alt><br>X = (x1, x2)  分别为512位的transformer输入<br>Q = (q1, q2) = X W^q = (x1Wq, x2Wq)<br><img src="/NLP/transformer/transformer/2021-10-19-11-05-19.png" alt></p>
<p><strong>如何计算得到attention的值</strong><br><img src="/NLP/transformer/transformer/2021-10-19-11-07-42.png" alt></p>
<blockquote>
<p><strong>为什么要除以d_k?</strong><br>softmax(x)，x比较大时函数太平缓，梯度很小，不容易做gradient decent。 也可以理解为做一次归一化，做一次降维</p>
</blockquote>
<p><strong>优势：</strong></p>
<ul>
<li>从效率角度<br>解决了RNN不能并行计算的问题</li>
</ul>
<h3 id="2-3-encoder"><a href="#2-3-encoder" class="headerlink" title="2.3. encoder"></a>2.3. encoder</h3><p><img src="/NLP/transformer/transformer/2021-10-19-08-44-24.png" alt></p>
<h4 id="2-3-1-输入部分"><a href="#2-3-1-输入部分" class="headerlink" title="2.3.1. 输入部分"></a>2.3.1. 输入部分</h4><ol>
<li>embedding</li>
<li>位置嵌入<br><strong>为什么需要位置编码？</strong><br>TODO:</li>
</ol>
<h4 id="2-3-2-positional-encoding"><a href="#2-3-2-positional-encoding" class="headerlink" title="2.3.2. positional encoding"></a>2.3.2. positional encoding</h4><p>将位置信息加入输入层。</p>
<ul>
<li>在self-attension中，是没有位置信息的</li>
<li>原始paper这样说： 每个位置都有自己唯一的位置向量编码（不是从数据中学来的）</li>
<li><p>换句话说： 每个embedding向量添加一个one hot的位置信息向量<br><img src="/NLP/transformer/transformer/2020-08-06-11-30-13.png" alt><br>RNN训练的是一套参数，transformer训练的是不同的参数</p>
<blockquote>
<p><strong>RNN 的梯度消失和普通网络的梯度消失有什么不同？</strong><br>说是RNN由于连乘效应产生梯度消失，并不十分准确，RNN的梯度是个总的梯度和，并不是会趋于0，它的梯度被近距离梯度主导，被远距离梯度忽略不计。</p>
<p><strong>RNN和transformer的区别</strong><br>RNN是串行化训练的序列，transformer的注意力机制是并行处理的，比RNN快，但是会缺少顺序信息，所以加入位置编码</p>
</blockquote>
<p> <img src="/NLP/transformer/transformer/2021-10-19-08-57-13.png" alt><br>2i理解成偶数位置， 2i+1理解成奇数位置，在偶数位置使用sin函数，奇数位置使用cos函数， 得到和embedding一样维度的位置编码。<br><img src="/NLP/transformer/transformer/2021-10-19-09-00-36.png" alt><br>将512维度的位置编码和512维度embedding相加，就是这个字整个transformer的输入编码。</p>
</li>
</ul>
<p><img src="/NLP/transformer/transformer/2021-10-19-09-02-40.png" alt></p>
<h4 id="2-3-3-注意力机制-self-attention"><a href="#2-3-3-注意力机制-self-attention" class="headerlink" title="2.3.3. 注意力机制 self-attention"></a>2.3.3. 注意力机制 self-attention</h4><!-- ![](transformer/2021-05-25-22-24-43.png)
让计算机关注到全部的信息，捕获重点的信息, 并且可以并行化
![](transformer/2020-08-05-17-17-30.png)
![](transformer/2020-08-05-17-18-53.png)
根号d可以理解为做一次归一化，做一次降维
![](transformer/2020-08-05-17-21-58.png)
![](transformer/2020-08-05-17-23-53.png)  -->
<!-- 矩阵运算理解并行化
![](transformer/2020-08-06-10-43-25.png)
![](transformer/2020-08-06-10-45-07.png)
![](transformer/2020-08-06-11-16-05.png)
反正就是一堆矩阵乘法，GPU可以加速并行化。
![](transformer/2020-07-02-10-23-23.png) -->
<h4 id="2-3-4-multi-head-self-attention"><a href="#2-3-4-multi-head-self-attention" class="headerlink" title="2.3.4. multi-head self-attention"></a>2.3.4. multi-head self-attention</h4><p>训练多个q,k,v, 可以考虑到更多的细节局部信息的影响。但是好像没有考虑位置信息。<br><img src="/NLP/transformer/transformer/2021-10-19-12-50-49.png" alt><br><img src="/NLP/transformer/transformer/2021-10-19-12-52-27.png" alt><br>将多个头计算出的Z进行拼接， 得到多头机制的输出</p>
<h4 id="2-3-5-残差和LayerNorm"><a href="#2-3-5-残差和LayerNorm" class="headerlink" title="2.3.5. 残差和LayerNorm"></a>2.3.5. 残差和LayerNorm</h4><p><a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=4" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Di4y1c7Zm?p=4</a><br><img src="/NLP/transformer/transformer/2021-10-19-13-04-20.png" alt></p>
<h4 id="2-3-6-batch-normal-vs-layer-normal"><a href="#2-3-6-batch-normal-vs-layer-normal" class="headerlink" title="2.3.6. batch normal vs layer normal"></a>2.3.6. batch normal vs layer normal</h4><p><a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=5&amp;spm_id_from=pageDriver" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Di4y1c7Zm?p=5&amp;spm_id_from=pageDriver</a></p>
<p>batch normal效果在NLP中很少比layer normal效果好。</p>
<p><strong>batch normal</strong></p>
<p>batch Nomal是针对一个batch中的所有样本的某/每一个特征做batch normal, 比如每个同学的身高特征，体重特征，年龄特征。<br><img src="/NLP/transformer/transformer/2021-10-19-13-35-11.png" alt><br>优点：<br>可以解决内部的协变量偏移<br>缓解了梯度饱和问题，加快收敛<br>缺点：<br>batch size比较小，效果比较差（batch数据不能很好代表全部数据）<br>RNN中效果比较差 ，RNN输入是动态的，不能有效的获取batch中的均值和方差（同理，可能因为样本长度很长的地方，没有batch的数据能够代表整体数据）</p>
<p><strong>layer normal</strong></p>
<p>BN的那种表示对于文本情景不太适合，LN对一个样本的所有单词做缩放。</p>
<!-- ![](transformer/2020-08-06-11-22-49.png)
![](transformer/2020-08-06-11-22-06.png)
 -->
<h3 id="2-4-Decoder"><a href="#2-4-Decoder" class="headerlink" title="2.4. Decoder"></a>2.4. Decoder</h3><p><a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?p=7&amp;spm_id_from=pageDriver" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1Di4y1c7Zm?p=7&amp;spm_id_from=pageDriver</a></p>
<h4 id="2-4-1-masked-multi-head"><a href="#2-4-1-masked-multi-head" class="headerlink" title="2.4.1. masked multi-head"></a>2.4.1. masked multi-head</h4><p>为什么需要mask?<br>因为预测的时候，我们是不知道字符下一个特征的，所以把当前单词后面的特征mask掉</p>
<h4 id="2-4-2-交互层-multi-head"><a href="#2-4-2-交互层-multi-head" class="headerlink" title="2.4.2. 交互层 multi-head"></a>2.4.2. 交互层 multi-head</h4><p>encoder 是Q生成KV矩阵， Q来自于输入<br>decoder 是KV生成的Q矩阵，KV 来自于encoder</p>
<!-- ## 3. transformer

![](transformer/2020-08-06-17-51-35.png)
![](transformer/2020-08-07-08-39-38.png)

-->
      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="https://tlylft.github.io/NLP/transformer/transformer/" title="【transformer】 01-Transformer" target="_blank" rel="external">https://tlylft.github.io/NLP/transformer/transformer/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="https://github.com/tlylft" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/images/avatar.png" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="https://github.com/tlylft" target="_blank"><span class="text-dark">Icey</span><small class="ml-1x">小傻瓜,别回头</small></a></h3>
        <div>以梦为马，随处可栖。</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/NLP/NER/medical_NER/" title="医药领域实体识别"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;上一篇</span></a>
    </li>
    
    
    <li class="next">
      <a href="/tensorflow/tensor_intro/" title="tensorflow introduction"><span>下一篇&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  <!-- Button trigger modal -->
  <button type="button" class="btn btn-fancy btn-donate pop-onhover bg-gradient-warning" data-toggle="modal" data-target="#donateModal"><span>赏</span></button>
  <!-- <div class="wave-icon wave-icon-danger btn-donate" data-toggle="modal" data-target="#donateModal">
    <div class="wave-circle"><span class="icon"><i class="icon icon-bill"></i></span></div>
  </div> -->
  
  
  <div class="bar-right">
    
    <div class="share-component" data-sites="weibo,qq,wechat,facebook,twitter" data-mobile-sites="weibo,qq,qzone"></div>
    
  </div>
  </div>
</nav>
  
<!-- Modal -->
<div class="modal modal-center modal-small modal-xs-full fade" id="donateModal" tabindex="-1" role="dialog">
  <div class="modal-dialog" role="document">
    <div class="modal-content donate">
      <button type="button" class="close" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">&times;</span></button>
      <div class="modal-body">
        <div class="donate-box">
          <div class="donate-head">
            <p>感谢您的支持，我会继续努力的!</p>
          </div>
          <div class="tab-content">
            <div role="tabpanel" class="tab-pane fade active in" id="alipay">
              <div class="donate-payimg">
                <img src="/images/donate/alipayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开支付宝扫一扫，即可进行扫码打赏哦</p>
            </div>
            <div role="tabpanel" class="tab-pane fade" id="wechatpay">
              <div class="donate-payimg">
                <img src="/images/donate/wechatpayimg.png" alt="扫码支持" title="扫一扫" />
              </div>
              <p class="text-muted mv">扫码打赏，你说多少就多少</p>
              <p class="text-grey">打开微信扫一扫，即可进行扫码打赏哦</p>
            </div>
          </div>
          <div class="donate-footer">
            <ul class="nav nav-tabs nav-justified" role="tablist">
              <li role="presentation" class="active">
                <a href="#alipay" id="alipay-tab" role="tab" data-toggle="tab" aria-controls="alipay" aria-expanded="true"><i class="icon icon-alipay"></i> 支付宝</a>
              </li>
              <li role="presentation" class="">
                <a href="#wechatpay" role="tab" id="wechatpay-tab" data-toggle="tab" aria-controls="wechatpay" aria-expanded="false"><i class="icon icon-wepay"></i> 微信支付</a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</div>



</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/tlylft?tab=repositories" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
        <li><a href="https://www.weibo.com/tlylft/profile?rightmod=1&wvr=6&mod=personinfo" target="_blank" title="Weibo" data-toggle=tooltip data-placement=top><i class="icon icon-weibo"></i></a></li>
        
        <li><a href="https://www.facebook.com/profile.php?id=100009783614101" target="_blank" title="Facebook" data-toggle=tooltip data-placement=top><i class="icon icon-facebook"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/js/plugin.min.js"></script>


<script src="/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: '文章',
            PAGES: '页面',
            CATEGORIES: '分类',
            TAGS: '标签',
            UNTITLED: '(未命名)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'eL0FL59XsOPmrzo5OTb0zhgE-gzGzoHsz',
    appKey: '53Yf97Qf3bKKWC81Bh4EM9oY',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>