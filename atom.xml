<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>灵魂都失控</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tlylft.github.io/"/>
  <updated>2023-08-21T06:45:27.068Z</updated>
  <id>https://tlylft.github.io/</id>
  
  <author>
    <name>Icey Liu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>LLM（9）prompt learning提示学习</title>
    <link href="https://tlylft.github.io/LLM/09_prompt_learning/"/>
    <id>https://tlylft.github.io/LLM/09_prompt_learning/</id>
    <published>2023-08-17T02:37:52.000Z</published>
    <updated>2023-08-21T06:45:27.068Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考<br><a href="https://zhuanlan.zhihu.com/p/635686756" target="_blank" rel="noopener">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a><br><a href="https://zhuanlan.zhihu.com/p/635848732" target="_blank" rel="noopener">大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2</a></p><h1 id="1-background"><a href="#1-background" class="headerlink" title="1. background"></a>1. background</h1></blockquote><h1 id="2-fine-tuning-vs-prompt-learning"><a href="#2-fine-tuning-vs-prompt-learning" class="headerlink" title="2. fine-tuning vs prompt learning"></a>2. fine-tuning vs prompt learning</h1><p><strong>finetuning的局限性：</strong></p><ol><li>模型发展的参数规模越来越大，每次任务都需要全量参数进行训练调整，成本高</li><li>少样本学习能力差，容易过拟合。对于一些下游任务，不能很好的将预训练任务学习的东西迁移过来。</li></ol><p><strong>区别：</strong><br>finetuning: 通过改变模型参数，使模型适配下游任务<br>prompt learning: 模型结构不变，通过重构任务描述，使下游任务适配模型。<br><img src="/LLM/09_prompt_learning/2023-08-17-10-34-07.png" alt></p><h2 id="2-1-prompt-learning-vs-instruct-learning"><a href="#2-1-prompt-learning-vs-instruct-learning" class="headerlink" title="2.1. prompt learning vs. instruct learning"></a>2.1. prompt learning vs. instruct learning</h2><p>prompt learning和instruction tuning的核心区别在于instruction tuning会提供更多的指令引导模型输出更符合预期的结果，例如<br>提示学习：给女朋友买了这个项链，她很喜欢，这个项链太____了<br>指令微调：判断这句话的情感：给女朋友买了这个项链，她很喜欢。选项：A=好；B=一般；C=差<br>也可以暂简单理解instruction tuning为带人类指令的prompting</p><p><a href="https://mp.weixin.qq.com/s/2cNstrbu8cdRH53IXK24Mw" target="_blank" rel="noopener">大语言模型三种训练技术及区别：Prompt-Tuning、Instruction-Tuning和Chain-of-Thought</a><br>这篇文章认为：<br>prompt tuning是为了让模型更好的学习任务与上下文的关系<br>instruction tuning是为了让模型能够按照指令要求的格式，或者步骤输出。</p><h2 id="2-2-适用场景"><a href="#2-2-适用场景" class="headerlink" title="2.2. 适用场景"></a>2.2. 适用场景</h2><p>《<a href="https://www.bilibili.com/video/BV1Ug411N7KD" target="_blank" rel="noopener">继Fine-tune之后的新范式——Prompt进展梳理</a>》 这里讲的观点是：<br>prompt learning因为只对prompt做了可微调的embedding ,而模型本身是freeze的，所以它更多的是为了特定的如分类这种任务，学习与该任务相关prompt中词的attention关系,比如对情感分类任务进行提示学习训练，在训练中会更多的偏向情感类词语的attention权重的改变，以此达到提示学习特定任务的效果。但在阅读理解任务中，因为要根据不同的问题回答，而这个问题是不确定的，提升的效果就没有那么好了。</p><h1 id="3-prompt-learning"><a href="#3-prompt-learning" class="headerlink" title="3. prompt learning"></a>3. prompt learning</h1><p><a href="https://arxiv.org/abs/2107.13586" target="_blank" rel="noopener">prompt综述论文</a></p><h2 id="3-1-实现过程"><a href="#3-1-实现过程" class="headerlink" title="3.1. 实现过程"></a>3.1. 实现过程</h2><p><img src="/LLM/09_prompt_learning/2023-08-17-10-43-23.png" alt><br>图左侧的fine-tuning任务：</p><p>prompt learning过程：</p><ol><li>首先为任务设计一个模板，包含两个内容，一个[x]的input内容，一个[z]为模型的输出内容，根据任务的不同，x可以设置为多个空，如：假设推理任务，可以设置一个[x1]为前提，[x2]为假设，[z]为推理关系。</li><li>答案映射到结果</li></ol><h2 id="3-2-Prompt-template-Engineering-提示模板工程"><a href="#3-2-Prompt-template-Engineering-提示模板工程" class="headerlink" title="3.2. Prompt template Engineering 提示模板工程"></a>3.2. Prompt template Engineering 提示模板工程</h2><h3 id="3-2-1-方法分类"><a href="#3-2-1-方法分类" class="headerlink" title="3.2.1. 方法分类"></a>3.2.1. 方法分类</h3><p><img src="/LLM/09_prompt_learning/2023-08-17-14-14-28.png" alt></p><h4 id="3-2-1-1-按shape分类"><a href="#3-2-1-1-按shape分类" class="headerlink" title="3.2.1.1. 按shape分类"></a>3.2.1.1. 按shape分类</h4><p>按照[z]的位置，可以分为两类实现方式：</p><ol><li>cloze template: 填空提示<br>[z]在句子中间，将[z]mask掉， 更适合masked LM 预训练模型(双向)，如Bert<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> Input x &#x3D;&quot;I love this movie.&quot;</span><br><span class="line">template &#x3D;&quot;[x] Overall, it is a [z] movie.&quot;</span><br></pre></td></tr></table></figure></li><li>prefix template： 前缀提示<br>[z]在句子最后，将提示全都放在它的前面，更适合生成任务或自回归模型<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Input x &#x3D; &quot;I love this movie.&quot;</span><br><span class="line">template &#x3D;&quot;[x] Overall, this movie is [z].&quot;</span><br></pre></td></tr></table></figure><h4 id="3-2-1-2-按人类介入分类"><a href="#3-2-1-2-按人类介入分类" class="headerlink" title="3.2.1.2. 按人类介入分类"></a>3.2.1.2. 按人类介入分类</h4>按照模板生成方式，可以分为两类：</li><li>人工介入 Hand-crafted<br>人工设计的prompt差异的影响：模板的细微差异，导致最后模型的准确度是不一样的，且有很大差异。<br><img src="/LLM/09_prompt_learning/2023-08-17-14-30-06.png" alt></li><li>自动生成 <h2 id="3-2-2-方法介绍"><a href="#3-2-2-方法介绍" class="headerlink" title="3.2.2. 方法介绍"></a>3.2.2. 方法介绍</h2><h3 id="autoPrompt"><a href="#autoPrompt" class="headerlink" title="autoPrompt"></a>autoPrompt</h3><strong>autoPrompt算法</strong>：自动搜索prompt模板词（<strong>离散方式</strong>：search from discrete space,意思是找模板词的时候是一个词一个词的找的）<br>给定输入，模型到token词源库中找哪些词作为模板词，会使输出更接近想要的输出。<br><img src="/LLM/09_prompt_learning/2023-08-17-14-46-33.png" alt><h3 id="prefix-Tuning"><a href="#prefix-Tuning" class="headerlink" title="prefix Tuning"></a>prefix Tuning</h3>在Prefix Tuning之前的工作主要是人工设计离散的模版或者自动化搜索离散的模版。<br>动机：</li><li>对于人工设计的模版，模版的变化对模型最终的性能特别敏感，加一个词、少一个词或者变动位置都会造成比较大的变化。</li><li>对于自动化搜索模版，成本也比较高；</li><li>以前这些离散化的token搜索出来的结果可能并不是最优的。</li></ol><p>主要思想：</p><ol><li>Prefix Tuning提出固定预训练LM，为LM添加可训练，任务特定的前缀，这样就可以为不同任务保存不同的前缀，微调成本也小</li><li>这种Prefix实际就是连续可微的Virtual Token（Soft Prompt/Continuous Prompt），相比离散的Token，更好优化，效果更好。</li></ol><p>原理：</p><ul><li>在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而PLM中的其他部分参数固定。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-22-26.png" alt></li><li>对不同模型结构，构造不同的prefix<br>GPT3这种自回归模型结构：在句子前面添加前缀，得到 z = [PREFIX; x; y]<br>bert这种编码器-解码器架构模型：Encoder和Decoder都增加了前缀，得到 z = [PREFIX; x; PREFIX0; y]</li><li>Prefix则是可以学习的“隐式”的提示，改变了人工构造prompt无法更新参数的问题。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-42-04.png" alt></li><li>为了防止直接更新Prefix的参数导致训练不稳定和性能下降的情况，在Prefix层前面加了MLP结构，训练完成后，只保留Prefix的参数。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-42-40.png" alt></li><li>证明只调整embedding层的表现力不够，因此，在每层都加了prompt的参数，改动较大。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-42-47.png" alt><h3 id="prompt-tuning"><a href="#prompt-tuning" class="headerlink" title="prompt tuning"></a><a href="https://arxiv.org/pdf/2104.08691v1" target="_blank" rel="noopener">prompt tuning</a></h3>原理：<br>背景同上，该方法可以看作是Prefix Tuning的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但只在输入层加入prompt tokens，并且不需要加入 MLP 进行调整来解决难训练的问题。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-25-45.png" alt><br>主要思想：</li></ul><ol><li>提出了 Prompt Ensembling，也就是在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方式询问同一个问题），这样相当于训练了不同模型，比模型集成的成本小多了。<br><img src="/LLM/09_prompt_learning/2023-08-21-10-27-10.png" alt><h3 id="p-tuning"><a href="#p-tuning" class="headerlink" title="p-tuning"></a>p-tuning</h3><strong>p-tuning方法</strong>： （<strong>连续方式</strong>： considers continuous space,意思是考虑连续空间，token embedding）<br>该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。</li></ol><p>实验结果：<br>随着预训练模型参数量的增加，Prompt Tuning的方法会逼近全参数微调的结果。</p><p>主要思想：</p><ol><li>相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；</li><li>保留关键词与MASK, 将其他词替换为隐向量<br><img src="/LLM/09_prompt_learning/2023-08-21-13-32-03.png" alt></li><li>virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。???<br><img src="/LLM/09_prompt_learning/2023-08-21-13-30-05.png" alt></li></ol><h3 id="P-Tuning-V2"><a href="#P-Tuning-V2" class="headerlink" title="P-Tuning V2"></a><a href="https://arxiv.org/pdf/2110.07602v2" target="_blank" rel="noopener">P-Tuning V2</a></h3><p>背景：<br>Prompt Tuning和P-Tuning等方法存在两个主要的问题：</p><ul><li>缺乏模型参数规模通用性：Prompt Tuning论文中表明当模型规模超过100亿个参数时，提示优化可以与全量微调相媲美。但是对于那些较小的模型（从100M到1B），提示优化和全量微调的表现有很大差异，这大大限制了提示优化的适用性。</li><li>缺乏模型任务通用性：尽管Prompt Tuning和P-tuning在一些 NLU 基准测试中表现出优势，但提示调优对硬序列标记任务（即序列标注）的有效性尚未得到验证。</li><li>缺少深度提示优化： 连续提示只被插入transformer第一层的输入embedding序列中，在接下来的transformer层中，插入连续提示的位置的embedding是由之前的transformer层计算出来的，这可能导致两个可能的优化挑战</li></ul><p><strong>原理</strong>：可看成在prefix tuning基础上，适配到NLU任务中，然后做了一些改进。</p><ul><li>在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层： 1. 带来了更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。 2. 加入到更深层结构中的Prompt能给模型预测带来更直接的影响。</li><li><strong>移除重参数化的编码器</strong>：在 P-tuning v2 中，作者发现重参数化的改进很小，尤其是对于较小的模型，同时还会影响模型的表现。</li><li><strong>针对不同任务采用不同的提示长度</strong>：提示长度在提示优化方法的超参数搜索中起着核心作用。在实验中，我们发现不同的理解任务通常用不同的提示长度来实现其最佳性能，这与Prefix-Tuning中的发现一致，不同的文本生成任务可能有不同的最佳提示长度。</li><li><strong>引入多任务学习</strong>。先在多任务的Prompt上进行预训练，然后再适配下游任务。多任务学习对我们的方法来说是可选的，但可能是相当有帮助的。一方面，连续提示的随机惯性给优化带来了困难，这可以通过更多的训练数据或与任务相关的无监督预训练来缓解；另一方面，连续提示是跨任务和数据集的特定任务知识的完美载体。我们的实验表明，在一些困难的序列任务中，多任务学习可以作为P-tuning v2的有益补充。</li><li><strong>回归传统的分类标签范式，而不是映射器</strong>。标签词映射器（Label Word Verbalizer）一直是提示优化的核心组成部分，它将one-hot类标签变成有意义的词，以利用预训练语言模型头。尽管它在few-shot设置中具有潜在的必要性，但在全数据监督设置中，Verbalizer并不是必须的。它阻碍了提示调优在我们需要无实际意义的标签和句子嵌入的场景中的应用。因此，P-Tuning v2回归传统的CLS标签分类范式，采用随机初始化的分类头（Classification Head）应用于tokens之上，以增强通用性，可以适配到序列标注任务。</li></ul><p><strong>简单理解四个模型的区别</strong><br>核心区别在于以下两点：</p><ol><li>prompt的embedding引入的位置 1) prefix tuning在输入层或每个encoder的输入层 2) prompt tuning、p-tuning只在输入层 3) p-tuningv2 在每个encoder的输入层和内部layer层</li><li>为解决难训练问题使用的重参数化的解决方式 1) prefix tuning适应MLP层 2） prompt tuning不需要MLP 3) p-tuning使用 LSTM+MLP 4) p-tuningv2 认为上述两种方式的提升并不大，所以去掉了。</li></ol><p>总结：<br>prefix tuning：每一层 MLP + prefix_emb， 只适合大模型<br>prompt tuning: 输入层 没有MLP + prefix_emb，<br>p-tuning: 输入层 LSTM + keep keyword + prefix/inner_emb， 只适合部分任务<br>p-tuning v2: 每一层 没有重参数化 + prefix_emb， 多任务学习+设置不同提示长度， 适配模型和各种任务</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;参考&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/635686756&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tunin
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM（8）分布式并行训练</title>
    <link href="https://tlylft.github.io/LLM/08_LLM_parallelism/"/>
    <id>https://tlylft.github.io/LLM/08_LLM_parallelism/</id>
    <published>2023-08-15T00:48:21.000Z</published>
    <updated>2023-08-29T01:09:52.598Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考资料：<br><a href="https://zhuanlan.zhihu.com/p/635825108" target="_blank" rel="noopener">LLM的分布式并行训练方式总结</a><br><a href="https://mp.weixin.qq.com/s/zoXxkdVj70XOuAMdkGtuEQ" target="_blank" rel="noopener">万字长文详解大模型平台技术栈</a><br><a href="https://zhuanlan.zhihu.com/p/630734624" target="_blank" rel="noopener">deepspeed入门教程</a><br><a href="https://www.deepspeed.ai/getting-started/" target="_blank" rel="noopener">deepspeed 官方文档</a></p></blockquote><p><strong>目录</strong><br>[TOC]</p><h1 id="1-并行技术"><a href="#1-并行技术" class="headerlink" title="1. 并行技术"></a>1. 并行技术</h1><h2 id="1-1-数据并行"><a href="#1-1-数据并行" class="headerlink" title="1.1. 数据并行"></a>1.1. 数据并行</h2><p><strong>核心思路：每个GPU处理一部分数据，常用于单机多卡。</strong><br>将模型参数和优化器状态复制到多个 GPU 上，每个 Worker 并行处理数据的一部分，执行前向和反向传播以获取梯度。在不同 GPU 上计算的梯度将进一步聚合以获得整个批量的梯度，从而更新所有 GPU 上的模型。<br><strong>DP：</strong> 参数服务器<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-43-15.png" alt><br><strong>DDP:</strong> 解决DP在多机情况下，通讯负载不均的问题，将Server上的通讯压力均衡转到各个Worker上。<br>1) 数据切分： 假如有n块gpu, 每块gpu上的数据也被分成n份（避免出现oom）。</p><p>2）Reduce-Scatter：定义网络拓扑关系，使得每个GPU只和其相邻的两块GPU通讯。每次发送对应位置的数据进行累加。每一次累加更新都形成一个拓扑环，因此被称为Ring。<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-45-31.png" alt><br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-45-46.png" alt><br>缺点：<br>1） 存储开销大。每块GPU上都存了一份完整的模型，造成冗余。<br>2）通讯开销大。Server需要和每一个Worker进行梯度传输，Worker间并不通讯。因此Server承担了系统所有的通讯压力。 当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。<br>3）当模型可以放进单个 GPU 时才有效，模型太大无法做数据并行。<br>改进：<br>为了解决这个问题，微软提出了 ZeRO，</p><h2 id="1-2-流水线并行"><a href="#1-2-流水线并行" class="headerlink" title="1.2. 流水线并行"></a>1.2. 流水线并行</h2><p><strong>核心思路：每个GPU处理一部分模型层</strong></p><h3 id="朴素流水线并行："><a href="#朴素流水线并行：" class="headerlink" title="朴素流水线并行："></a>朴素流水线并行：</h3><p>将模型的不同 layer 分配到多个 GPU 中，数据在不同层（也即是不同 GPU ）之间移动。下图中 a 展示了流水线并行的计算方式，前向传播时，数据从 Device0 依次传递到 Device3；然后经历反向传播，计算梯度从 Device3 依次传递回 Device0。<br><strong>缺点：</strong><br>1） gpu利用率低，因为每个 GPU 必须等待前一个 GPU 计算完成，从而导致不必要的气泡开销。<br>2）中间激活激活值结果占据大量内存。<br><strong>改进：</strong><br>为了减少气泡开销，GPipe[60] 和 PipeDream[61] 提出如果同时进行多个迭代，每个节点在同一时刻负责不同迭代的计算，就可以避免数据依赖，不用在原地干等了。<br><img src="/LLM/08_LLM_parallelism/2023-08-28-15-31-52.png" alt></p><h2 id="1-3-张量并行"><a href="#1-3-张量并行" class="headerlink" title="1.3. 张量并行"></a>1.3. 张量并行</h2><p>张量并行把全连接层的参数和计算分割到多个 GPU 上，与流水线不同，张量并行专注于分解模型的张量（参数矩阵），Megatron-LM [62] 论文中关于这个问题有详细阐述。</p><h2 id="1-4-3D-Parallelism概念"><a href="#1-4-3D-Parallelism概念" class="headerlink" title="1.4. 3D Parallelism概念"></a>1.4. 3D Parallelism概念</h2><p>3D 并行实际上是三种常用并行训练技术的组合，即数据并行、流水线并行和张量并行。</p><p><img src="/LLM/08_LLM_parallelism/2023-08-28-15-16-55.png" alt><br><strong>通讯量比较：TP &gt; DP &gt; PP</strong></p><p>因此优先将张量并行放在同一机器上（机器内的带宽大，通讯时间也会小）。 流水线并行因为通讯量消耗低，可以跨机器。 若张量并行没有跨机器，则数据并行也不需要跨机器；否则数据并行组也需要跨机器。</p><h2 id="1-5-ZeRO"><a href="#1-5-ZeRO" class="headerlink" title="1.5. ZeRO"></a>1.5. ZeRO</h2><p>零冗余优化器，Zero Redundancy Optimizer</p><p>这是一种新的并行优化器，它可以大大减少模型和数据并行所需的资源，同时可以大量增加可训练的参数数量。</p><p>其原理就是普通的数据并行，只是每个 GPU 没有都复制完整的模型参数、梯度和优化器状态，而是只存储其中的一部分。在之后的训练过程，当需要给定层的完整层参数时，所有 GPU 通过通信同步以相互提供它们缺失的部分。</p><p>ZeRO 具有三个主要的优化阶段，分别对应于优化器状态、梯度和参数分区。</p><p><img src="/LLM/08_LLM_parallelism/2023-08-28-15-22-27.png" alt><br><strong>特点：</strong></p><ol><li>克服了数据并行和模型并行的局限性，同时实现两者的优点，它通过跨数据并行进程将模型状态划分为上图所示的参数、梯度和优化器状态分区，而不是复制它们，从而消除了数据并行进程之间的内存冗余。</li><li>在训练期间使用动态通信规划（dynamic communication schedule），在分布式设备之间共享必要的状态，以保持数据并行的计算粒度和通信量。</li><li>与 PyTorch 兼容，DeepSpeed API 是在 PyTorch 上进行的轻量级封装</li><li>DeepSpeed 管理着所有样板化的 SOTA 训练技术，例如分布式训练、混合精度、梯度累积和检查点，开发者可以专注于模型开发。</li></ol><p><strong>优点：</strong></p><ol><li>规模大： 可运行当前最先进的模型参数量，多达1000亿个参数。</li><li>速度快： 吞吐量比SOTA高出5呗。</li><li>成本低： 提高吞吐量意味着大大降低训练成本，例如，要训练具有 200 亿个参数的模型，DeepSpeed 需要的资源是原来的 3/4。</li><li>易用性强： 只需更改几行代码即可使 PyTorch 模型使用 DeepSpeed 和 ZeRO。不需要重新设计代码或重构模型，它也没有对模型尺寸、批处理大小或任何其它训练参数加以限制。<h1 id="2-通用概念"><a href="#2-通用概念" class="headerlink" title="2. 通用概念"></a>2. 通用概念</h1><h2 id="2-1-local-rank与global-rank"><a href="#2-1-local-rank与global-rank" class="headerlink" title="2.1. local_rank与global_rank"></a>2.1. local_rank与global_rank</h2>假设：有三个server，每个server有8个GPU，一共24个GPU，<br>word_size: 是整个分布式环境中进程的总数。<br>local_rank： 就是在当前server上本地的rank,范围在0-7， 当设置为-1时，表示不再分布式设置下运行。<br>global_rank：就是每个进程在全局分布式训练中的的rank,用于标识该进程在分布式环境中的位置， 范围在0到word_size-1</li></ol><p>获取方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 设置local_rank:</span><br><span class="line">torch.device(&#39;cuda&#39;, local_rank)</span><br><span class="line">deepspeed.init_distributed()  # 初始化分布式后端</span><br><span class="line"># 获取gobal</span><br><span class="line">global_rank &#x3D; torch.distributed.get_rank()</span><br></pre></td></tr></table></figure></p><p><strong>torch.distributed.barrier()</strong><br>是一个同步函数，用于在分布式环境中同步各个进程的状态。 调用时进程会阻塞等待，所有进程都调用后解除。<br>用途： </p><ol><li>同步梯度更新， 每个进程完成一轮反向传播后，互相同步梯度后再进行下一轮更新。</li><li>并行训练使，数据读取只在local_rank=0的进程中进行，其他进程阻塞等待。</li></ol><h2 id="2-2-卡间通讯方式"><a href="#2-2-卡间通讯方式" class="headerlink" title="2.2. 卡间通讯方式"></a>2.2. 卡间通讯方式</h2><p>gpu之间的通信时间会限制multi-gpu的训练速度，而gpu之间的通信模式如果不是NVLink，多块卡的训练速度可能比一块卡要慢。 通过以下命令查看卡间通讯方式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi topo -m</span><br></pre></td></tr></table></figure></p><h1 id="3-分布式框架-pytorch"><a href="#3-分布式框架-pytorch" class="headerlink" title="3. 分布式框架 pytorch"></a>3. 分布式框架 pytorch</h1><h2 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net &#x3D; torch.nn.DataParallel(model, device_ids&#x3D;[0, 1, 2])</span><br><span class="line">output &#x3D; net(input_var)</span><br></pre></td></tr></table></figure><h1 id="4-分布式框架-DeepSpeed"><a href="#4-分布式框架-DeepSpeed" class="headerlink" title="4. 分布式框架 DeepSpeed"></a>4. 分布式框架 DeepSpeed</h1><p>DeepSpeed是微软推出的大规模模型分布式训练的工具，主要实现了ZeRO并行训练算法。通过提高规模、速度、可用性并降低成本，可以在当前一代的 GPU 集群上训练具有超过 1000 亿个参数的深度学习模型，极大促进大型模型的训练。同时，与最新技术相比，其系统性能可以提高 5 倍以上。</p><p>原始文档：<a href="https://huggingface.co/docs/transformers/main/main_classes/deepspeed" target="_blank" rel="noopener">https://huggingface.co/docs/transformers/main/main_classes/deepspeed</a></p><p><img src="/LLM/08_LLM_parallelism/2023-08-28-22-44-32.png" alt></p><h2 id="4-1-主要功能"><a href="#4-1-主要功能" class="headerlink" title="4.1. 主要功能"></a>4.1. 主要功能</h2><ol><li>Optimizer state partitioning (ZeRO stage 1)</li><li>Gradient partitioning (ZeRO stage 2)</li><li>Parameter partitioning (ZeRO stage 3)</li><li>Custom mixed precision training handling</li><li>A range of fast CUDA-extension-based optimizers</li><li>ZeRO-Offload to CPU and NVMe</li></ol><h2 id="4-2-参数说明"><a href="#4-2-参数说明" class="headerlink" title="4.2. 参数说明"></a>4.2. 参数说明</h2><p>与数据相关的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data path: 数据路径，huggingface数据库， 比如:Dahoas&#x2F;rm-static</span><br><span class="line">data_split&quot; 数据的拆分方式，比如2,4,4 是为step1，2，3分配的数据比例</span><br><span class="line">max_seq_len: 最大序列长度, 超过会对数据进行截断，过大会内存不足</span><br><span class="line">data_output_path:相关数据的存储地址，需要是local storage，不能是shared storage</span><br></pre></td></tr></table></figure><br>与模型相关的参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model_name_or_path: 模型地址，huggingface模型，比如:facebook&#x2F;opt-1.3b</span><br><span class="line">lora_dim: 如果大于0，则使用LORA优化</span><br><span class="line">lora_module_name: 设置LORA的范围，比如可以只针对 decoder.layers</span><br><span class="line">only_optimize lora:是否只优化LORA的参数</span><br></pre></td></tr></table></figure><br>与训练相关的参数(需要不断调试和调整)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">per_device_train_batch_size :训练时的 Batch size (per device:每个GPU的size)</span><br><span class="line">per_device eval_batch_size:评价时的的 Batch size (per device)，增加可提高训练的稳定性</span><br><span class="line">learning_rate:学习率</span><br><span class="line">weight_decay:权重衰减，防止模型过拟合的技术</span><br><span class="line">num_train_epochs:训练 epoch 数</span><br><span class="line">gradient_accumulation_steps : 累积多少个 mini-batch 的梯度后再进行一次参数更新</span><br><span class="line">Ir_scheduler_type: learning rate的调整策略，比如 linear，cosine</span><br></pre></td></tr></table></figure><br>deepspeed相关参数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">zero_stage:这个对应者DeepSpeed工具中的zero方式，分别是0，1，2，3 (一个gpu装不下需设置为3)</span><br><span class="line">offload: ZeRO-0ffload 通过利用主机CPU上的计算和内存资源来执行优化器，从而减少此类模型的GPU计算和内存需求</span><br><span class="line">local_rank :分布式训练时的一个变量，用于标识当前 GPU 设备的本地排名 (本机排名，与global-rank不同)gradient_checkpointing:降低深度学习模型训练过程中内存消耗的技术</span><br><span class="line"></span><br><span class="line">seed: 随机排序是的seed</span><br><span class="line">output_dir: 模型的存储目录</span><br></pre></td></tr></table></figure></p><h2 id="4-3-训练启动配置"><a href="#4-3-训练启动配置" class="headerlink" title="4.3. 训练启动配置"></a>4.3. 训练启动配置</h2><p>相关默认路径： /job/hostfile<br>相关参数：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- num_gpus 设置为1时，不再查找hostfile文件</span><br><span class="line">-- hostfile 集群ip配置文件，默认为&#x2F;job&#x2F;hostfile</span><br><span class="line">-- include</span><br></pre></td></tr></table></figure></p><h3 id="4-3-1-单机单卡训练"><a href="#4-3-1-单机单卡训练" class="headerlink" title="4.3.1. 单机单卡训练"></a>4.3.1. 单机单卡训练</h3><p>如果我们只在单个节点（具有一个或多个 GPU）上运行，DeepSpeed 不需要如上所述的主机文件。如果未检测到或传入主机文件，则DeepSpeed将查询本地计算机上的GPU数量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 删除&#x2F;job&#x2F;hostfile文件，执行下面语句</span><br><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0 deepspeed --num_gpus&#x3D;1 train.py</span><br></pre></td></tr></table></figure></p><blockquote><p><strong>为何单机单卡也是用deepspeed？</strong><br>1.使用ZeRO-offload，将部分数据offload到CPU，降低对显存的需求2<br>2.提供了对显存的管理，减少显存中的碎片</p></blockquote><h3 id="4-3-2-单机指定单GPU训练"><a href="#4-3-2-单机指定单GPU训练" class="headerlink" title="4.3.2. 单机指定单GPU训练"></a>4.3.2. 单机指定单GPU训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deepspeed --include localhost:1 train.py ... --deepspeed ds_config.json</span><br></pre></td></tr></table></figure><h3 id="4-3-3-单机多卡训练"><a href="#4-3-3-单机多卡训练" class="headerlink" title="4.3.3. 单机多卡训练"></a>4.3.3. 单机多卡训练</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES&#x3D;0,1 deepspeed train.py ... --deepspeed ds_config.json</span><br><span class="line">或</span><br><span class="line">deepspeed --num_gpus&#x3D;2 train.py ... --deepspeed ds_config.json</span><br><span class="line">或</span><br><span class="line">torch.distributed.run --nproc_per_node&#x3D;2 your_program.py --deepspeed ds_config.json</span><br></pre></td></tr></table></figure><h3 id="4-3-4-多节点多卡"><a href="#4-3-4-多节点多卡" class="headerlink" title="4.3.4. 多节点多卡"></a>4.3.4. 多节点多卡</h3><ol><li>设置hostfile文件，只需在一个节点上启动<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hostname1 slots&#x3D;8</span><br><span class="line">hostname2 slots&#x3D;8</span><br><span class="line"># 然后运行</span><br><span class="line">deepspeed --num_gpus 8 --num_nodes 2 --hostfile hostfile_path --master_addr hostname1 --master_port&#x3D;9901 train.py ... --deepspeed ds_config.json</span><br></pre></td></tr></table></figure></li></ol><h2 id="4-4-ZeRO配置文件"><a href="#4-4-ZeRO配置文件" class="headerlink" title="4.4. ZeRO配置文件"></a>4.4. ZeRO配置文件</h2><p>选择不同的Zero stage和offload?<br>1) 从左到右，越来越慢<br>Stage 0 (DDP) &gt; Stage 1 &gt; Stage 2 &gt; Stage 2 + offload &gt; Stage 3 &gt; Stage 3 + offloads<br>2) 从左到右，所需GPU显存越来越少<br>Stage 0 (DDP) &lt; Stage 1 &lt; Stage 2 &lt; Stage 2 + offload &lt; Stage 3 &lt; Stage 3 + offloads</p><h3 id="NVMe-Support"><a href="#NVMe-Support" class="headerlink" title="NVMe Support ??"></a>NVMe Support ??</h3><ul><li>ZeRO-Infinity 需要使用 ZeRO-3</li><li>ZeRO-3 会比 ZeRO-2 慢很多。使用以下策略，可以使得ZeRO-3 的速度更接近ZeRO-2<ol><li>将stage3_param_persistence_threshold参数设置的很大，比如6 <em> hidden_size </em> hidden_size</li><li>将offload_params参数关闭（可以极大改善性能）<h3 id="Zero-0"><a href="#Zero-0" class="headerlink" title="Zero-0"></a>Zero-0</h3>stage 0会禁用所有的分片，然后把DeepSpeed当作时DDP来使用。<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 0</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Zero-1"><a href="#Zero-1" class="headerlink" title="Zero-1"></a>Zero-1</h3>只对优化器参数进行分片，可以加速一丢丢<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Zero-2"><a href="#Zero-2" class="headerlink" title="Zero-2"></a>Zero-2</h3>配置示例：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 2,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;allgather_partitions&quot;: true,</span><br><span class="line">        &quot;allgather_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;overlap_comm&quot;: true,   # 控制是否使用通信与计算的重叠。当设置为True时，DeepSpeed将在梯度计算时尝试并行执行梯度通信。可以有效地减少通信时间，从而加速整个训练过程。</span><br><span class="line">        &quot;reduce_scatter&quot;: true,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: 2e8,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 2000,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>参数说明：</li></ol></li><li>overlap_comm: 控制是否使用通信与计算的重叠。当设置为True时，DeepSpeed将在梯度计算时尝试并行执行梯度通信。可以有效地减少通信时间，从而加速整个训练过程</li><li>allgather_bucket_size：用于控制Allgather操作的分桶大小。Allgather操作是指在分布式训练中，每个进程收集其他所有进程的张量，并将这些张量按顺序拼接起来。通过将张量划分为较小的桶（buckets），可以在通信过程中更高效地传输数据。allgather_bucket_size值越大，每个桶的大小越大，通信操作可能会变得更快，但也需要更多的内存来存储中间结果。合适的桶大小要根据实际情况调整。</li><li>reduce_bucket_size：类似于allgather_bucket_size，用于控制Allreduce操作的分桶大小。Allreduce操作是将所有进程的某个张量进行规约（例如求和），并将结果广播回所有进程。通过将张量划分为较小的桶，可以更高效地传输数据。reduce_bucket_size值越大，每个桶的大小越大，通信操作可能会变得更快，但同时也需要更多的内存来存储中间结果。合适的桶大小需要根据实际情况进行调整。</li><li>overlap_comm使用的是allgather_bucket_size和reduce_bucket_size值的4.5倍。如果它们被设置为5e8，需要9GB显存（5e8 x 2Bytes x 2 x 4.5）。如果内存大小是8GB或更小，需要将这些参数减少到约2e8，从而避免OOM，这需要3.6GB显存。如果在大容量GPU上也出现OOM，也需要做同样的调整。</li><li>在deepspeed==0.4.4中新增了 round_robin_gradients 选项，可以并行化CPU的offload。当梯度累积的步数增加，或者GPU数量增加时，会有更好的性能优势。</li></ul><h3 id="Zero-3"><a href="#Zero-3" class="headerlink" title="Zero-3"></a>Zero-3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;fp16&quot;: &#123;</span><br><span class="line">        &quot;enabled&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;loss_scale&quot;: 0,</span><br><span class="line">        &quot;loss_scale_window&quot;: 1000,</span><br><span class="line">        &quot;initial_scale_power&quot;: 16,</span><br><span class="line">        &quot;hysteresis&quot;: 2,</span><br><span class="line">        &quot;min_loss_scale&quot;: 1</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;optimizer&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;AdamW&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;betas&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;eps&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;weight_decay&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;scheduler&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;WarmupLR&quot;,</span><br><span class="line">        &quot;params&quot;: &#123;</span><br><span class="line">            &quot;warmup_min_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_max_lr&quot;: &quot;auto&quot;,</span><br><span class="line">            &quot;warmup_num_steps&quot;: &quot;auto&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;zero_optimization&quot;: &#123;</span><br><span class="line">        &quot;stage&quot;: 3,</span><br><span class="line">        &quot;offload_optimizer&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;offload_param&quot;: &#123;</span><br><span class="line">            &quot;device&quot;: &quot;cpu&quot;,</span><br><span class="line">            &quot;pin_memory&quot;: true</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;overlap_comm&quot;: true,</span><br><span class="line">        &quot;contiguous_gradients&quot;: true,</span><br><span class="line">        &quot;sub_group_size&quot;: 1e9,</span><br><span class="line">        &quot;reduce_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_prefetch_bucket_size&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_param_persistence_threshold&quot;: &quot;auto&quot;,</span><br><span class="line">        &quot;stage3_max_live_parameters&quot;: 1e9,</span><br><span class="line">        &quot;stage3_max_reuse_distance&quot;: 1e9,</span><br><span class="line">        &quot;stage3_gather_16bit_weights_on_model_save&quot;: true</span><br><span class="line">    &#125;,</span><br><span class="line"></span><br><span class="line">    &quot;gradient_accumulation_steps&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;gradient_clipping&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;steps_per_print&quot;: 2000,</span><br><span class="line">    &quot;train_batch_size&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;train_micro_batch_size_per_gpu&quot;: &quot;auto&quot;,</span><br><span class="line">    &quot;wall_clock_breakdown&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>stage3_max_live_parameters 是保留在 GPU 上的完整参数数量的上限。</li><li>stage3_max_reuse_distance 是指将来何时再次使用参数的指标，从而决定是丢弃参数还是保留参数。 如果一个参数在不久的将来要再次使用（小于 stage3_max_reuse_distance），可以保留以减少通信开销。 使用activation checkpointing时，这一点非常有用。</li><li>如果遇到 OOM，可以减少 stage3_max_live_parameters 和 stage3_max_reuse_distance。 除非正在使用activation checkpointing，否则它们对性能的影响应该很小。 1e9 会消耗 ~2GB。 内存由 stage3_max_live_parameters 和 stage3_max_reuse_distance 共享，所以不是相加的，一共 2GB。</li><li>stage3_gather_16bit_weights_on_model_save 在保存模型时启用模型 fp16 权重合并。 对大型模型和多GPU，在内存和速度方面都是一项昂贵的操作。 如果打算恢复训练，目前需要使用它。 未来的更新将消除此限制。</li><li>sub_group_size 控制在optimizer steps中更新参数的粒度。 参数被分组到 sub_group_size 的桶中，每个桶一次更新一个。 当与 ZeRO-Infinity 中的 NVMe offload一起使用时，sub_group_size 控制模型状态在optimizer steps期间从 NVMe 移入和移出 CPU 内存的粒度。 防止超大模型耗尽 CPU 内存。不使用NVMe offload时，使其保持默认值。出现OOM时，减小sub_group_size。当优化器迭代很慢时，可以增大sub_group_size 。</li><li>ZeRO-3 中未使用 allgather_partitions、allgather_bucket_size 和 reduce_scatter 配置参数<h3 id="训练文件加载配置"><a href="#训练文件加载配置" class="headerlink" title="训练文件加载配置"></a>训练文件加载配置</h3>train.py <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 加载配置文件</span><br><span class="line">with open(args.deepspeed, &quot;r&quot;, encoding&#x3D;&quot;utf-8&quot;) as fh:</span><br><span class="line">    ds_config &#x3D; json.load(fh)</span><br><span class="line"># init deepspeed</span><br><span class="line">model, optimizer, _, lr_scheduler &#x3D; deepspeed.initialize(model&#x3D;model, args&#x3D;args, config&#x3D;ds_config, dist_init_required&#x3D;True)</span><br></pre></td></tr></table></figure><h3 id="调参步骤"><a href="#调参步骤" class="headerlink" title="调参步骤"></a>调参步骤</h3>原则： 先保证显存够用，再保证速度够快<br>大致的顺序是先尝试stage0-&gt;stage3；如果还不够，开启offload到cpu；降低一些参数设置，使用混合精度训练。<br><strong>参考设置</strong><br>如果训模型from scratch，hidden size最好可以被16整除<br>batch size最好可以被2整除<br><strong>参考步骤：</strong></li></ul><ol><li>将batch_size设置为1，通过梯度累积实现任意的有效batch_size</li><li>如果OOM则，设置—gradient_checkpointing 1 (HF Trainer)，或者 model.gradient_checkpointing_enable()</li><li>如果OOM则，尝试ZeRO stage 2</li><li>如果OOM则，尝试ZeRO stage 2 + offload_optimizer</li><li>如果OOM则，尝试ZeRO stage 3</li><li>如果OOM则，尝试offload_param到CPU</li><li>如果OOM则，尝试offload_optimizer到CPU</li><li>如果OOM则，尝试降低一些默认参数。比如使用generate时，减小beam search的搜索范围</li><li>如果OOM则，使用混合精度训练，在Ampere的GPU上使用bf16，在旧版本GPU上使用fp16</li><li>如果仍然OOM，则使用ZeRO-Infinity ，使用offload_param和offload_optimizer到NVME</li><li>一旦使用batch_size=1时，没有导致OOM，测量此时的有效吞吐量，然后尽可能增大batch_size</li><li>开始优化参数，可以关闭offload参数，或者降低ZeRO stage，然后调整batch_size，然后继续测量吞吐量，直到性能比较满意（调参可以增加66%的性能）</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;参考资料：&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/635825108&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LLM的分布式并行训练方式总结&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;htt
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>【transformer】 03-GPT系列 微调到prompt</title>
    <link href="https://tlylft.github.io/LLM/transformer/GPT/"/>
    <id>https://tlylft.github.io/LLM/transformer/GPT/</id>
    <published>2023-08-13T13:12:24.000Z</published>
    <updated>2023-08-17T09:28:01.313Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-GPT1：-生成式的预训练"><a href="#1-GPT1：-生成式的预训练" class="headerlink" title="1. GPT1： 生成式的预训练"></a>1. GPT1： 生成式的预训练</h1><p>OpenAI 2018 论文：《<a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noopener">Improving Language Understanding by Generative Pre-Training</a>》<br>预训练(单向Transformer) + Fine-tuning<br>GPT是“Generative Pre-Training”的简称，从名字看其含义是指的生成式的预训练。<br>GPT提出两阶段训练方式，第一个阶段是利用语言模型进行预训练，第二阶段通过Fine-tuning的模式解决下游任务</p><p><strong>预训练过程</strong></p><ol><li>从ELMO中优化：特征抽取器使用Transformer解码器，将输入序列编码为固定长度的向量。</li><li>使用单向语言模型：ELMO在做语言模型预训练的时候，预测单词可以同时使用上文和下文，用的双向LSTM结构，而GPT则只采用单词的上文来进行单词预测，而抛开了下文。说人话，就是让你根据提示造句，从左到右，是单向的<br><img src="/LLM/transformer/GPT/2023-08-14-09-31-21.png" alt></li></ol><blockquote><p>关于为啥使用单向的说明，需要再详细看一下</p></blockquote><p><strong>第二阶段：下游任务</strong><br><img src="/LLM/transformer/GPT/2023-08-14-09-37-54.png" alt></p><blockquote><p>??? 什么是结构改造？</p></blockquote><h1 id="2-GPT2"><a href="#2-GPT2" class="headerlink" title="2. GPT2:"></a>2. GPT2:</h1><p>OpenAI 论文：《<a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">Language Models are Unsupervised Multitask Learners</a>》<br>初步具备小样本学习上的潜能</p><p>不同规模的版本：<br>小、中、大、特大（15亿参数）： 解码器层数分别为12,24,36,48</p><h1 id="3-GPT3"><a href="#3-GPT3" class="headerlink" title="3. GPT3"></a>3. GPT3</h1><p>OpenAI 论文：《<a href="https://arxiv.org/pdf/2005.14165" target="_blank" rel="noopener">Language Models are Few-Shot Learners</a>》<br>增大参数量： 成为1750亿的大语言模型</p><p><img src="/LLM/transformer/GPT/2023-08-14-10-42-53.png" alt><br><img src="/LLM/transformer/GPT/2023-08-14-10-43-07.png" alt></p><p>为什么通过提示可以不微调？<br>few-shot: 通过上下文学习学到了一种隐士微调，学习到了映射的分布。</p><h1 id="4-GPT-3-5-从InstructGPT到ChatGPT初版的迭代过程"><a href="#4-GPT-3-5-从InstructGPT到ChatGPT初版的迭代过程" class="headerlink" title="4. GPT 3.5:从InstructGPT到ChatGPT初版的迭代过程"></a>4. GPT 3.5:从InstructGPT到ChatGPT初版的迭代过程</h1><p>加强代码推理能力： 使用github的代码库作为训练数据<br>本质是三个阶段都是在做微调<br>继续增强代码训练<br>增加指令微调： 借鉴instructGPT<br>RHLF强化学习</p><h2 id="4-1-指令微调"><a href="#4-1-指令微调" class="headerlink" title="4.1. 指令微调"></a>4.1. 指令微调</h2><p>Google论文：《<a href="https://arxiv.org/pdf/2109.01652" target="_blank" rel="noopener">FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS</a>》<br><img src="/LLM/transformer/GPT/2023-08-14-10-59-41.png" alt><br>和pretrain模型的比较：<br><img src="/LLM/transformer/GPT/2023-08-14-14-24-10.png" alt></p><h2 id="4-2-COT：增强模型推理能力"><a href="#4-2-COT：增强模型推理能力" class="headerlink" title="4.2. COT：增强模型推理能力"></a>4.2. COT：增强模型推理能力</h2><p>google论文： 《Chain-of-Thought Prompting Elicits Reasoning in Large Language Models》<br>本质也是prompt学习的过程，将推理过程加入到prompt中。<br><img src="/LLM/transformer/GPT/2023-08-14-14-26-26.png" alt><br>主要特性：</p><ol><li>原则上让模型将多不问题分解为中间步骤，可以将一些计算分配给推理步骤。</li><li>思维链为模型行为提供了可解释的窗口，可以表明模型在推理过程中所遵循的逻辑，并因此提供了调试推理过程的机会。</li><li>可以用于数学问题，尝试推理等任务。</li><li>提高few-shot的准确性。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-GPT1：-生成式的预训练&quot;&gt;&lt;a href=&quot;#1-GPT1：-生成式的预训练&quot; class=&quot;headerlink&quot; title=&quot;1. GPT1： 生成式的预训练&quot;&gt;&lt;/a&gt;1. GPT1： 生成式的预训练&lt;/h1&gt;&lt;p&gt;OpenAI 2018 论文：《
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
      <category term="transformer" scheme="https://tlylft.github.io/categories/LLM/transformer/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（2）-马尔科夫决策过程MDP</title>
    <link href="https://tlylft.github.io/reinforcement_learning/02_rf_mdp/"/>
    <id>https://tlylft.github.io/reinforcement_learning/02_rf_mdp/</id>
    <published>2023-08-09T09:13:54.000Z</published>
    <updated>2023-08-10T08:44:41.707Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>强化学习极简入门1.2：<a href="https://blog.csdn.net/v_JULY_v/article/details/128965854" target="_blank" rel="noopener">https://blog.csdn.net/v_JULY_v/article/details/128965854</a></p></blockquote><h1 id="1-马尔科夫决策过程（MDP）"><a href="#1-马尔科夫决策过程（MDP）" class="headerlink" title="1. 马尔科夫决策过程（MDP）"></a>1. 马尔科夫决策过程（MDP）</h1><p>RL其实是一个马尔可夫决策过程(Markov decision process，MDP)，而为说清楚MDP，得先从随机过程、马尔可夫过程(Markov process，简称MP)开始讲起，故为考虑逻辑清晰，我们还是把整个继承/脉络梳理下。</p><p>概念理解顺序：<br>马尔科夫过程 -&gt; 马尔科夫奖励() -&gt; 马尔科夫决策过程（MDP）</p><h2 id="1-1-MDP的前置知识：随机过程、马尔可夫过程"><a href="#1-1-MDP的前置知识：随机过程、马尔可夫过程" class="headerlink" title="1.1. MDP的前置知识：随机过程、马尔可夫过程"></a>1.1. MDP的前置知识：随机过程、马尔可夫过程</h2><h2 id="随机过程"><a href="#随机过程" class="headerlink" title="随机过程"></a>随机过程</h2><p>核心：状态转移矩阵，表示当前状态下执行下一动作的概率，也是需要学习的参数？</p><h2 id="马尔可夫奖励-MRP"><a href="#马尔可夫奖励-MRP" class="headerlink" title="马尔可夫奖励(MRP)"></a>马尔可夫奖励(MRP)</h2><p>在马尔可夫过程的基础上加入奖励函数R和折扣因子\gamma，就可以得到马尔可夫奖励过程(Markov reward process，MRP)</p><ul><li>奖励函数，某个状态s的奖励R(s)，是指转移到该状态s时可以获得奖励的期望，有$R(s) = E[R_{t+1}|S_t = s]$<br>注意，有的书上奖励函数和下面回报公式中的R_{t+1}的下标t+1<br>写为t，其实严格来说，先有t时刻的状态/动作之后才有t+1时刻的奖励，但应用中两种下标法又都存在，读者注意辨别</li></ul><ul><li>在实际中，奖励可以分为即时奖励和持久奖励。<br>持久奖励是当前动作对未来n步影响的收益，这个收益可以用小于1的折扣因子的n次方* 未来产生的即时奖励 求和获得，因为一个状态可以得到的奖励是持久的，所有奖励的衰减之和称为回报，可用G表示当下即时奖励和所有持久奖励等一切奖励的加权和(考虑到一般越往后某个状态给的回报率越低，也即奖励因子或折扣因子越小，用\gamma表示)，从而有<br>$Gt=R_{t+1}+γ⋅R_{t+2}+γ^2⋅R_{t+3}+γ^3⋅R_{t+4}+⋯=R_{t+1}+γ(R_{t+2}+γ⋅R_{t+3}+γ^2⋅R_{t+4}+⋯)=R_{t+1}+γG_{t+1}$</li><li>通过奖励计算回报<br>引入概率，推导过程忽略<br>就是所谓的贝尔曼方程(bellman equation),hu<br>$V(s)=R(s)+γ∑s′∈SP(s′|s)V(s′)$<h2 id="马尔可夫决策过程-MDP-：马尔可夫奖励-MRP-智能体动作因素"><a href="#马尔可夫决策过程-MDP-：马尔可夫奖励-MRP-智能体动作因素" class="headerlink" title="马尔可夫决策过程(MDP)：马尔可夫奖励(MRP) + 智能体动作因素"></a>马尔可夫决策过程(MDP)：马尔可夫奖励(MRP) + 智能体动作因素</h2></li></ul><h2 id="aa"><a href="#aa" class="headerlink" title="aa"></a>aa</h2><p>动作价值函数<br>状态价值函数</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;强化学习极简入门1.2：&lt;a href=&quot;https://blog.csdn.net/v_JULY_v/article/details/128965854&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.c
      
    
    </summary>
    
    
      <category term="强化学习" scheme="https://tlylft.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://tlylft.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>强化学习（3）-RLHF基于人类反馈的强化学习</title>
    <link href="https://tlylft.github.io/reinforcement_learning/03_rf_rlhf/"/>
    <id>https://tlylft.github.io/reinforcement_learning/03_rf_rlhf/</id>
    <published>2023-08-09T09:13:54.000Z</published>
    <updated>2023-08-10T00:49:44.092Z</updated>
    
    <content type="html"><![CDATA[<p>简单介绍：<br><a href="https://www.bilibili.com/video/BV1eT411S7Yx" target="_blank" rel="noopener">ChatGPT狂飙：强化学习RLHF与PPO！【ChatGPT】原理第02篇</a><br><a href="https://www.bilibili.com/video/BV1Uv4y1V7Ec" target="_blank" rel="noopener">LLM大型语言模型如何进行微调？ RLHF强化学习代码解读</a></p><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>finetuning 20B LLM, 微调大语言模型超过200亿参数的模型是非常困难的，需要大量的语料，否则参数调不动起不到效果，于是提出了RLHF(Reinforcement Learning  Human Feedback)强化学习的微调方式，强化学习通过人类反馈来训练大语言模型，从而提高大语言模型的性能。</p><p>实际上，RLHF(Reinforcement Learning with Human Feedback)这一概念最早被定义为基于人类反馈的强化学习，它最早是在2008年<a href="https://arxiv.org/pdf/1706.03741" target="_blank" rel="noopener">《TAMER：Training an Agent Manually via Evaluative Reinforcement》</a>一文中被提及的.<br>在2017年前后，深度强化学习(Deep Reinforcement Learning)逐渐发展并流行起来，如你所见，2017年6月由OpenAI联合Google DeepMind一块推出：基于人类偏好的深度强化学习《Deep Reinforcement Learning from Human Preferences》，也简称RLHF</p><p>huggingface 提供了trl开源库，实现微调大语言模型的方式。<a href="https://github.com/lvwerra/trl/tree/main/examples" target="_blank" rel="noopener">https://github.com/lvwerra/trl/tree/main/examples</a></p><h1 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h1><p>简单说明：<br>首先，得到一个预训练语言模型的输出结果，然后，通过人类反馈对模型打分，将对输出结果的评分作为reward，来训练一个RL模型，得到新的模型参数。</p><p>也可以通过这种方式做指令微调，根据不同的指令设置不同的reward.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;简单介绍：&lt;br&gt;&lt;a href=&quot;https://www.bilibili.com/video/BV1eT411S7Yx&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT狂飙：强化学习RLHF与PPO！【ChatGPT】原理第02篇&lt;/a&gt;&lt;
      
    
    </summary>
    
    
      <category term="强化学习" scheme="https://tlylft.github.io/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="强化学习" scheme="https://tlylft.github.io/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>LLM（4）-chatGLM-清华</title>
    <link href="https://tlylft.github.io/LLM/04_chatGLM/"/>
    <id>https://tlylft.github.io/LLM/04_chatGLM/</id>
    <published>2023-08-09T06:40:59.000Z</published>
    <updated>2023-08-28T09:18:43.096Z</updated>
    
    <content type="html"><![CDATA[<p>代码：<br><a href="https://github.com/THUDM" target="_blank" rel="noopener">https://github.com/THUDM</a><br>包含 ChatGLM-6B、GLM-130B、ChatGLM2-6B<br>130B space: <a href="https://huggingface.co/spaces/THUDM/GLM-130B" target="_blank" rel="noopener">https://huggingface.co/spaces/THUDM/GLM-130B</a><br>SFT 微调：<br>四种方法： Freeze方法、Lora方法、P-Tuning方法、全量参数等<br><a href="https://github.com/liucongg/ChatGLM-Finetuning" target="_blank" rel="noopener">https://github.com/liucongg/ChatGLM-Finetuning</a><br><a href="https://github.com/mymusise/ChatGLM-Tuning" target="_blank" rel="noopener">https://github.com/mymusise/ChatGLM-Tuning</a></p><p><a href="https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md" target="_blank" rel="noopener">https://github.com/hiyouga/ChatGLM-Efficient-Tuning/blob/main/README_zh.md</a><br>这个不维护了，改成下面的维护，支持多种模型的lora, QLora微调<br><a href="https://github.com/hiyouga/LLaMA-Efficient-Tuning/tree/main" target="_blank" rel="noopener">https://github.com/hiyouga/LLaMA-Efficient-Tuning/tree/main</a></p><p>微调踩坑：<br><a href="https://zhuanlan.zhihu.com/p/627741267" target="_blank" rel="noopener">ChatGLM多卡微调踩坑记录</a></p><ol><li>Error: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0!</li></ol><p>使用deepspeed多卡训练时会遇到这个问题，原因是由于ChatGLM进行了一次更新，使用离线下载的老版模型参数和AutoModel加载的新版配置会发生冲突。</p><p>可以通过模型中的config.json文件鉴别下载到本地的ChatGLM是老版还是新版。<br>老版的ChatGLM的vocab_size为150528<br>新版的ChatGLM的vocab_size为130528</p><p>解决方法：如果使用新版的ChatGLM可以直接使用huggingface的AutoModel，如果使用老版ChatGLM可以从之前文件的modeling_chatglm和tokenization_chatglm中加载模型。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 老模型</span><br><span class="line">from modeling_chatglm import ChatGLMForConditionalGeneration</span><br><span class="line">from tokenization_chatglm import ChatGLMTokenizer</span><br><span class="line">model &#x3D; ChatGLMForConditionalGeneration.from_pretrained(</span><br><span class="line">        model_args.model_name_or_path, trust_remote_code&#x3D;True</span><br><span class="line">    )</span><br><span class="line">tokenizer &#x3D; ChatGLMTokenizer.from_pretrained(model_args.tokenizer_name, trust_remote_code&#x3D;True)</span><br><span class="line"># 新模型</span><br><span class="line">model &#x3D; AutoModel.from_pretrained(model_path, trust_remote_code&#x3D;True).</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;代码：&lt;br&gt;&lt;a href=&quot;https://github.com/THUDM&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/THUDM&lt;/a&gt;&lt;br&gt;包含 ChatGLM-6B、GLM-130B、ChatGLM2-
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>redis</title>
    <link href="https://tlylft.github.io/tools/redis/"/>
    <id>https://tlylft.github.io/tools/redis/</id>
    <published>2023-07-19T00:21:34.000Z</published>
    <updated>2023-07-24T07:34:41.016Z</updated>
    
    <content type="html"><![CDATA[<p>linux 安装： <a href="https://blog.csdn.net/qq_39187538/article/details/126485922" target="_blank" rel="noopener">https://blog.csdn.net/qq_39187538/article/details/126485922</a></p><h1 id="1-下载安装"><a href="#1-下载安装" class="headerlink" title="1. 下载安装"></a>1. 下载安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 下载</span><br><span class="line">yum install -y wget</span><br><span class="line">wget https:&#x2F;&#x2F;download.redis.io&#x2F;releases&#x2F;redis-7.0.2.tar.gz</span><br><span class="line"># 默认安装到了&#x2F;usr&#x2F;local&#x2F;bin&#x2F;目录，但是我想自定义安装到&#x2F;data&#x2F;software&#x2F;</span><br><span class="line">#解压</span><br><span class="line">tar -zxf redis-7.0.2.tar.gz -C &#x2F;data&#x2F;software&#x2F;</span><br><span class="line">#编译</span><br><span class="line">make</span><br><span class="line">#安装</span><br><span class="line">make install PREFIX&#x3D;&#x2F;data&#x2F;software&#x2F;</span><br></pre></td></tr></table></figure><p>添加环境变量<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi ~.&#x2F;bashrc</span><br><span class="line">REDIS_HOME&#x3D;&#x2F;data&#x2F;software&#x2F;</span><br><span class="line">PATH&#x3D;$PATH:$REDIS_HOME&#x2F;bin</span><br><span class="line">#配置生效</span><br><span class="line">source ~&#x2F;.bash_profile</span><br></pre></td></tr></table></figure></p><h1 id="启动和停止"><a href="#启动和停止" class="headerlink" title="启动和停止"></a>启动和停止</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#实际是去找&#x2F;data&#x2F;software&#x2F;redis-7.0.2&#x2F;bin的这个启动语句,并使用redis配置文件</span><br><span class="line">redis-server &#x2F;data&#x2F;software&#x2F;redis-7.0.2&#x2F;redis.conf</span><br><span class="line">#&#x2F;data&#x2F;software&#x2F;redis-7.0.2&#x2F;bin的这个预计进行停止</span><br><span class="line">redis-cli shutdown</span><br></pre></td></tr></table></figure><h2 id="redis-conf文件说明"><a href="#redis-conf文件说明" class="headerlink" title="redis.conf文件说明"></a>redis.conf文件说明</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#设置后台启动，如果不是后台启动，每次推出redis就关闭了</span><br><span class="line">daemonize yes</span><br><span class="line">#开启密码保护，注释则不需要密码</span><br><span class="line">requirepass 密码</span><br><span class="line">#设置端口号</span><br><span class="line">port 端口号</span><br><span class="line">#允许访问的ip，改为0.0.0.0就是所有ip均可</span><br><span class="line">bind 127.0.0.1 -::1</span><br><span class="line">bind 0.0.0.0</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;linux 安装： &lt;a href=&quot;https://blog.csdn.net/qq_39187538/article/details/126485922&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/qq_3
      
    
    </summary>
    
    
      <category term="tools" scheme="https://tlylft.github.io/categories/tools/"/>
    
    
      <category term="tools" scheme="https://tlylft.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>LLM（2）-Instruct GPT</title>
    <link href="https://tlylft.github.io/LLM/02_instruct_gpt/"/>
    <id>https://tlylft.github.io/LLM/02_instruct_gpt/</id>
    <published>2023-06-21T01:38:32.000Z</published>
    <updated>2023-08-14T09:30:41.412Z</updated>
    
    <content type="html"><![CDATA[<p>OpenAI : <a href="https://arxiv.org/pdf/2203.02155" target="_blank" rel="noopener">Training language models to follow instructions with human feedback</a></p><p>1.基于GPT-3作为底层模型进行微调 SFT</p><ol><li>人类反馈 RM</li><li>优化参数  PPO</li></ol><h1 id="1-大模型的问题"><a href="#1-大模型的问题" class="headerlink" title="1. 大模型的问题"></a>1. 大模型的问题</h1><h1 id="2-instruct的意义"><a href="#2-instruct的意义" class="headerlink" title="2. instruct的意义"></a>2. instruct的意义</h1><h1 id="3-实现方式"><a href="#3-实现方式" class="headerlink" title="3. 实现方式"></a>3. 实现方式</h1><p><img src="/LLM/02_instruct_gpt/2023-06-21-10-37-46.png" alt></p><h2 id="3-1-基于GPT-3作为底层模型进行微调-SFT"><a href="#3-1-基于GPT-3作为底层模型进行微调-SFT" class="headerlink" title="3.1. 基于GPT-3作为底层模型进行微调 SFT"></a>3.1. 基于GPT-3作为底层模型进行微调 SFT</h2><p>(chatgpt使用的gpt3.5)<br>过程：</p><ol><li>预训练模型准备：首先预训练得到gpt-3的模型</li><li>数据集准备：<br>13k大小的标注好的问答对: <x1, y1>…</x1,></li><li>finetuning微调： 16epochs, 余弦学习率衰减, residual dropout=0.2<h2 id="3-2-人类反馈-RM"><a href="#3-2-人类反馈-RM" class="headerlink" title="3.2. 人类反馈 RM"></a>3.2. 人类反馈 RM</h2>训练一个奖励模型，为模型的回答提供评分排序，而不是评分，因为评分在标注过程中存在个体差异。<br>训练集为6k大小，为每个问题生成4-9个回答的输出，人工对这些回答的好坏进行排序，以此训练这个奖励模型。<h2 id="3-3-优化参数-PPO"><a href="#3-3-优化参数-PPO" class="headerlink" title="3.3. 优化参数  PPO"></a>3.3. 优化参数  PPO</h2>31k的训练数据集，对三个不同规模的预训练模型进行PPO模型训练验证，分别为1.3B, 6B, 175B， 发现经过PPO模型训练后的1.3B模型比175B的SFT模型效果更好。</li></ol><p>PPO模型的初始策略是用的 $\pi^{SFT}$，产生的策略为 $\pi^{RL}$, 每一次训练都会产生不同的 $\pi^{RL_1}$, $\pi^{RL_2}$,$\pi^{RL_3}$，用$\pi^{RL’}$表示模型上一次的策略。<br>每次策略产生变化，要重新计算对应的奖励和状态。<br>为了数据利用率更高效，涉及到重要性采样，出来重要性比值，用KL散度或截断方式做约束，确保更新幅度不会太大</p><blockquote><p>第七课 00::09:00 之前 也讲了一些PPO<br>第七课：1:24:00  PPO目标函数讲解</p></blockquote><h1 id="6-模型"><a href="#6-模型" class="headerlink" title="6. 模型"></a>6. 模型</h1><p>RM loss<br>将奖励的差异作为交叉熵损失函数，使模型更能学习到好和坏的差异。且尽量最大化差异，如选择评分8.5和4分的模型进行学习，而不是8.5和8分的数据（容易引起模型混淆）</p><p>RL-PPO</p><ol><li>最大化收益</li><li>KL散度 限制新学习的πRL强化策略 和初始的模型生成策略πSFT之间的差距不要太大，</li><li>偏置项</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;OpenAI : &lt;a href=&quot;https://arxiv.org/pdf/2203.02155&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Training language models to follow instructions with h
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM（5）-LLaMA</title>
    <link href="https://tlylft.github.io/LLM/05_LLaMA/"/>
    <id>https://tlylft.github.io/LLM/05_LLaMA/</id>
    <published>2023-05-31T09:11:11.000Z</published>
    <updated>2023-08-28T13:59:14.090Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>LLaMA发展详解：<a href="https://agi-sphere.com/llama-models/" target="_blank" rel="noopener">https://agi-sphere.com/llama-models/</a><br>Paper: LLaMA: Open and Efficient Foundation Language Models<br>meta AI blog: <a href="https://ai.facebook.com/blog/large-language-model-llama-meta-ai/" target="_blank" rel="noopener">https://ai.facebook.com/blog/large-language-model-llama-meta-ai/</a></p></blockquote><h1 id="1-LLaMA目标"><a href="#1-LLaMA目标" class="headerlink" title="1. LLaMA目标"></a>1. LLaMA目标</h1><p>Meta(Facebook)做LLM没有了先发优势，于是将LLaMA做了开源。<br>LLaMA 的目标是为有限的推理预算构建性能最佳的模型，例如，可以使用小于 10GB 的 VRAM 在 NVIDIA 3090 上运行。</p><h1 id="2-预训练过程"><a href="#2-预训练过程" class="headerlink" title="2. 预训练过程"></a>2. 预训练过程</h1><h2 id="2-1-训练数据"><a href="#2-1-训练数据" class="headerlink" title="2.1. 训练数据"></a>2.1. 训练数据</h2><p><strong>1.4T token:<br>主要包括: 维基百科、github, 科学数据，科学和工程的高质量问答，CommonCrawl清洗后的数据。<br>分词器使用<a href="https://github.com/google/sentencepiece" target="_blank" rel="noopener">SentencePiece</a>进行字节对编码。</strong><br>detail:</p><ul><li>English CommonCrawl (67%): Removed non-English text and duplicated content. Only includes pages used as references in Wikipedia.</li><li>C4 (15%): A cleaned version of CommonCrawl. The same filters were applied.</li><li>Github (4.5%): Public GitHub dataset available on Google BigQuery.</li><li>Wikipedia (4.5%): From June-August 2022 period covering 20 languages.</li><li>Gutenberg and Books3 (4.5%): Both are book datasets.</li><li>ArXiv (45%): Scientific data.</li><li>StackExchange (2%): High-quality Q&amp;As covering science and engineering topics.<h2 id="2-2-训练资源"><a href="#2-2-训练资源" class="headerlink" title="2.2. 训练资源"></a>2.2. 训练资源</h2>65B模型训练： 2048块A100 80GB GPU, 大约21天。</li></ul><h1 id="3-LLaMA变体发展"><a href="#3-LLaMA变体发展" class="headerlink" title="3. LLaMA变体发展"></a>3. LLaMA变体发展</h1><p>development:<br>| Model               | Size        | Training data                                           |<br>|——————————-|——————-|————-|<br>| LLaMA (base model)   | 7B, 13B, 33B, 65B | Various     |<br>| Alpaca              | 7B, 13B     | 52k GPT-3 instructions  |<br>| Vicuna              | 7B, 13B     | 70k ChatGPT conversations  |<br>| Koala-distill       | 7B, 13B     | 117k cleaned ChatGPT conversations  |<br>| GPT4-x-Alpaca       | 13B         | 20k GPT4 instructions        |<br>| WizardML            | 7B          | 70k instructions synthesized with ChatGPT/GPT-3|<br>| OpenAssistant LLaMA | 13B, 30B    | 600k human interactions (OpenAssistant Conversations) |</p><h1 id="4-LLaMA模型架构"><a href="#4-LLaMA模型架构" class="headerlink" title="4. LLaMA模型架构"></a>4. LLaMA模型架构</h1><p>LLaMA是一种类似于GPT的transformer模型，优化以下结构：</p><ol><li>Normalize the input of each transformer sub-layer to improve training stability.<br>对每个transformer子层的输入进行Normalize，以提高训练稳定性。</li><li>Use SwiGLU instead of ReLU to improve performance.<br>使用 <a href>SwiGLU</a> 而不是 ReLU 来提高性能。</li><li>Use rotary embedding instead of absolute positioning to improve performance.<br>使用<a href>rotary embedding</a>而不是absolute positioning来提高性能。</li></ol><div class="table-container"><table><thead><tr><th>Parameters</th><th>Layers</th><th>Attention heads</th><th>Embedding dimension</th></tr></thead><tbody><tr><td>7B</td><td>6.7B</td><td>32</td><td>32</td><td>4,096</td></tr><tr><td>13B</td><td>13B</td><td>40</td><td>40</td><td>5,120</td></tr><tr><td>33B</td><td>33B</td><td>60</td><td>52</td><td>6,656</td></tr><tr><td>65B</td><td>65B</td><td>80</td><td>64</td><td>8,192</td></tr></tbody></table></div><h1 id="变体微调"><a href="#变体微调" class="headerlink" title="变体微调"></a>变体微调</h1><h2 id="stanford-alpaca"><a href="#stanford-alpaca" class="headerlink" title="stanford alpaca"></a>stanford alpaca</h2><p>用gpt3/3.5的接口数据生成指令数据，</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;LLaMA发展详解：&lt;a href=&quot;https://agi-sphere.com/llama-models/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://agi-sphere.com/llama-models/
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>xgboost回归预测</title>
    <link href="https://tlylft.github.io/machine_learning/time_series/xgboost_regression/"/>
    <id>https://tlylft.github.io/machine_learning/time_series/xgboost_regression/</id>
    <published>2023-05-26T12:53:14.000Z</published>
    <updated>2023-06-26T08:44:13.121Z</updated>
    
    <content type="html"><![CDATA[<p>api文档： <a href="https://xgboost.readthedocs.io/en/stable/python/index.html" target="_blank" rel="noopener">https://xgboost.readthedocs.io/en/stable/python/index.html</a></p><p>原理解释：<a href="https://baijiahao.baidu.com/s?id=1665813533927052719&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">https://baijiahao.baidu.com/s?id=1665813533927052719&amp;wfr=spider&amp;for=pc</a><br>说清楚的原理：<a href="https://www.cnblogs.com/mantch/p/11164221.html" target="_blank" rel="noopener">https://www.cnblogs.com/mantch/p/11164221.html</a><br><a href="https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md" target="_blank" rel="noopener">https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.3%20XGBoost/3.3%20XGBoost.md</a></p><h1 id="1-intro"><a href="#1-intro" class="headerlink" title="1. intro"></a>1. intro</h1><p>XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。<br>XgBoost是梯度提升树GBDT（Gradient Boosting Decision Tree）算法的一种变体，能够更快的、更高效率的训练模型。它将多个决策树集成在一起，以提高预测准确性。<br>XgBoost在气量预测中的应用比较广泛，主要是因为它能够处理大规模、高维度的数据集，并且能够在较短时间内生成高质量的预测模型。例如，在天然气供应链中，XgBoost可以被用于预测气量需求，提前做好供应准备。</p><h2 id="1-1-regression-tree"><a href="#1-1-regression-tree" class="headerlink" title="1.1. regression tree"></a>1.1. regression tree</h2><p><img src="/machine_learning/time_series/xgboost_regression/2023-06-26-16-37-31.png" alt></p><h2 id="1-2-XGBoost与GBDT有什么不同"><a href="#1-2-XGBoost与GBDT有什么不同" class="headerlink" title="1.2. XGBoost与GBDT有什么不同"></a>1.2. XGBoost与GBDT有什么不同</h2><p>除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。</p><p>GBDT是机器学习算法，XGBoost是该算法的工程实现。<br>在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。<br>GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。<br>传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。<br>传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。<br>传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。</p><h2 id="1-3-为什么XGBoost要用泰勒展开，优势在哪里？"><a href="#1-3-为什么XGBoost要用泰勒展开，优势在哪里？" class="headerlink" title="1.3. 为什么XGBoost要用泰勒展开，优势在哪里？"></a>1.3. 为什么XGBoost要用泰勒展开，优势在哪里？</h2><p>XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。</p><h2 id="线性树结构的劣势"><a href="#线性树结构的劣势" class="headerlink" title="线性树结构的劣势"></a>线性树结构的劣势</h2><h1 id="2-model"><a href="#2-model" class="headerlink" title="2. model"></a>2. model</h1><ol><li>添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数f(x)，去拟合上次预测的残差。</li><li>训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数</li><li>需要将每棵树对应的分数加起来就是该样本的预测值。</li></ol><h2 id="2-1-举例："><a href="#2-1-举例：" class="headerlink" title="2.1. 举例："></a>2.1. 举例：</h2><p>预测不同人游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。<br><img src="/machine_learning/time_series/xgboost_regression/2023-06-26-16-38-07.png" alt><br>就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示：<br><img src="/machine_learning/time_series/xgboost_regression/2023-06-26-16-38-15.png" alt><br>这跟上文介绍的GBDT乃异曲同工，事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义。XGBoost的目标函数如下图所示：<br><img src="/machine_learning/time_series/xgboost_regression/2023-06-26-16-39-37.png" alt></p><h1 id="3-Multiple-Outputs"><a href="#3-Multiple-Outputs" class="headerlink" title="3. Multiple Outputs"></a>3. Multiple Outputs</h1><p>并行多模型运算<br>分类：<a href="https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html" target="_blank" rel="noopener">https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html</a><br>回归：<a href="https://xgboost.readthedocs.io/en/stable/python/examples/multioutput_regression.html#sphx-glr-python-examples-multioutput-regression-py" target="_blank" rel="noopener">https://xgboost.readthedocs.io/en/stable/python/examples/multioutput_regression.html#sphx-glr-python-examples-multioutput-regression-py</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;api文档： &lt;a href=&quot;https://xgboost.readthedocs.io/en/stable/python/index.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://xgboost.readthedocs.io
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/categories/machine-learning/"/>
    
      <category term="time series" scheme="https://tlylft.github.io/categories/machine-learning/time-series/"/>
    
    
      <category term="time series" scheme="https://tlylft.github.io/tags/time-series/"/>
    
  </entry>
  
  <entry>
    <title>Neural Prophet</title>
    <link href="https://tlylft.github.io/machine_learning/time_series/nerualprophet/"/>
    <id>https://tlylft.github.io/machine_learning/time_series/nerualprophet/</id>
    <published>2023-05-26T12:39:59.000Z</published>
    <updated>2023-06-26T08:35:15.993Z</updated>
    
    <content type="html"><![CDATA[<p>github: <a href="https://github.com/ourownstory/neural_prophet" target="_blank" rel="noopener">https://github.com/ourownstory/neural_prophet</a><br>官方教程：<a href="https://neuralprophet.com/tutorials/tutorial01.html" target="_blank" rel="noopener">https://neuralprophet.com/tutorials/tutorial01.html</a></p><h1 id="intro"><a href="#intro" class="headerlink" title="intro"></a>intro</h1><p>NeuralProphet是Facebook Research开发的一种新型时间序列预测模型，它是Prophet的升级版。它建立在PyTorch之上，并受到Facebook Prophet和AR-Net库的极大启发。NeuralProphet将Prophet里的模型结构改成了神经网络的形式，从而提高了模型的有效性和灵活性。相比于Prophet，NeuralProphet更能适应非线性的时间序列问题，比如气量预测中存在的带噪声的时间序列。它可以根据数据自动优化模型参数，提升预测的准确性。</p><h2 id="Neural-Prophet-vs-prophet"><a href="#Neural-Prophet-vs-prophet" class="headerlink" title="Neural Prophet vs. prophet"></a>Neural Prophet vs. prophet</h2><p><strong>功能改进：</strong><br>1）    一次可以预测未来多步<br>2）    模型结构改成了神经网络的形式，提高了模型的有效性和灵活性<br>3）    加入历史时间t预测值的自回归效应（如：参考历史多日的气量）<br>4）    加入历史时间t外生变量的回归效应（如：参考历史多日的温度）<br>5）    加入未来时间t外生变量的回归效应（如：预测多日的温度）</p><p>根据NeuralProphet的文档，增加的技术点：</p><ul><li>使用PyTorch的Gradient Descent进行优化，使建模过程比Prophet快得多</li><li>使用AR-Net建模时间序列自相关（也称为序列相关）</li><li>自定义损失和指标</li><li>具有前馈神经网络的可配置非线性层</li></ul><p>区别：</p><ul><li>不支持再训练，可以直接传入历史数据和未来特征进行预测。<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><blockquote><p>原理参考：<br><a href="https://towardsdatascience.com/in-depth-understanding-of-neuralprophet-through-a-complete-example-2474f675bc96" target="_blank" rel="noopener">1.In-Depth Understanding of NeuralProphet through a Complete Example</a><br><a href="https://liumin.blog.csdn.net/article/details/126489531" target="_blank" rel="noopener">2.时间序列论文: NeuralProphet: Explainable Forecasting at Scale</a><br><a href="https://zhuanlan.zhihu.com/p/458061787" target="_blank" rel="noopener">3.NeuralProphet模型概述:详细公式</a></p></blockquote></li></ul><p>yˆt = T(t) + S(t) + E(t) + F(t) + A(t) + L(t)<br>T(t) = 时间 t 的趋势<br>S(t) = 时间 t 的季节性影响<br>E(t) = 时间 t 的事件和假日效应<br>F(t) = 未来已知外生变量在时间 t 的回归效应<br>A(t) = 基于过去观察的时间 t 的自回归效应<br>L(t) = t 时刻外生变量滞后观测的回归效应<br>详细公式参考3中链接</p><h1 id="预测梳理"><a href="#预测梳理" class="headerlink" title="预测梳理"></a>预测梳理</h1><ol><li>数据准备<br>ds y event1 event2 regression_1  future_regresssion_1</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;github: &lt;a href=&quot;https://github.com/ourownstory/neural_prophet&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/ourownstory/neural_prop
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/categories/machine-learning/"/>
    
      <category term="time series" scheme="https://tlylft.github.io/categories/machine-learning/time-series/"/>
    
    
      <category term="time series" scheme="https://tlylft.github.io/tags/time-series/"/>
    
  </entry>
  
  <entry>
    <title>LLM（3）- chatgpt</title>
    <link href="https://tlylft.github.io/LLM/03_chatgpt/"/>
    <id>https://tlylft.github.io/LLM/03_chatgpt/</id>
    <published>2023-05-16T00:52:19.000Z</published>
    <updated>2023-08-10T09:30:33.201Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/shizhediao/ChatGPTPapers" target="_blank" rel="noopener">https://github.com/shizhediao/ChatGPTPapers</a><br>CSDN Blogs：<a href="https://blog.csdn.net/ganxiwu9686" target="_blank" rel="noopener">https://blog.csdn.net/ganxiwu9686</a><br>Zhihu Column: <a href="https://zhuanlan.zhihu.com/p/58108759" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/58108759</a></p><h1 id="类chatgpt模型"><a href="#类chatgpt模型" class="headerlink" title="类chatgpt模型"></a>类chatgpt模型</h1><p>openAI: gpt3.0<br>openAI: gpt3.5 更大的数据量+context learning， 用2021年之前的数据进行的训练<br>复旦：Moss</p><p>2023.03.15 google AI<br>2023.03.15 openAI gpt4.0<br>2023.03.16 百度 文心一言<br>2023.03.16 微软 Copilot<br>2023.03.22 google bardd<br>2023.03.22 github Copilot X<br>2023.03.23 openAI 插件功能，赋予chatgpt使用工具、联网、运算的能力</p><h1 id="chatGPT基本概念"><a href="#chatGPT基本概念" class="headerlink" title="chatGPT基本概念"></a>chatGPT基本概念</h1><p>chat Generative Pre-trained Transformer</p><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><ul><li>之前的大规模模型(bert,bass)在输出方面和人类真实想要的结果往往无法很好对齐。</li><li>ChatGPT的一个主要训练方法是在GPT3的基础之上，通过人类提供反馈的强化学习(Reinforcement Learning from Human Feedback（RLHF）)，来让模型的输出逼近人类的意图。需要人工标注问答对的评分。</li><li>更加聚焦在人类的意图</li><li>他在很多任务上的表现效果都很好，同时他也会去拒绝用户提出的一些不合理，不合宜的请求。能够通过交互式的方式来逼近真实想要的结果。可以进一步对我们领域做一些数据增强的工作<h2 id="发展路线"><a href="#发展路线" class="headerlink" title="发展路线"></a>发展路线</h2><strong>instructGPT(基于提示学习的一系列模型)</strong>：通过设定prompt的模板，让模型输出模板的结果，而不需要进行参数的再学习，但要求设计出一套合理适用的模板，模板设计不好对输出结果有很大的影响，主要研究如何构造更好的模板让大模型进行理解。如： “这个电影剧情紧凑，非常感人” -&gt; 情感正向。 之前通过做意图训练和学习，但是prompt是让预训练模型输出完型填空：“这个电影剧情紧凑，非常感人，以上评述是说这是一个__电影” —&gt; 好<br><strong>GPT3.5(大规模预训练语言模型)</strong>，参数量超1750亿。高校、小公司做不起来。成本太高。结合prompt learning去做。<br>ChatGPT模型(高质量数据标注和反馈学习)， 专注于1. 如何用高质量数据对大规模模型进行微调 2. 如何结合人类反馈用强化学习逼近人类意图。<br><strong>ChatGPT模型</strong>高质量数据标注以及反馈学习<h2 id="技术手段"><a href="#技术手段" class="headerlink" title="技术手段"></a>技术手段</h2><img src="/LLM/03_chatgpt/2023-05-16-15-23-44.png" alt><strong>引入“人工标注数据+强化学习”</strong></li></ul><p>gpt用自回归的方式（transformer decoder）生成，侧重于做生成任务，bert用自编码的方式（transformer encoder）生成,侧重于做理解性任务<br><strong>step1： 收集高质量的数据，有监督微调语言模型</strong><br>为了让GPT 3.5初步具备理解指令中蕴含的意图，首先会从测试用户提交的prompt(就是指令或问题)中随机抽取一批，靠专业的标注人员，给出指定prompt的高质量答案，然后用这些人工标注好的<prompt,answer>数据来Fine-tune GPT 3.5模型</prompt,answer></p><p><strong>step2: 训练回报模型（Reward Model,RM）</strong></p><ol><li>随机抽样一批用户提交的prompt(大部分和第一阶段的相同)，使用第一阶段Fine-tune好的冷启动模型，对于每个prompt，由冷启动模型生成K个不同的回答，于是模型产生出了<prompt,answer1>,<prompt,answer2>….<prompt,answerK>数据</prompt,answerK></prompt,answer2></prompt,answer1></li><li>标注人员对K个结果按照很多标准（上面提到的相关性、富含信息性、有害信息等诸多标准）综合考虑进行排序打分</li></ol><p><strong>step3: 利用上一阶段学好的RM模型，靠RM打分结果来更新预训练模型参数</strong></p><ol><li>从用户提交的prompt里随机采样一批新的命令（指的是和第一第二阶段不同的新的prompt，相当于测试数据）</li><li>再使用PPO算法微调模型的参数</li><li>重复step2 和 step 3，反复迭代</li></ol><p>在一轮session对话中，会倾向于取悦用户的意图，如果反复强调1+1=3，模型会逐步接受，同意1+1=3，但释放对话后重新发起，它仍会返回1+1=2？  为什么</p><h2 id="存在的问题和可能的原因（局限性）"><a href="#存在的问题和可能的原因（局限性）" class="headerlink" title="存在的问题和可能的原因（局限性）"></a>存在的问题和可能的原因（局限性）</h2><p><strong>缺点</strong><br>以下的内容主要是来自于推特的一些失败案例，大致可以分成9类+Other<br> 推理 Reasoning： 空间推理（相对位置，导航等）物理推理和时间推理（时间发生先后顺序，事件持续时间）和心理推理（关于人物的行为或者心里要素）存在问题<br> 逻辑 Logic: 偏向于需要三段论、演绎、归纳，逐渐得到结论的过程<br> Math and Arithmetic：大数相乘，求根，计算能力(尤其是分数)，以及无理数的加减法存在问题<br> Factual Errors：有些简单的通过搜索引擎很容易找到答案，但是他会回答错误，数据基于2021年之前有关，而且基于概率计算，极大可能在编造事实。<br> Bias and Discrimination：种族偏见<br> Wit and Humor：没有幽默特性，但可以生成幽默的句子<br> Coding：可以提供一些功能性代码，但不能解决复杂的编程、以及复杂代码的debug<br> Syntactic Structure, Spelling, and Grammar：大部分可以解决，以字符为级别的往往回答错误，缺少字符集语义表示<br> Self Awareness：伦理问题，无自我意识</p><p><strong>可能的原因</strong></p><ul><li>缺乏世界模型——像ChatGPT这样的模型没有“世界模型”，因为它们对物理和社会世界没有全面的理解，也没有能力推理不同概念和实体之间的关系。他们只能根据从训练数据中学习到的模式生成文本。</li><li>缺乏检索模型——像ChatGPT这样的模型不具备从外部存储器或数据库检索信息的能力。这意味着他们可能无法准确回忆事实。（对一些历史事件回答错误）目前和Biying的结合已经能做了。但是下架了说明还有一些问题。</li><li>缺少字符级嵌入——许多像ChatGPT这样的模型都没有使用字符级嵌入进行训练。这可能会导致词汇表外的单词和拼写错误，以及对单词中不同字符之间的关系缺乏理解。（使用的词向量表示）</li><li>生成一些取悦人类的话术，无法评判生成数据的准确性<h2 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h2>chatgpt的成功，证明了大模型技术路线的正确性。意味着AI从之前大数据统计分类的阶段，走向类人逻辑沟通极端。<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1>文本分类/情感分析<br>序列标注：POS， 命名实体识别， 语义标注<br>句子关系判断：Entailment/QA/自然语言推理<br>生成式任务(机器翻译/文本摘要/ 智能问答）<br>其他应用：修复代码，写规划<br><img src="/LLM/03_chatgpt/2023-05-16-16-43-10.png" alt><h1 id="规模"><a href="#规模" class="headerlink" title="规模"></a>规模</h1>训练数据：45TB<h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1></li></ul><ol><li><a href="https://openai.com/" target="_blank" rel="noopener">https://openai.com/</a><br>gpt-3.5-turbo模型 1000token(请求和返回) 0.0002美元</li><li>new bing<br>输入：2000字符<br>指令：简洁明了，明确目的</li></ol><p><strong>区分gpt3.5和4.0</strong><br>问：鲁迅为什么暴打周树人？<br>3.5：#@￥I#$(#的理由<br>4.0：鲁迅和周树人是一个人<br>问：树上9只鸟，打掉一只，还剩几只？<br>3.5:8只<br>4.0: 0只，其他被吓跑了</p><h1 id="微调"><a href="#微调" class="headerlink" title="微调"></a>微调</h1><p>微调： 1w句子左右</p><h1 id="未来展望"><a href="#未来展望" class="headerlink" title="未来展望"></a>未来展望</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/shizhediao/ChatGPTPapers&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/shizhediao/ChatGPTPapers&lt;/a&gt;&lt;br&gt;C
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM（1）-大语言模型简介</title>
    <link href="https://tlylft.github.io/LLM/01_intro/"/>
    <id>https://tlylft.github.io/LLM/01_intro/</id>
    <published>2023-05-16T00:52:19.000Z</published>
    <updated>2023-08-28T15:03:42.177Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p><a href="https://mp.weixin.qq.com/s/NOcUlNPOXZheXJ9yX4wrLQ" target="_blank" rel="noopener">大模型选型的一点思考</a><br><a href="https://blog.csdn.net/v_JULY_v/article/details/128579457" target="_blank" rel="noopener">ChatGPT技术原理解析：从RL之PPO算法、RLHF到GPT4、instructGPT</a><br><a href="https://github.com/RUCAIBox/LLMSurvey" target="_blank" rel="noopener">大语言模型总数</a><br><a href="https://github.com/liguodongiot/llm-action" target="_blank" rel="noopener">github分享的大模型相关技术原理以及实战经验!!!!!!!</a></p></blockquote><p>[TOC]</p><h1 id="1-LLM"><a href="#1-LLM" class="headerlink" title="1. LLM"></a>1. LLM</h1><p>大语言模型（LLM）是指使用大量文本数据训练的深度学习模型，可以生成自然语言文本或理解语言文本的含义。大语言模型可以处理多种自然语言任务，如文本分类、问答、对话等，是通向人工智能的一条重要途径。</p><p>现在是2023年5月，截止目前，网络上已经开源了众多的LLM，如何用较低的成本，判断LLM的基础性能，选到适合自己任务的LLM，成为一个关键。</p><h1 id="2-模型发展历史"><a href="#2-模型发展历史" class="headerlink" title="2. 模型发展历史"></a>2. 模型发展历史</h1><p><img src="/LLM/01_intro/2023-08-15-08-40-57.png" alt><br>| 时间 | 公司 | 模型 | 描述 |<br>| —- | —- | —- | —- |<br>| 2017 |  | 微积分、概率统计、最优化、策略梯度、TRPO算法(2015年提出) |  |<br>|2017年6月| OpenAI联合DeepMind| RLHF| Deep Reinforcement Learning from Human Preferences，即基于人类偏好的深度强化学习|<br>|2017年6月| | <a href="https://blog.csdn.net/v_JULY_v/article/details/127411638" target="_blank" rel="noopener">Transformer</a>| self-attention|<br>|2017年7月|OpenAI|<a href="https://blog.csdn.net/v_JULY_v/article/details/128965854" target="_blank" rel="noopener">PPO算法</a>|TRPO算法的改进|<br>| 2018 | Google | BERT |  |<br>| 2018.07 | OpenAI | GPT(Generative Pre-trained Transformer) | 基于Transformer-Decoder的Masked Self-Attention |<br>| 2019.02 | OpenAI | GPT2 | 融合prompt learning的GPT2，prompt learning的意义在于不用微调也能做任务 |<br>| 2019年10月 | Google | T5(transfer text to text transformer) | 基于transformer的encoder-decoder架构，区别于BERT的编码器架构与GPT的解码器架构 |<br>| 2020年05月 | OpenAI | GPT3.0 | 参数规模到了1750亿，终于真正做到预训练之后<strong>不用再微调模式</strong>，通过In-context learning(简称ICL)开启prompt新范式 |<br>| 2021年7月 | OpenAI | Codex | 通过对GPT3进行大量的代码训练迭代而出Codex，从而具备代码/推理能力 159G的python代码微调|<br>| 2021年9月 | Google | FLAN | 基于指令微调技术Instruction Fine-Tuning (IFT)的大模型 |<br>| 2021年第四季度 | OpenAI | <strong>GPT3.5</strong> | 更大的数据量+context learning，用2021年之前的数据进行的训练 |<br>| 2022年1月 | Google | CoT | 研究者提出的思维链技术(Chain of Thought，简称CoT) ，<strong>模仿推理能力</strong>|<br>| 2022年3月 | OpenAI | instructGPT | GPT3 + instruction tuning + RLHF + PPO |<br>| 2022年11月 |OpenAI- | <strong>ChatGPT</strong> | 语言模型层面的核心架构是GPT3.5(基于Transformer-Decoder的Masked Self-Attention且融合了Codex的代码/推理能力、instruction tuning等技术) + RLHF + PPO3 |<br>| 2022年1月 | Google | LaMDA |发布LaMDA论文『 LaMDA: Language Models for Dialog Applications』|<br>| 2022年4月 | Google | PaLM | 提出PaLM: Scaling Language Modeling with Pathways，5400亿参数|<br>| 2022年10月 | Google | Flan-T5 | 提出Flan-T5|<br>| 23年3月6日 | Google | PaLM-E | 提出多模态LLM模型|<br>| 2023.02.24 | Facebook | Meta llama | 基础模型|<br>| 2023.03.15 | OpenAI | <strong>GPT4.0</strong> | 增加了多模态(支持图片的输入形式)|<br>| 2023.03.16 | 百度 | <strong>文心一言</strong> | |<br>| 2023.03.17 | 微软 | Microsoft 365 Copilot | 集成GPT4的能力，实现自动化办公|<br>| 2023.03.22 | Google | Bard | |<br>| 2023.03.23 | Github | Copilot X | |<br>| 2023.03.24 | OpenAI | - | 插件功能，赋予chatgpt使用工具、联网、运算的能力|</p><h3 id="2-1-GPT3：In-context-learning正式开启prompt新范式-小样本学习"><a href="#2-1-GPT3：In-context-learning正式开启prompt新范式-小样本学习" class="headerlink" title="2.1. GPT3：In-context learning正式开启prompt新范式(小样本学习)"></a>2.1. GPT3：In-context learning正式开启prompt新范式(小样本学习)</h3><p> GPT3在0样本、单样本、小样本下的突出能力<br>GPT3简单来说，就是参数规模大(有钱)、训练数据规模大(多金)、效果出奇好，具体而言<br>为形象描述，举一个GPT3在0样本、单样本、少量样本下的机器翻译使用范例，如下图<br><img src="/LLM/01_intro/2023-06-18-14-08-39.png" alt></p><h3 id="2-2-Prompt技术的升级与创新：指令微调技术-IFT-与思维链技术-CoT"><a href="#2-2-Prompt技术的升级与创新：指令微调技术-IFT-与思维链技术-CoT" class="headerlink" title="2.2. Prompt技术的升级与创新：指令微调技术(IFT)与思维链技术(CoT)"></a>2.2. Prompt技术的升级与创新：指令微调技术(IFT)与思维链技术(CoT)</h3><p>OpenAI的GPT3虽然不再微调模型(pre-training + prompt)，但Google依然坚持预训练 + 微调的模式<br>Google提出FLAN大模型：基于指令微调技术Instruction Fine-Tuning (IFT)<br>基于思维链(Chain-of-thought)技术下的prompt</p><p>因此导致后续OpenAI的chatgpt不得不关注再次关注微调技术，也转向指令微调</p><h1 id="3-开源模型-截止至2023-06-08"><a href="#3-开源模型-截止至2023-06-08" class="headerlink" title="3. 开源模型 截止至2023/06/08"></a>3. <a href="https://blog.csdn.net/zengNLP/article/details/131119734" target="_blank" rel="noopener">开源模型 截止至2023/06/08</a></h1><blockquote><p><a href="https://huaweidevelopers.csdn.net/64c1290ebfca273ff3548e81.html" target="_blank" rel="noopener">https://huaweidevelopers.csdn.net/64c1290ebfca273ff3548e81.html</a></p></blockquote><div class="table-container"><table><thead><tr><th>time</th><th>model</th><th>github</th><th>owner</th><th>language</th><th>desc</th></tr></thead><tbody><tr><td></td><td><a href="https://arxiv.org/abs/2302.13971v1" target="_blank" rel="noopener">LLaMA</a></td><td><a href="https://github.com/facebookresearch/llama" target="_blank" rel="noopener">代码</a></td><td>meta</td><td>英</td><td>在模型参数量降低的同时，增加训练的数据量，这样可以保证模型的效果。LLaMA完全是在公共开源预训练数据上训练。并且取得相当不错的效果，LaMA-13B在绝大部分的benchmarks上超越了GPT-3(175 B)，并且LLaMA-65B的效果能够和最好的大模型，Chinchilla-70B以及PaLM-540B相比。</td></tr><tr><td></td><td>[Chinchilla]</td><td></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td><a href="https://chatglm.cn/blog" target="_blank" rel="noopener">chatGLM-6B</a></td><td><a href="https://github.com/THUDM/ChatGLM-6B" target="_blank" rel="noopener">代码</a></td><td>清华</td><td>中英</td><td>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 General Language Model (GLM) 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。 ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答。</td></tr><tr><td></td><td><a href="https://arxiv.org/abs/2212.10560" target="_blank" rel="noopener">Alpaca</a></td><td><a href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" rel="noopener">代码</a></td><td>斯坦福</td><td>英</td><td>一个遵循指令的LLaMA模型</td></tr><tr><td></td><td><a href="https://arxiv.org/pdf/2305.03025v1.pdf" target="_blank" rel="noopener">PandaLLM</a></td><td><a href="https://github.com/dandelionsllm/pandallm" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>中英</td><td>海外中文开源大语言模型：Panda 系列语言模型目前基于 Llama-7B, -13B, -33B, -65B 进行中文领域上的持续预训练, 使用了接近 15M 条数据, 并针对推理能力在中文 benchmark 上进行了评测, 希望能够为中文自然语言处理领域提供具有泛用性的通用基础工具.</td></tr><tr><td></td><td>[GPT4ALL]</td><td><a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>xxx</td><td>可在cpu上运行的开源LLM</td></tr><tr><td></td><td><a href="https://arxiv.org/pdf/2304.01097.pdf" target="_blank" rel="noopener">DoctorGLM (MedicalGPT-zh v2)</a></td><td><a href="https://github.com/xionghonglin/DoctorGLM" target="_blank" rel="noopener">代码</a> <a href="https://huggingface.co/zhaozh/medical_chat-en-zh" target="_blank" rel="noopener">huggingface</a></td><td>xxx</td><td>中英医疗</td><td>基于 ChatGLM-6B的中文问诊模型</td></tr><tr><td></td><td><a href="https://arxiv.org/pdf/2304.01097.pdf" target="_blank" rel="noopener">MedicalGPT-zh v1</a></td><td><a href="https://github.com/MediaBrain-SJTU/MedicalGPT-zh" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>中英医疗</td><td>基于ChatGLM-6B LoRA 16-bit指令微调的中文医疗通用模型。基于共计28科室的中文医疗共识与临床指南文本，我们生成医疗知识覆盖面更全，回答内容更加精准的高质量指令数据集。以此提高模型在医疗领域的知识与对话能力。</td></tr><tr><td></td><td>[Cornucopia-LLaMA-Fin-Chinese]</td><td><a href="https://github.com/jerry1993-tech/Cornucopia-LLaMA-Fin-Chinese/tree/main" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>中文金融</td><td>基于中文金融知识的LLaMA微调模型：经过中文金融知识指令精调/指令微调(Instruct-tuning) 的LLaMA-7B模型。通过中文金融公开数据+爬取的金融数据构建指令数据集，并在此基础上对LLaMA进行了指令微调，提高了 LLaMA 在金融领域的问答效果。</td></tr><tr><td></td><td>[minGPT]</td><td><a href="https://github.com/karpathy/minGPT" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td>[InstructGLM]</td><td><a href="https://github.com/yanqiangmiffy/InstructGLM" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>中英指令</td><td>基于ChatGLM-6B+LoRA在指令数据集上进行微调。</td></tr><tr><td></td><td>[FastChat]</td><td><a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td>[Luotuo-Chinese-LLM]</td><td><a href="https://github.com/LC1332/Luotuo-Chinese-LLM" target="_blank" rel="noopener">代码</a></td><td>商汤&amp;华中师范</td><td>中文</td><td>开源中文大语言模型</td></tr><tr><td></td><td>[CamelBell-Chinese-LoRA]</td><td><a href="https://github.com/LC1332/CamelBell-Chinese-LoRA" target="_blank" rel="noopener">代码</a></td><td>商汤&amp;华中师范</td><td>中文</td><td>开源中文大语言模型</td></tr><tr><td></td><td>[alpaca-lora]</td><td><a href="https://github.com/tloen/alpaca-lora" target="_blank" rel="noopener">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td>2023.06</td><td><a href="xxx">chatGLM2</a></td><td><a href="https://github.com/THUDM/ChatGLM2-6B" target="_blank" rel="noopener">代码</a> <a href="https://huggingface.co/THUDM/chatglm2-6b" target="_blank" rel="noopener">huggingface</a></td><td>清华</td><td>中英</td><td>1.基座模型升级，性能更强大 2. 支持8K-32k的上下文 3. 推理性能提升了42%</td></tr><tr><td></td><td><a href>baichuan</a></td><td><a href="xxx">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td><a href="xxx">LLama-2</a></td><td><a href="xxx">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td><a href="xxx"></a></td><td><a href="xxx">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr><tr><td></td><td><a href="xxx"></a></td><td><a href="xxx">代码</a></td><td>xxx</td><td>xxx</td><td>xxx</td></tr></tbody></table></div><h1 id="4-多模态"><a href="#4-多模态" class="headerlink" title="4. 多模态"></a>4. 多模态</h1><h3 id="4-1-LLaVA"><a href="#4-1-LLaVA" class="headerlink" title="4.1. LLaVA"></a>4.1. LLaVA</h3><h3 id="4-2-MiniGPT-4"><a href="#4-2-MiniGPT-4" class="headerlink" title="4.2. MiniGPT-4"></a>4.2. MiniGPT-4</h3><h3 id="4-3-KOSMOS-1-微软"><a href="#4-3-KOSMOS-1-微软" class="headerlink" title="4.3. KOSMOS-1 微软"></a>4.3. KOSMOS-1 微软</h3><p>《Language Is Not All You Need: Aligning Perception with Language Models》<br>论文地址：<a href="https://arxiv.org/pdf/2302.14045.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/2302.14045.pdf</a></p><p>项目地址：<a href="https://github.com/microsoft/unilm" target="_blank" rel="noopener">https://github.com/microsoft/unilm</a><br><a href="https://www.thepaper.cn/newsDetail_forward_22122932?commTag=true" target="_blank" rel="noopener">https://www.thepaper.cn/newsDetail_forward_22122932?commTag=true</a></p><h3 id="4-4-X-LLM-中文多模态大模型"><a href="#4-4-X-LLM-中文多模态大模型" class="headerlink" title="4.4. X-LLM 中文多模态大模型"></a>4.4. X-LLM 中文多模态大模型</h3><p>简介：中文多模态大模型力作，中科院发布多模态 ChatGPT，图片、语言、视频都可以 Chat<br>来源：中科院 X-LLM<br>Paper: <a href="https://arxiv.org/pdf/2305.04160.pdf" target="_blank" rel="noopener">X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages</a><br>主页：<a href="https://x-llm.github.io/" target="_blank" rel="noopener">https://x-llm.github.io/</a><br>思路：</p><ol><li>假设“GPT-4 的多模态能力来源于其更先进，更大的语音模型，即 GPT-4 是用语言的形式表达出了其他模态的内容”</li></ol><p>这个假设也就是讲，<strong>需要将多模态的数据“对齐”到语言数据之中，然后再投入大模型以获得多模态能力</strong>， 在这个假设的基础上，作者提出了 X2L 接口，其中 X 意味着多模态数据，而 L 则表示语言，<strong>X2L 接口即将多个单模态编码器与一个大规模语言模型（LLM）进行对齐</strong>。其中，图像接口 I2L 采用 BLIP-2 中的 Q-Former，视频接口 V2L 复用图像接口的参数，但是考虑了编码后的视频特征，语言接口 S2L 采用 CIF 与 Transformer 结构将语音转换为语言。整个 X-LLM 的训练包含三个阶段，分别是（1）转换多模态信息；（2）将 X2L 对齐到 LLM；（3）将多模态数据整合到 LLM 中。<br><img src="/LLM/01_intro/2023-05-21-21-01-19.png" alt></p><h1 id="5-主流模型参数性能对比"><a href="#5-主流模型参数性能对比" class="headerlink" title="5. 主流模型参数性能对比"></a>5. 主流模型参数性能对比</h1><p>1B = 10亿</p><div class="table-container"><table><thead><tr><th>model</th><th>Parameters</th><th>Layers</th><th>Attention heads</th><th>Embedding dimension</th><th>gpu</th><th>train data</th></tr></thead><tbody><tr><td>transformer</td><td>-</td><td>-</td><td>8</td><td>8*64=512</td><td>-</td></tr><tr><td>GPT-1</td><td>1.17亿</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPT-2</td><td>1.5B</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>GPT-3</td><td>175B</td><td>96</td><td>96</td><td>96*128=12888</td><td>-</td><td>45T</td></tr><tr><td>LLaMA 7B</td><td>6.7B</td><td>32</td><td>32</td><td>4,096</td><td>-</td></tr><tr><td>LLaMA 13B</td><td>13B</td><td>40</td><td>40</td><td>5,120</td><td>1 V100</td></tr><tr><td>LLaMA 33B</td><td>33B</td><td>60</td><td>52</td><td>6,656</td></tr><tr><td>LLaMA 65B</td><td>65B</td><td>80</td><td>64</td><td>8,192</td></tr><tr><td>chatgpt</td><td>可能175B</td><td>-</td><td>-</td><td>-</td><td>5个80GB的A100加载</td><td></td></tr><tr><td>baidu文心</td><td>2600亿</td><td>-</td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>GPT-4</td><td>100000B</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>KOSMOS-1（微软多模态）</td><td>1.6B</td><td>-</td><td>-</td><td>-</td><td>-</td></tr></tbody></table></div><h1 id="6-影响LLM性能的主要因素"><a href="#6-影响LLM性能的主要因素" class="headerlink" title="6. 影响LLM性能的主要因素"></a>6. 影响LLM性能的主要因素</h1><p>openAI 论文：Scaling Laws for Neural Language Models<br><img src="/LLM/01_intro/2023-05-20-22-33-35.png" alt><br>OpenAI的论文Scaling Laws中列举了影响模型性能最大的三个因素：<strong>计算量、数据集大小、模型参数量</strong>。也就是说，当其他因素不成为瓶颈时，计算量、数据集大小、模型参数量这3个因素中的单个因素指数增加时，loss会线性的下降。同时，DeepMind的研究也得出来和OpenAI类似的结论。那么我们可以基本确定，如果一个模型在这3个方面，均做的不错，那么将会是一个很好的备选。<br>模型参数量是我们最容易注意到的，一般而言，LLM也只在训练数据上训练1个epoch（如果还有算力，其实可以扩更多的新数据），那么，数据集的大小就是很关键的参数。训练OPT-175B的Susan Zhang在Stanford分享的时候，也提到了，如果能够重新再来一次，她会选择much much more data。可见数据量的重要性。</p><p>了解到Scaling Laws之后，<strong>为了降低模型的推理成本，可以在模型参数量降低的同时，增加训练的数据量，这样可以保证模型的效果。Chinchilla和LLaMA就是这样的思路。</strong></p><p>除了以上的因素之外，还有一个比较大的影响因素就是数据质量。</p><h1 id="7-一些概念"><a href="#7-一些概念" class="headerlink" title="7. 一些概念"></a>7. 一些概念</h1><h2 id="7-1-预训练和微调的区别"><a href="#7-1-预训练和微调的区别" class="headerlink" title="7.1. 预训练和微调的区别"></a>7.1. 预训练和微调的区别</h2><p>直观体现在数据集体量， 很大规模的训练集训练出的模型可称为预训练模型。<br>预训练： 首先在一个大规模的数据集上训练一个深度学习模型，例如使用自监督学习或者无监督学习算法进行预训练；<br>微调： 使用目标任务的训练集对预训练模型进行微调。通常，只有预训练模型中的一部分层被微调，例如只微调模型的最后几层或者某些中间层。在微调过程中，通过反向传播算法对模型进行优化，使得模型在目标任务上表现更好；也可进行参数全量微调。<br>评估： 使用目标任务的测试集对微调后的模型进行评估，得到模型在目标任务上的性能指标。</p><h2 id="7-2-finetuning-promot-和-instruct和的区别"><a href="#7-2-finetuning-promot-和-instruct和的区别" class="headerlink" title="7.2. finetuning, promot 和 instruct和的区别"></a>7.2. finetuning, promot 和 instruct和的区别</h2><p>参考LLM-09笔记</p><h2 id="指令微调是什么"><a href="#指令微调是什么" class="headerlink" title="指令微调是什么"></a>指令微调是什么</h2><h1 id="8-相关技术原理和实战经验"><a href="#8-相关技术原理和实战经验" class="headerlink" title="8. 相关技术原理和实战经验"></a>8. 相关技术原理和实战经验</h1><p><a href="https://github.com/liguodongiot/llm-action" target="_blank" rel="noopener">https://github.com/liguodongiot/llm-action</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://mp.weixin.qq.com/s/NOcUlNPOXZheXJ9yX4wrLQ&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;大模型选型的一点思考&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https:
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM（6）-BARD</title>
    <link href="https://tlylft.github.io/LLM/06_bard/"/>
    <id>https://tlylft.github.io/LLM/06_bard/</id>
    <published>2023-05-16T00:52:19.000Z</published>
    <updated>2023-08-10T12:24:19.042Z</updated>
    
    <content type="html"><![CDATA[<p>bard.google.com</p><p>优点：</p><ol><li><p>联网，时效性强，依靠强大的Google搜索引擎，bard能够搜索到实时的信息，例如今天的新闻、今天的比赛比分等</p></li><li><p>谷歌希望将Bard打造成谷歌全家桶的入口，因此可以方便地通过Bard和谷歌搜索、谷歌地图、gmail邮件等谷歌全家桶联动使用，提高各个工具的使用效率。</p></li><li><p>响应速度惊艳，而且同时可以提供三个答案以供选择，还有导出结果等功能，使用更加方便快捷</p></li></ol><p>缺点：</p><ol><li>目前还不支持中文提问，只支持英语、日文、韩文，但谷歌表示会在之后的版本中支持中文。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;bard.google.com&lt;/p&gt;
&lt;p&gt;优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;联网，时效性强，依靠强大的Google搜索引擎，bard能够搜索到实时的信息，例如今天的新闻、今天的比赛比分等&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;谷歌希望将Bard打造成谷歌全家桶的入口，
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LLM（7）-大模型微调技术</title>
    <link href="https://tlylft.github.io/LLM/07_finetuning/"/>
    <id>https://tlylft.github.io/LLM/07_finetuning/</id>
    <published>2023-05-16T00:52:19.000Z</published>
    <updated>2023-08-18T00:33:48.766Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>参考文献：<br><a href="https://zhuanlan.zhihu.com/p/618028708?utm_id=0" target="_blank" rel="noopener">ChatGPT微调技术框架-完美Clone ChatGPT背后的技术</a><br><a href="https://baijiahao.baidu.com/s?id=1771636050471546897&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">人工智能大语言模型微调技术：SFT、LoRA、Freeze 监督微调方法</a><br><a href="https://zhuanlan.zhihu.com/p/635152813" target="_blank" rel="noopener">大模型参数高效微调技术原理综述（一）-背景、参数高效微调简介</a><br><a href="https://zhuanlan.zhihu.com/p/635686756" target="_blank" rel="noopener">大模型参数高效微调技术原理综述（二）-BitFit、Prefix Tuning、Prompt Tuning</a><br>[TOC]</p><h1 id="1-background"><a href="#1-background" class="headerlink" title="1. background"></a>1. background</h1><h2 id="1-1-全量参数微调与参数高效微调"><a href="#1-1-全量参数微调与参数高效微调" class="headerlink" title="1.1. 全量参数微调与参数高效微调"></a>1.1. 全量参数微调与参数高效微调</h2><p>以BERT模型为代表的“预训练语言模型 + 下游任务微调”训练模式成为了自然语言处理研究和应用的新范式。此处的下游任务微调是基于模型全量参数进行微调。</p></blockquote><p>但是，以 GPT3 为代表的预训练语言模型（PLM）参数规模变得越来越大，这使得在消费级硬件上进行全量微调变得不可行。<br>除此之外，模型全量微调还会损失多样性，存在灾难性遗忘的问题。</p><p>传统的监督微调方法已经不再能适用现阶段的大语言模型。为了解决微调参数量太多的问题，同时也要保证微调效果，急需研发出参数高效的微调方法（Parameter Efficient Fine Tuning, PEFT）</p><p><strong>全量参数微调和参数高效微调区别</strong></p><ul><li>微调， Fine-Tuning，一般指全参数的微调 (全量微调)，指是一类较早诞生的微调方法，全参数微调需要消耗大量的算力，实际使用起来并不方便，不能适用于大模型敏捷开发。因此不久之后又诞生了只围绕部分参数进行微调的高效微调方法;</li><li>高效微调，State-of-the-art Parameter-Efficient Fine-Tuning (SOTA PEFT)，是指微调少量或额外的模型参数，固定大部分预训练模型（LLM）参数，从而大大降低了计算和存储成本，同时，也能实现与全量参数微调相当的性能。参数高效微调方法甚至在某些情况下比全量微调效果更好，可以更好地泛化到域外场景。</li><li>除此之外，Fine-Tuning也可以代指全部微调方法，同时OpenAl中模型微调API的名称也是Fine-Tuning，实际也是PEFT</li></ul><h2 id="1-2-高效参数微调"><a href="#1-2-高效参数微调" class="headerlink" title="1.2. 高效参数微调"></a>1.2. 高效参数微调</h2><p>高效微调技术可以粗略分为以下三大类：增加额外参数（A）、选取一部分参数更新（S）、引入重参数化（R）。而在增加额外参数这类方法中，又主要分为类适配器（Adapter-like）方法和软提示（Soft prompts）两个小类。<br><img src="/LLM/07_finetuning/2023-08-17-17-38-48.png" alt></p><h1 id="2-微调方法"><a href="#2-微调方法" class="headerlink" title="2. 微调方法"></a>2. 微调方法</h1><h2 id="2-1-LoRA"><a href="#2-1-LoRA" class="headerlink" title="2.1. LoRA"></a>2.1. LoRA</h2><p><strong>一句话： 冻结预训练参数，微调新增层结构，大大减少训练参数量。</strong><br>LoRA（Low-Rank Adaptation of Large Language Models），直译为大语言模型的低阶自适应。<br><strong>背景：</strong><br>随着大语言模型的发展，模型的参数量越来越大，比如 GPT-3 参数量已经高达 1750 亿，因此，微调所有模型参数变得不可行。LoRA 微调方法由微软提出，通过只微调新增参数的方式，大大减少了下游任务的可训练参数数量。<br><strong>原理简述：</strong><br>基于大模型的内在低秩特性，增加旁路矩阵来<strong>模拟全参数微调</strong>;<br>LoRA 的基本原理是冻结预训练好的模型权重参数，在冻结原模型参数的情况下，通过往模型中加入额外的网络层，并只训练这些新增的网络层参数。由于这些新增参数数量较少，这样不仅 finetune 的成本显著下降，还能获得和全模型参数参与微调类似的效果。<br>在大语言模型微调的过程中，LoRA 冻结了预先训练好的模型权重，并将可训练的秩的分解矩阵注入到 Transformer 体系结构的每一层。例如，对于预训练的权重矩阵W0，可以让其更新受到用低秩分解表示后者的约束：<br><img src="/LLM/07_finetuning/2023-08-11-10-55-28.png" alt><br><strong>优势：</strong></p><ul><li>预训练模型参数可以被共享，用于为不同的任务构建许多小的 LoRA 模块。冻结共享模型，并通过替换矩阵 A 和 B 可以有效地切换任务，从而显著降低存储需求和多个任务切换的成本。<br>当使用自适应优化器时，由于不需要计算梯度以及保存太多模型参数，LoRA 使得微调效果更好，并将微调的硬件门槛降低了 3 倍。</li><li>低秩分解采用线性设计的方式使得在部署时能够将可训练的参数矩阵与冻结的参数矩阵合并，与完全微调的方法相比，不引入推理延迟。</li><li>LoRA 与其它多种微调方法不冲突，可以与其它微调方法相结合，比如将要介绍的前缀调优方法等。</li><li>在图像生成任务中diffusion model进行微调，表现惊艳。<h2 id="2-2-Prefix-tuning"><a href="#2-2-Prefix-tuning" class="headerlink" title="2.2. Prefix-tuning"></a>2.2. Prefix-tuning</h2><blockquote><p>问题：更新训练的embedding层，最后在推理的时候也要加上吗？</p></blockquote></li></ul><p>Prefix-tuning（ Optimizing Continuous Prompts for Generation）基于提示词前缀优化的微调方法。由斯坦福大学提出。<br><strong>原理简述：</strong> 在原始模型基础上，增加一个可被训练的Embedding层，用于给提示词增加前缀，从而让模型更好的理解提示词意图并在训练过程中不断优化这些参数。<br>在模型中加入 prefix，即连续的特定任务向量，微调时只优化这一小段参数。<br><img src="/LLM/07_finetuning/2023-08-11-11-00-15.png" alt><br><strong>优势：</strong><br>既能够在模型结构上增加一些新的灵活性，又能够在模型使用上提供一种自动的、能够改进模型表现的天天机创</p><h2 id="2-3-P-tuning-Prompt-tuning-v1"><a href="#2-3-P-tuning-Prompt-tuning-v1" class="headerlink" title="2.3. P-tuning/Prompt tuning v1"></a>2.3. P-tuning/Prompt tuning v1</h2><p>P-tuning v1 微调方法由谷歌提出的轻量级优化方法<br><strong>原理简述：</strong><br>无需调整模型参数，在已有参数中，选择一部分参数作为可学习参数，用于创建每个Prompt的前缀，从而帮助模型更好地理解和处理特定的任务。<br>不同于Prefix方法，Prompt Tuning训练得到的前缀是具备可解释性的我们可以通过查看这些前缀，来查看模型是如何帮我们优化prompt的。类似于<strong>自动化提示工程的工具。</strong><br>将Prompt 加入到微调过程中，只对 Prompt 部分的参数进行训练，而语言模型的参数固定不变。</p><p><img src="/LLM/07_finetuning/2023-08-11-11-03-32.png" alt><br><strong>优势：</strong><br>该方法在参数规模非常大的模型微调时效果很好，当参数规模达100亿时和全量微调效果一致;<br><strong>不足：</strong><br>P-tuning v1 微调方法缺少普遍性。实验表明，对于那些较小的模型，P-tuning v1 方法和全参数微调方法的表现有很大差异，效果很差。同时，P-tuning v1 缺少跨任务的通用性，在序列标注任务中的有效性没有得到验证。序列标注需要预测一连串的标签，而且大都是无实际意义的标签，对于 P-tuning v1 微调方法极具挑战。此外，当模型层数很深时，微调时模型的稳定性难以保证。模型层数越深，第一层输入的 prompt 对后面的影响难以预估。</p><h2 id="2-4-P-tuning-v2"><a href="#2-4-P-tuning-v2" class="headerlink" title="2.4. P-tuning v2"></a>2.4. P-tuning v2</h2><p>P-tuning v2 微调方法是 P-tuning v1 微调方法的改进版，同时借鉴了 prefix-tuning 微调的方法。<br><img src="/LLM/07_finetuning/2023-08-10-22-14-49.png" alt><br>与 P-tuning v1 微调方法相比，P-tuning v2 微调方法采用了 prefix-tuning 的做法，在输入前面的每一层都加入可微调的参数。在 prefix 部分，每一层的 transformer 的 embedding 输入都需要被微调，而 P-tuning v1 只在第一层进行微调。同时，对于 prefix 部分，每一层 transformer 的输入不是从上一层输出，而是随机初始化的 embedding 作为输入。</p><p>此外，P-Tuning v2 还包括以下改进：</p><p>移除 Reparamerization 加速训练方式；<br>采用多任务学习优化：基于多任务数据集的 Prompt 进行预训练，然后再适配的下游任务。<br>舍弃词汇 Mapping 的 Verbalizer 的使用，重新利用 [CLS] 和字符标签，跟传统微调方法一样利用 cls 或者 token 的输出做自然语言理解，以增强通用性，可以适配到序列标注任务。<br><strong>优点：</strong></p><ul><li><p>P-tuning v2 微调方法解决了 P-tuning v1 方法的缺陷，是一种参数高效的大语言模型微调方法。</p></li><li><p>P-tuning v2 微调方法仅精调 0.1% 参数量（固定 LM 参数），在各个参数规模语言模型上，均取得和 - Fine-tuning 相比肩的性能，解决了 P-tuning v1 在参数量不够多的模型中微调效果很差的问题。如下图所示（横坐标表示模型参数量，纵坐标表示微调效果）：</p></li><li><p>将 Prompt tuning 技术首次拓展至序列标注等复杂的 NLU 任务上，而 P-tuning v1 在此任务上无法运作。</p></li><li>非常适合GLM这种双向预训练大模型微调<h2 id="2-5-freeze"><a href="#2-5-freeze" class="headerlink" title="2.5. freeze"></a>2.5. freeze</h2>Freeze 方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行 TP 或 PP 操作，就可以对大模型进行训练。在语言模型模型微调中，Freeze 微调方法仅微调 Transformer 后几层的全连接层参数，而冻结其它所有参数<br>优点：<br>大量减少了大语言模型的微调参数，是一种参数高效的微调方法；<br>由于只需微调高层特征，加快了模型的收敛，节约了微调的时间；<br>最大程度地保留了大语言模型预训练所学习到的语言的 “共性”，可解释性较强。</li></ul><h1 id="3-Instruct-gpt-chatgpt的三个可微调阶段"><a href="#3-Instruct-gpt-chatgpt的三个可微调阶段" class="headerlink" title="3. Instruct gpt/chatgpt的三个可微调阶段"></a>3. Instruct gpt/chatgpt的三个可微调阶段</h1><p>根据instructGPT论文和ChatGPT的技术报告，背后的微调技术逐步挖掘出来，大致分为：SFT；ReWard模型训练；RLHF（PPO）三个阶段。</p><p><img src="/LLM/07_finetuning/2023-08-10-22-21-36.png" alt></p><h2 id="3-1-stage1-SFT"><a href="#3-1-stage1-SFT" class="headerlink" title="3.1. stage1: SFT"></a>3.1. stage1: SFT</h2><p>SFT（supervised-fintuning），使用instruction datasets数据进行监督微调</p><h2 id="3-2-stage2-Reward-Model"><a href="#3-2-stage2-Reward-Model" class="headerlink" title="3.2. stage2: Reward Model"></a>3.2. stage2: Reward Model</h2><p>训练奖励模型，它通过对于同一个 prompt 的不同输出进行人工排序，得到对应分数，监督训练奖励模型。</p><h2 id="3-3-stage3-RLHF-Reinforcement-Learning-from-Human-Feedback"><a href="#3-3-stage3-RLHF-Reinforcement-Learning-from-Human-Feedback" class="headerlink" title="3.3. stage3: RLHF(Reinforcement Learning from Human Feedback)"></a>3.3. stage3: RLHF(Reinforcement Learning from Human Feedback)</h2><p>这个部分是Openai， ChatGPT和GPT4作为核心的部分,该训练流程基于PPO（Proximal Policy Optimization，近端策略优化）算法来进行优化。<br>RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法。<br><img src="/LLM/07_finetuning/2023-08-10-20-48-36.png" alt></p><h1 id="4-微调框架和流程"><a href="#4-微调框架和流程" class="headerlink" title="4. 微调框架和流程"></a>4. 微调框架和流程</h1><h2 id="4-1-模型选择"><a href="#4-1-模型选择" class="headerlink" title="4.1. 模型选择"></a>4.1. 模型选择</h2><p>根据业务场景和算例条件，选择合适的底座参数模型</p><h2 id="4-2-pipeline训练"><a href="#4-2-pipeline训练" class="headerlink" title="4.2. pipeline训练"></a>4.2. pipeline训练</h2><p><strong><a href="https://github.com/hpcaitech/ColossalAI/blob/main/docs/README-zh-Hans.md" target="_blank" rel="noopener">ColossalAI</a>初步打造全流程雏形</strong><br>根据底层技术优化很好的把三个阶段训练进行了串联，实现微调的高效性。体验基于Meta LLAM 7B 微调后模型效果，中文的效果不行（可能是meta llama底座对中文不是非常友好，单纯微调中文英文的可以基于GLM模型来微调）<br>注：</p><ol><li>一些介绍：<a href="https://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&amp;mid=2650872068&amp;idx=1&amp;sn=2cb6257bbbc10dc4da19e88d4fc58cbc" target="_blank" rel="noopener">0门槛克隆ChatGPT方案再升级，开源模型完整复现</a></li><li>似乎只是基于LLaMA</li></ol><p><strong><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener">DeepSpeed Chat</a></strong></p><ol><li>一些介绍：<a href="https://zhuanlan.zhihu.com/p/621780753" target="_blank" rel="noopener">DeepSpeed-Chat：最强ChatGPT训练框架，一键完成RLHF训练</a></li><li>似乎只是gpt-2这种，但github上有chatglm相关的</li></ol><p>目前整个开源社区还没有其他能够在一个框架上面训练如上的三个阶段的流程化训练，大家都是分阶段独立训练</p><h3 id="4-2-1-stage1-SFT"><a href="#4-2-1-stage1-SFT" class="headerlink" title="4.2.1. stage1: SFT"></a>4.2.1. stage1: SFT</h3><p>SFT（supervised-fintuning），使用instruction datasets数据进行监督微调<br>数据集：<br>instruct:xxx<br>output: xxxx<br>微调方式：</p><ul><li>全量微调</li><li>Freeze</li><li>lora</li><li>P-Tuning</li><li>Prefix Tuning</li><li>PromptTuning</li><li>AdaLora</li></ul><p>开源框架：<br><strong><a href="https://github.com/huggingface/peft" target="_blank" rel="noopener">peft</a></strong>: 支持多个模型，如：GPT-2，LLaMA, ChatGLM<br><strong><a href="https://github.com/liucongg/ChatGLM-Finetuning" target="_blank" rel="noopener">ChatGLM-Finetuning</a></strong>: 支持chatGLM</p><h3 id="4-2-2-stage2-Reward-Model-amp-stage-3-RLHF"><a href="#4-2-2-stage2-Reward-Model-amp-stage-3-RLHF" class="headerlink" title="4.2.2. stage2: Reward Model &amp; stage 3: RLHF"></a>4.2.2. stage2: Reward Model &amp; stage 3: RLHF</h3><p>数据集：<br>微调方式：</p><p>开源框架：<br><a href="https://github.com/huggingface/trl" target="_blank" rel="noopener">TRL</a>: 支持Reward模型训练和PPO优化过程<br><strong><a href="https://github.com/microsoft/DeepSpeed" target="_blank" rel="noopener">DeepSpeed Chat</a></strong>： 微软开源提供的最好的RLHF流程</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;参考文献：&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/618028708?utm_id=0&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ChatGPT微调技术框架-完美Clone Chat
      
    
    </summary>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/categories/LLM/"/>
    
    
      <category term="LLM" scheme="https://tlylft.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>prophet</title>
    <link href="https://tlylft.github.io/machine_learning/time_series/prophet/"/>
    <id>https://tlylft.github.io/machine_learning/time_series/prophet/</id>
    <published>2023-05-04T00:40:23.000Z</published>
    <updated>2023-06-26T08:28:41.315Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-what-is-prophet"><a href="#1-what-is-prophet" class="headerlink" title="1. what is prophet"></a>1. what is prophet</h1><p><a href="https://github.com/facebook/prophet" target="_blank" rel="noopener">github链接</a><br><a href="https://facebook.github.io/prophet/docs/installation.html" target="_blank" rel="noopener">官方文档</a><br>Prophet是Facebook数据科学团队于2017年发布的开源预测软件包，其内容发表在《Forecasting at scale》论文中。目前可以通过Python和R进行实现，该模型可以通过简单的参数配置，实现高精准的时间序列预测。</p><h1 id="2-Prophet适用场景"><a href="#2-Prophet适用场景" class="headerlink" title="2. Prophet适用场景"></a>2. Prophet适用场景</h1><p>预测模型均有其适用的场景，Prophet也不例外，只有在合适的场景下，才能发挥模型本身的威力，具体适用场景如下：</p><p>训练数据：拥有至少一个完整周期的数据，让模型完整学习规律。</p><p>数据趋势：数据有一定正常的周期效应，例如：周末效应、季节效应等。</p><p>跳变情况：明确可能发生跳变的时间点及窗口期，例如：双十一、国庆节等。</p><p>缺失值符合预期：历史数据的缺失值和异常值保持在合理范围内。</p><h1 id="3-模型使用QuickStart"><a href="#3-模型使用QuickStart" class="headerlink" title="3. 模型使用QuickStart"></a>3. 模型使用QuickStart</h1><h2 id="3-1-环境配置"><a href="#3-1-环境配置" class="headerlink" title="3.1. 环境配置"></a>3.1. 环境配置</h2><p>python 3.7+<br>pip install prophet</p><h2 id="3-2-训练数据格式"><a href="#3-2-训练数据格式" class="headerlink" title="3.2. 训练数据格式"></a>3.2. 训练数据格式</h2><div class="table-container"><table><thead><tr><th>DS</th><th>Y</th></tr></thead><tbody><tr><td>2007-12-10</td><td>9.590761</td></tr><tr><td>2007-12-11</td><td>8.519590</td></tr><tr><td>2007-12-12</td><td>8.183677</td></tr><tr><td>2007-12-13</td><td>8.072467</td></tr><tr><td>2007-12-14</td><td>7.893572</td></tr></tbody></table></div><h2 id="3-3-快速调用"><a href="#3-3-快速调用" class="headerlink" title="3.3. 快速调用"></a>3.3. 快速调用</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">from prophet import Prophet</span><br><span class="line">df &#x3D; pd.read_csv(&#39;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;facebook&#x2F;prophet&#x2F;main&#x2F;examples&#x2F;example_wp_log_peyton_manning.csv&#39;)</span><br><span class="line">df.head()</span><br><span class="line"># 训练</span><br><span class="line">m &#x3D; Prophet()</span><br><span class="line">m.fit(df)</span><br><span class="line"># 生成预测数据</span><br><span class="line">future &#x3D; m.make_future_dataframe(periods&#x3D;365)  # 包含训练数据的和未来365天的预测日期</span><br><span class="line">future.tail()</span><br><span class="line"># 对上面训练+预测数据进行预测拟合</span><br><span class="line">forecast &#x3D; m.predict(future)  # 可以返回一个Prophet.Forecast对象或者对象的历</span><br><span class="line">forecast[[&#39;ds&#39;, &#39;yhat&#39;, &#39;yhat_lower&#39;, &#39;yhat_upper&#39;]].tail()</span><br><span class="line"># 画图</span><br><span class="line">fig1 &#x3D; m.plot(forecast)</span><br><span class="line">fig2 &#x3D; m.plot_components(forecast)</span><br></pre></td></tr></table></figure><h1 id="4-模型原理详解"><a href="#4-模型原理详解" class="headerlink" title="4. 模型原理详解"></a>4. 模型原理详解</h1><p>简单来说，Prophet 把时序分为四个部分，认为任意时刻的值，就是这四个部分相加的结果。这四个部分分别是：<br><img src="/machine_learning/time_series/prophet/2023-06-26-16-23-46.png" alt></p><ul><li>趋势成分g(t)：时序最内核的一个增长趋势，可设定线性或非线性增长<br><img src="/machine_learning/time_series/prophet/2023-06-26-16-22-50.png" alt></li><li>季节性成分s(t)：使用傅里叶级数拟合时序中跟时间强烈相关、周期性上升下降的成分</li><li>节假日影响h(t)：时序中非周期性出现的影响，将过去、将来相同的节假日设置成一个虚拟变量，并拟合该变量的相关性</li><li>误差项：不可预测的假设为正态分布的误差部分</li></ul><p>对于上面第二个相乘的形式，当考虑对数时，也转换为四部分相加的形式，所以两种形式都可以作为可加模型进行讨论。<br><img src="/machine_learning/time_series/prophet/2023-06-26-16-26-24.png" alt><br><a href="https://blog.csdn.net/weixin_42470516/article/details/108961132" target="_blank" rel="noopener">各部分公式详解1</a><br><a href="https://blog.csdn.net/a358463121/article/details/70194279" target="_blank" rel="noopener">各部分公式详解2</a></p><h1 id="5-模型应用调优技巧"><a href="#5-模型应用调优技巧" class="headerlink" title="5. 模型应用调优技巧"></a>5. 模型应用调优技巧</h1><h2 id="趋势项调优"><a href="#趋势项调优" class="headerlink" title="趋势项调优"></a>趋势项调优</h2><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="6-推理速度优化"><a href="#6-推理速度优化" class="headerlink" title="6. 推理速度优化"></a>6. 推理速度优化</h2><p>refer:<a href="https://towardsdatascience.com/how-to-run-facebook-prophet-predict-x100-faster-cce0282ca77d" target="_blank" rel="noopener">https://towardsdatascience.com/how-to-run-facebook-prophet-predict-x100-faster-cce0282ca77d</a></p><p>prophet推理速度时间大概为1.15 s ± 55.9 ms per loop，比训练的时间还长，不符合通常模型的常规情况。<br>通过翻阅源码debug， 发现在predict过程中，Prophet’s uncertainty占据了大部分的预测时间，这个步骤主要是确定预测的不确定范围。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-what-is-prophet&quot;&gt;&lt;a href=&quot;#1-what-is-prophet&quot; class=&quot;headerlink&quot; title=&quot;1. what is prophet&quot;&gt;&lt;/a&gt;1. what is prophet&lt;/h1&gt;&lt;p&gt;&lt;a href=
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/categories/machine-learning/"/>
    
      <category term="time series" scheme="https://tlylft.github.io/categories/machine-learning/time-series/"/>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>charles 抓包工具</title>
    <link href="https://tlylft.github.io/crawler/charles/"/>
    <id>https://tlylft.github.io/crawler/charles/</id>
    <published>2023-04-17T12:54:58.000Z</published>
    <updated>2023-08-11T09:11:12.641Z</updated>
    
    <content type="html"><![CDATA[<p>charles 中文乱码问题</p><p>proxy配置</p><p>手机代理：<br>下载夜游模拟器，设置网络无线代理为windows的ip地址。</p><p>打开浏览器： charles.pro/ssl 下载证书，<br>在手机设置，从sd卡安装，安装刚下载的证书<br>在浏览器设置，—隐私和安全—显示安全警告（不勾选）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;charles 中文乱码问题&lt;/p&gt;
&lt;p&gt;proxy配置&lt;/p&gt;
&lt;p&gt;手机代理：&lt;br&gt;下载夜游模拟器，设置网络无线代理为windows的ip地址。&lt;/p&gt;
&lt;p&gt;打开浏览器： charles.pro/ssl 下载证书，&lt;br&gt;在手机设置，从sd卡安装，安装刚下载的证书
      
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tlylft.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://tlylft.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>爬虫加密算法</title>
    <link href="https://tlylft.github.io/crawler/encryption/"/>
    <id>https://tlylft.github.io/crawler/encryption/</id>
    <published>2023-04-17T12:54:58.000Z</published>
    <updated>2023-08-11T09:11:06.013Z</updated>
    
    <content type="html"><![CDATA[<p>这个网站可以查看很多加密算法的结果：<br><a href="https://spidertools.cn/#/crypto" target="_blank" rel="noopener">https://spidertools.cn/#/crypto</a></p><h1 id="base64"><a href="#base64" class="headerlink" title="base64"></a>base64</h1><p>是一种编码，不算加密，<strong>一般末尾会有个等号</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">                      u         p</span><br><span class="line">ASCII码对应16进制数据  7  5     7      0</span><br><span class="line">转换成2进制         0111 0101  0111 0000</span><br><span class="line">按6位重组数据       011101  010111 0000(00 000000)  不足六位进行补00  再补六个0</span><br><span class="line">转换成10进制         29     23       A       &#x3D;    补位为等号</span><br><span class="line">最终结果              dXA&#x3D;</span><br></pre></td></tr></table></figure></p><h1 id="md5"><a href="#md5" class="headerlink" title="md5"></a>md5</h1><p>信息摘要算法，可用于文本表示， 可以产出一个128位（16字节）的散列值，表示为16进制<strong>是32位</strong>。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">md5_test</span><span class="params">(text)</span>:</span></span><br><span class="line">    result = hashlib.md5(text.encode(<span class="string">'utf-8'</span>).hexdigest())</span><br></pre></td></tr></table></figure></p><h1 id="SHA-1"><a href="#SHA-1" class="headerlink" title="SHA-1"></a>SHA-1</h1><p>sha-1通常长度为16进制是40位<br>sha-224通常长度为16进制是56位<br>sha-256通常长度为16进制是64位<br>sha-384通常长度为16进制是96位<br>sha-512通常长度为16进制是128位<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">SHA_test</span><span class="params">(text)</span>:</span></span><br><span class="line">    result = hashlib.sha1(text.encode(<span class="string">'utf-8'</span>).hexdigest())</span><br><span class="line">    result = hashlib.sha224(text.encode(<span class="string">'utf-8'</span>).hexdigest())</span><br><span class="line">    result = hashlib.sha246(text.encode(<span class="string">'utf-8'</span>).hexdigest())</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这个网站可以查看很多加密算法的结果：&lt;br&gt;&lt;a href=&quot;https://spidertools.cn/#/crypto&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://spidertools.cn/#/crypto&lt;/a&gt;&lt;/p&gt;
&lt;h
      
    
    </summary>
    
    
      <category term="爬虫" scheme="https://tlylft.github.io/categories/%E7%88%AC%E8%99%AB/"/>
    
    
      <category term="爬虫" scheme="https://tlylft.github.io/tags/%E7%88%AC%E8%99%AB/"/>
    
  </entry>
  
  <entry>
    <title>python迭代器和生成器</title>
    <link href="https://tlylft.github.io/python/python/python_iter/"/>
    <id>https://tlylft.github.io/python/python/python_iter/</id>
    <published>2023-03-30T09:03:48.000Z</published>
    <updated>2023-03-31T01:09:47.116Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是可迭代对象"><a href="#什么是可迭代对象" class="headerlink" title="什么是可迭代对象"></a>什么是可迭代对象</h1><pre><code>元组、列表、字典、集合、字符串nums = [11, 22, 33]for num in nums:    print(num)</code></pre><h1 id="判断可迭代对象"><a href="#判断可迭代对象" class="headerlink" title="判断可迭代对象"></a>判断可迭代对象</h1><h2 id="for-循环测试"><a href="#for-循环测试" class="headerlink" title="for 循环测试"></a>for 循环测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">num &#x3D; 3.14</span><br><span class="line"># for i in num:</span><br><span class="line">#     print(i)</span><br></pre></td></tr></table></figure><h2 id="2-使用Iterable类进行迭代对象的判断"><a href="#2-使用Iterable类进行迭代对象的判断" class="headerlink" title="2. 使用Iterable类进行迭代对象的判断"></a>2. 使用Iterable类进行迭代对象的判断</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from collections.abc import Iterable</span><br><span class="line">str_data &#x3D; &#39;abc&#39;</span><br><span class="line"></span><br><span class="line"># 判断浮点类型的实例对象是否是Iterable类的子类</span><br><span class="line">print(isinstance(num, Iterable))</span><br><span class="line"></span><br><span class="line">print(isinstance(str_data, Iterable))</span><br></pre></td></tr></table></figure><h1 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h1><p>迭代对象不一定是迭代器，迭代对象可用迭代器加载<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from collections.abc import Iterator, Iterable</span><br><span class="line"></span><br><span class="line">nums &#x3D; [11, 22, 33, 44]</span><br><span class="line"></span><br><span class="line"># num是一个可迭代对象</span><br><span class="line">print(isinstance(nums, Iterable))</span><br><span class="line"></span><br><span class="line"># 但不是一个迭代器</span><br><span class="line">print(isinstance(nums, Iterator))</span><br><span class="line"></span><br><span class="line"># 可以创建一个迭代器对象的</span><br><span class="line">nums_iter &#x3D; iter(nums)</span><br><span class="line">print(isinstance(nums_iter, Iterator))  # True</span><br><span class="line">print(isinstance(nums_iter, Iterable))  # True</span><br></pre></td></tr></table></figure></p><p>自定义一个迭代器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">class StuSystem(object):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    学生管理系统</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.index &#x3D; 0</span><br><span class="line">        self.student_name &#x3D; []</span><br><span class="line"></span><br><span class="line">    def add(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        添加一个新的学生</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        name &#x3D; input(&quot;请输入新学生的姓名:&quot;)</span><br><span class="line">        # tel &#x3D; input(&quot;请输入新学生的手机号:&quot;)</span><br><span class="line">        # address &#x3D; input(&quot;请输入新学生的住址:&quot;)</span><br><span class="line"></span><br><span class="line">        new_stu &#x3D; dict()</span><br><span class="line">        new_stu[&quot;name&quot;] &#x3D; name</span><br><span class="line">        # new_stu[&quot;tel&quot;] &#x3D; tel</span><br><span class="line">        # new_stu[&quot;address&quot;] &#x3D; address</span><br><span class="line"></span><br><span class="line">        self.student_name.append(new_stu)</span><br><span class="line"></span><br><span class="line">    def __iter__(self):</span><br><span class="line">        return self</span><br><span class="line"></span><br><span class="line">    def __next__(self):</span><br><span class="line">        if self.index &lt; len(self.student_name):</span><br><span class="line">            item &#x3D; self.student_name[self.index]</span><br><span class="line">            self.index +&#x3D; 1</span><br><span class="line">            return item</span><br><span class="line">        else:</span><br><span class="line">            raise StopIteration</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">stu_sys &#x3D; StuSystem()</span><br><span class="line">stu_sys.add(&#39;1&#39;)</span><br><span class="line">stu_sys.add(&#39;2&#39;)</span><br><span class="line">stu_sys.add(&#39;3&#39;)</span><br><span class="line"></span><br><span class="line">for item in stu_sys:</span><br><span class="line">    print(item)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;什么是可迭代对象&quot;&gt;&lt;a href=&quot;#什么是可迭代对象&quot; class=&quot;headerlink&quot; title=&quot;什么是可迭代对象&quot;&gt;&lt;/a&gt;什么是可迭代对象&lt;/h1&gt;&lt;pre&gt;&lt;code&gt;元组、列表、字典、集合、字符串


nums = [11, 22, 33]

      
    
    </summary>
    
    
      <category term="python" scheme="https://tlylft.github.io/categories/python/"/>
    
      <category term="python" scheme="https://tlylft.github.io/categories/python/python/"/>
    
    
      <category term="python" scheme="https://tlylft.github.io/tags/python/"/>
    
  </entry>
  
  <entry>
    <title>arima模型</title>
    <link href="https://tlylft.github.io/machine_learning/time_series/arima/"/>
    <id>https://tlylft.github.io/machine_learning/time_series/arima/</id>
    <published>2023-03-17T02:13:48.000Z</published>
    <updated>2023-03-22T00:19:05.339Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://blog.csdn.net/lam_yx/article/details/107887284" target="_blank" rel="noopener">https://blog.csdn.net/lam_yx/article/details/107887284</a></p><p>adf平稳性检验：<a href="https://blog.csdn.net/FrankieHello/article/details/86766625/" target="_blank" rel="noopener">https://blog.csdn.net/FrankieHello/article/details/86766625/</a></p><p>I 差分： 是前一个减去后一个的值，通过差分实现平稳化，一般不超过两阶，<br>时序输入：15-50个<br>预测输出：1-15个</p><p>step1: 要求序列满足平稳性，查看ADF结果分析t值，分析是否可以显著拒绝序列不平稳的假设（p&lt;0.05或0.01)<br>Step2: 查看差分前后数据对比图，判斯是否平稳（上下幅度不大），同时对时间序列进行偏自相关分析、自相关分析，根据截尾情况估算其 p、q 值。<br>Step3:ARIMA 模型要求时间序列数据具备纯随机性，即模型残差为白噪声，查看模型检验表，根据 Q 统计量的p 值对模型白噪声进行检验，也可以结合信息准则 AIC和 BIC值进行分析《越低越好》，也可通过 ACF/PACF 图进行分析<br>Step4: 根据模型参数表，出模型公式<br>Step5: 结合时间序列分析图进行分析，得到向后预测的阶数结果</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://blog.csdn.net/lam_yx/article/details/107887284&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://blog.csdn.net/lam_yx/article/deta
      
    
    </summary>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/categories/machine-learning/"/>
    
      <category term="time series" scheme="https://tlylft.github.io/categories/machine-learning/time-series/"/>
    
    
      <category term="machine learning" scheme="https://tlylft.github.io/tags/machine-learning/"/>
    
  </entry>
  
</feed>
